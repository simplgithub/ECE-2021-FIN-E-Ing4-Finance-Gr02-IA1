{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simplgithub/ECE-2021-FIN-E-Ing4-Finance-Gr02-IA1/blob/main/Projet_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubCeIvtF6R4W"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place un modèle à réseau de neurones récurrent de type LSTM pour réaliser des prédictions sur notre série temporelle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RRhtHsNn5fc3"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vJfSLtub6sdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "e9a48552-8d92-4e3f-b594-e712e3ae4e5c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Fonction permettant d'importer les données\n",
        "def get_melbourne_data() -> pd.DataFrame:\n",
        "    raw_url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
        "    response = urllib.request.urlopen(raw_url)\n",
        "    response = response.read().decode('utf-8')\n",
        "    data = pd.read_csv(StringIO(response))\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    return data\n",
        "\n",
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# importation et visualisation de le ST\n",
        "df = get_melbourne_data()\n",
        "df[\"serie\"] = df['Temp']\n",
        "serie = df[\"serie\"]\n",
        "df[\"temps\"] = range(0, df.shape[0])\n",
        "temps = df[\"temps\"]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie)\n",
        "plt.title(\"Série temporelle des temperature\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atOtbHUk7AYt"
      },
      "source": [
        "# Préparation des données X et Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78vEUlpIFMp6"
      },
      "outputs": [],
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.shuffle(buffer_melange).map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"Date\"] == \"1987-01-12\"]\n",
        "# Le temps de séparation qui correspond à au \"1987-01-12\" est 2201"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "n1dII2z3QK2v",
        "outputId": "d69e3e56-964c-4c5d-b1c8-c0851d5650b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Date  Temp  serie  temps\n",
              "2201 1987-01-12  12.4   12.4   2201"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88a3ef89-2068-4597-9f7b-116ac9de8519\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Temp</th>\n",
              "      <th>serie</th>\n",
              "      <th>temps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2201</th>\n",
              "      <td>1987-01-12</td>\n",
              "      <td>12.4</td>\n",
              "      <td>12.4</td>\n",
              "      <td>2201</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88a3ef89-2068-4597-9f7b-116ac9de8519')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88a3ef89-2068-4597-9f7b-116ac9de8519 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88a3ef89-2068-4597-9f7b-116ac9de8519');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Le temps de séparation qui correspond à au \"1987-01-12\" est t=2201. Nous allons ainsi définir les enchantillons d'apprentissage et de test***"
      ],
      "metadata": {
        "id": "8ZTkdAYcZB3b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHgcL7ZPJlws"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_14USRg5JvSu"
      },
      "outputs": [],
      "source": [
        "temps_separation = 2201\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjh4mjMrKhA5"
      },
      "source": [
        "**2. Préparation des données X et des labels Y**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdqfYabAKoE2"
      },
      "source": [
        "On commence par créer notre dataset à partir de la série (remarque : les valeurs ci-dessous sont en réalité mélangées) :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2CDLaYDoDms"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/split_XY_2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziajfOefKvsu"
      },
      "outputs": [],
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWJQ0ajCbXTY"
      },
      "source": [
        "**3. Normalisation des données**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0hJrSCFbRUw"
      },
      "outputs": [],
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSdWLIqDbUYw"
      },
      "outputs": [],
      "source": [
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "# 1. Création du réseau et adaptation des formats d'entrée et de sortie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at-Z_8-L-aaL"
      },
      "source": [
        "Le réseau que nous allons mettre en place est un réseau de neurones récurrent, composé de :\n",
        "  - Une couche récurrente LSTM à 40 neurones, de type séquence vers vecteur (sequence to vector) (Encodeur)\n",
        "  - Une couche dense avec 40 neurones et fonction d'activation tanh (Décodeur)\n",
        "  - Une couche dense avec 1 neurone (Générateur)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7uyzTcI-4fc"
      },
      "source": [
        "Pour adapter les données en entrée et en sortie du réseau au format attendu, nous allons utiliser une [couche de type lambda](https://keras.io/api/layers/core_layers/lambda/) avec Keras. Ce type de couche nous permet d'intégrer une fonction spécifique en tant que couche dans notre réseau de neurone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7iwhon8AD_9"
      },
      "source": [
        "- En entrée de la première couche récurrente, le format attendu est de type : [batch_size, #instants, #dims].   \n",
        "Le **format d'entrée attendu est donc [None,None,1]** car notre série temporelle est de type univariée, qu'on souhaite pouvoir traiter une séquence infinie, et que le batch size est géré automatiquement par tensorflow.  \n",
        "Il faut donc en entrée transformer le format [taille_fenetre, 1] vers le format [None, None, 1].  \n",
        "\n",
        "- En sortie de la deuxième couche récurrente, le format est de type [batch_size, #instants, #neurones]. Il sera de type [None, None, 40].  \n",
        "- La couche dense possède un seul neurone. En **sortie de la couche dense, le format est de type [None, None, 1]**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQifKMDBvcy"
      },
      "source": [
        "Sous Keras, nous allons utiliser une couche récurrente de type LSTM avec la classe [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/). En sortie, la fonction d'activation par défaut est de type `tanh` qui sort une sortie comprise entre [-1,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTM_1111.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86TUv-QxEFy9"
      },
      "source": [
        "Pour insérer une dimension de type `None` au format de l'entrée, on utilise la méthode [expand_dims](https://www.tensorflow.org/api_docs/python/tf/expand_dims) de tensorflow. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9KYLYtT7Qj2"
      },
      "outputs": [],
      "source": [
        "# Remise à zéro de tous les états générés par Keras\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDTE4-i0YHGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1933e539-2add-4d6e-d147-6a194625f074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 20)]         0           []                               \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 20, 1)        0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 40)           6720        ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 40)           1640        ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 80)           0           ['dense[0][0]',                  \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            81          ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,441\n",
            "Trainable params: 8,441\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre))\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees)\n",
        "s_encodeur = tf.keras.layers.LSTM(40)(e_adapt)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(40,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtCYAViV1uzQ"
      },
      "outputs": [],
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"model_initial.hdf5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kjgIQKYeN0q"
      },
      "source": [
        "# 2. Optimisation du taux d'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aixSutOy_X-T"
      },
      "outputs": [],
      "source": [
        "# Remise à zéro de tous les états générés par Keras\n",
        "tf.keras.backend.clear_session()\n",
        "model.load_weights(\"model_initial.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeECx41tT3bT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99beadb-aa5b-4f75-da9e-e7e554902ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     66/Unknown - 3s 10ms/step - loss: 0.4006 - mae: 0.7720\n",
            "Epoch 1: loss improved from inf to 0.39813, saving model to poids.hdf5\n",
            "68/68 [==============================] - 3s 15ms/step - loss: 0.3981 - mae: 0.7689 - lr: 1.0000e-08\n",
            "Epoch 2/100\n",
            "62/68 [==========================>...] - ETA: 0s - loss: 0.4012 - mae: 0.7728\n",
            "Epoch 2: loss did not improve from 0.39813\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3985 - mae: 0.7695 - lr: 1.2589e-08\n",
            "Epoch 3/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.4013 - mae: 0.7733\n",
            "Epoch 3: loss improved from 0.39813 to 0.39808, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3981 - mae: 0.7689 - lr: 1.5849e-08\n",
            "Epoch 4/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3986 - mae: 0.7697\n",
            "Epoch 4: loss did not improve from 0.39808\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3983 - mae: 0.7690 - lr: 1.9953e-08\n",
            "Epoch 5/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3982 - mae: 0.7692\n",
            "Epoch 5: loss did not improve from 0.39808\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3981 - mae: 0.7689 - lr: 2.5119e-08\n",
            "Epoch 6/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.4003 - mae: 0.7720\n",
            "Epoch 6: loss improved from 0.39808 to 0.39790, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3979 - mae: 0.7688 - lr: 3.1623e-08\n",
            "Epoch 7/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3976 - mae: 0.7678\n",
            "Epoch 7: loss improved from 0.39790 to 0.39735, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3973 - mae: 0.7682 - lr: 3.9811e-08\n",
            "Epoch 8/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3998 - mae: 0.7711\n",
            "Epoch 8: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3984 - mae: 0.7693 - lr: 5.0119e-08\n",
            "Epoch 9/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3990 - mae: 0.7708\n",
            "Epoch 9: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3986 - mae: 0.7697 - lr: 6.3096e-08\n",
            "Epoch 10/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3989 - mae: 0.7701\n",
            "Epoch 10: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3979 - mae: 0.7687 - lr: 7.9433e-08\n",
            "Epoch 11/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4000 - mae: 0.7728\n",
            "Epoch 11: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3982 - mae: 0.7694 - lr: 1.0000e-07\n",
            "Epoch 12/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3988 - mae: 0.7698\n",
            "Epoch 12: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3979 - mae: 0.7687 - lr: 1.2589e-07\n",
            "Epoch 13/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.3966 - mae: 0.7669\n",
            "Epoch 13: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.3983 - mae: 0.7692 - lr: 1.5849e-07\n",
            "Epoch 14/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3985 - mae: 0.7704\n",
            "Epoch 14: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3985 - mae: 0.7696 - lr: 1.9953e-07\n",
            "Epoch 15/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3951 - mae: 0.7645\n",
            "Epoch 15: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3976 - mae: 0.7683 - lr: 2.5119e-07\n",
            "Epoch 16/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3999 - mae: 0.7720\n",
            "Epoch 16: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3979 - mae: 0.7688 - lr: 3.1623e-07\n",
            "Epoch 17/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3998 - mae: 0.7719\n",
            "Epoch 17: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3977 - mae: 0.7686 - lr: 3.9811e-07\n",
            "Epoch 18/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3970 - mae: 0.7683\n",
            "Epoch 18: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3975 - mae: 0.7683 - lr: 5.0119e-07\n",
            "Epoch 19/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.3970 - mae: 0.7680\n",
            "Epoch 19: loss did not improve from 0.39735\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3975 - mae: 0.7683 - lr: 6.3096e-07\n",
            "Epoch 20/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3953 - mae: 0.7654\n",
            "Epoch 20: loss improved from 0.39735 to 0.39682, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3968 - mae: 0.7675 - lr: 7.9433e-07\n",
            "Epoch 21/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.3979 - mae: 0.7685\n",
            "Epoch 21: loss improved from 0.39682 to 0.39679, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3968 - mae: 0.7677 - lr: 1.0000e-06\n",
            "Epoch 22/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3975 - mae: 0.7695\n",
            "Epoch 22: loss improved from 0.39679 to 0.39477, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3948 - mae: 0.7651 - lr: 1.2589e-06\n",
            "Epoch 23/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3986 - mae: 0.7700\n",
            "Epoch 23: loss improved from 0.39477 to 0.39445, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3945 - mae: 0.7645 - lr: 1.5849e-06\n",
            "Epoch 24/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3950 - mae: 0.7652\n",
            "Epoch 24: loss improved from 0.39445 to 0.39355, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3935 - mae: 0.7636 - lr: 1.9953e-06\n",
            "Epoch 25/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3900 - mae: 0.7601\n",
            "Epoch 25: loss improved from 0.39355 to 0.39300, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3930 - mae: 0.7627 - lr: 2.5119e-06\n",
            "Epoch 26/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3911 - mae: 0.7603\n",
            "Epoch 26: loss improved from 0.39300 to 0.39114, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3911 - mae: 0.7603 - lr: 3.1623e-06\n",
            "Epoch 27/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3909 - mae: 0.7593\n",
            "Epoch 27: loss improved from 0.39114 to 0.38966, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3897 - mae: 0.7585 - lr: 3.9811e-06\n",
            "Epoch 28/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3854 - mae: 0.7521\n",
            "Epoch 28: loss improved from 0.38966 to 0.38752, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3875 - mae: 0.7560 - lr: 5.0119e-06\n",
            "Epoch 29/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3824 - mae: 0.7491\n",
            "Epoch 29: loss improved from 0.38752 to 0.38367, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3837 - mae: 0.7511 - lr: 6.3096e-06\n",
            "Epoch 30/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3816 - mae: 0.7489\n",
            "Epoch 30: loss improved from 0.38367 to 0.38061, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3806 - mae: 0.7475 - lr: 7.9433e-06\n",
            "Epoch 31/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3761 - mae: 0.7411\n",
            "Epoch 31: loss improved from 0.38061 to 0.37578, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3758 - mae: 0.7414 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3681 - mae: 0.7311\n",
            "Epoch 32: loss improved from 0.37578 to 0.37025, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3703 - mae: 0.7341 - lr: 1.2589e-05\n",
            "Epoch 33/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3646 - mae: 0.7268\n",
            "Epoch 33: loss improved from 0.37025 to 0.36463, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3646 - mae: 0.7268 - lr: 1.5849e-05\n",
            "Epoch 34/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3558 - mae: 0.7155\n",
            "Epoch 34: loss improved from 0.36463 to 0.35666, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3567 - mae: 0.7169 - lr: 1.9953e-05\n",
            "Epoch 35/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3471 - mae: 0.7035\n",
            "Epoch 35: loss improved from 0.35666 to 0.34671, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3467 - mae: 0.7037 - lr: 2.5119e-05\n",
            "Epoch 36/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3351 - mae: 0.6890\n",
            "Epoch 36: loss improved from 0.34671 to 0.33512, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3351 - mae: 0.6890 - lr: 3.1623e-05\n",
            "Epoch 37/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3232 - mae: 0.6733\n",
            "Epoch 37: loss improved from 0.33512 to 0.32233, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.3223 - mae: 0.6723 - lr: 3.9811e-05\n",
            "Epoch 38/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3024 - mae: 0.6465\n",
            "Epoch 38: loss improved from 0.32233 to 0.30712, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3071 - mae: 0.6522 - lr: 5.0119e-05\n",
            "Epoch 39/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.2929 - mae: 0.6323\n",
            "Epoch 39: loss improved from 0.30712 to 0.29008, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2901 - mae: 0.6294 - lr: 6.3096e-05\n",
            "Epoch 40/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.2756 - mae: 0.6102\n",
            "Epoch 40: loss improved from 0.29008 to 0.27312, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2731 - mae: 0.6060 - lr: 7.9433e-05\n",
            "Epoch 41/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2554 - mae: 0.5826\n",
            "Epoch 41: loss improved from 0.27312 to 0.25542, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2554 - mae: 0.5826 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2432 - mae: 0.5657\n",
            "Epoch 42: loss improved from 0.25542 to 0.23822, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2382 - mae: 0.5584 - lr: 1.2589e-04\n",
            "Epoch 43/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2265 - mae: 0.5419\n",
            "Epoch 43: loss improved from 0.23822 to 0.22410, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2241 - mae: 0.5396 - lr: 1.5849e-04\n",
            "Epoch 44/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2163 - mae: 0.5298\n",
            "Epoch 44: loss improved from 0.22410 to 0.21445, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2144 - mae: 0.5272 - lr: 1.9953e-04\n",
            "Epoch 45/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2072 - mae: 0.5176\n",
            "Epoch 45: loss improved from 0.21445 to 0.20839, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2084 - mae: 0.5197 - lr: 2.5119e-04\n",
            "Epoch 46/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2063 - mae: 0.5183\n",
            "Epoch 46: loss improved from 0.20839 to 0.20538, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2054 - mae: 0.5169 - lr: 3.1623e-04\n",
            "Epoch 47/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2038 - mae: 0.5147\n",
            "Epoch 47: loss improved from 0.20538 to 0.20454, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2045 - mae: 0.5158 - lr: 3.9811e-04\n",
            "Epoch 48/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2066 - mae: 0.5197\n",
            "Epoch 48: loss improved from 0.20454 to 0.20332, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2033 - mae: 0.5144 - lr: 5.0119e-04\n",
            "Epoch 49/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2036 - mae: 0.5157\n",
            "Epoch 49: loss improved from 0.20332 to 0.20299, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2030 - mae: 0.5141 - lr: 6.3096e-04\n",
            "Epoch 50/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2035 - mae: 0.5147\n",
            "Epoch 50: loss did not improve from 0.20299\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2036 - mae: 0.5149 - lr: 7.9433e-04\n",
            "Epoch 51/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2048 - mae: 0.5161\n",
            "Epoch 51: loss did not improve from 0.20299\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2030 - mae: 0.5138 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2027 - mae: 0.5128\n",
            "Epoch 52: loss improved from 0.20299 to 0.20265, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2026 - mae: 0.5130 - lr: 0.0013\n",
            "Epoch 53/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2028 - mae: 0.5138\n",
            "Epoch 53: loss improved from 0.20265 to 0.20231, saving model to poids.hdf5\n",
            "68/68 [==============================] - 2s 18ms/step - loss: 0.2023 - mae: 0.5131 - lr: 0.0016\n",
            "Epoch 54/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2007 - mae: 0.5106\n",
            "Epoch 54: loss improved from 0.20231 to 0.20146, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2015 - mae: 0.5118 - lr: 0.0020\n",
            "Epoch 55/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1999 - mae: 0.5092\n",
            "Epoch 55: loss improved from 0.20146 to 0.19989, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1999 - mae: 0.5092 - lr: 0.0025\n",
            "Epoch 56/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1984 - mae: 0.5066\n",
            "Epoch 56: loss did not improve from 0.19989\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2008 - mae: 0.5101 - lr: 0.0032\n",
            "Epoch 57/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1984 - mae: 0.5078\n",
            "Epoch 57: loss improved from 0.19989 to 0.19911, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1991 - mae: 0.5079 - lr: 0.0040\n",
            "Epoch 58/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1980 - mae: 0.5054\n",
            "Epoch 58: loss improved from 0.19911 to 0.19771, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1977 - mae: 0.5050 - lr: 0.0050\n",
            "Epoch 59/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1982 - mae: 0.5051\n",
            "Epoch 59: loss improved from 0.19771 to 0.19677, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1968 - mae: 0.5035 - lr: 0.0063\n",
            "Epoch 60/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1914 - mae: 0.4972\n",
            "Epoch 60: loss improved from 0.19677 to 0.19557, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1956 - mae: 0.5029 - lr: 0.0079\n",
            "Epoch 61/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1948 - mae: 0.5013\n",
            "Epoch 61: loss improved from 0.19557 to 0.19396, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1940 - mae: 0.5003 - lr: 0.0100\n",
            "Epoch 62/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1933 - mae: 0.4986\n",
            "Epoch 62: loss improved from 0.19396 to 0.19326, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1933 - mae: 0.4986 - lr: 0.0126\n",
            "Epoch 63/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1918 - mae: 0.4975\n",
            "Epoch 63: loss improved from 0.19326 to 0.18967, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1897 - mae: 0.4942 - lr: 0.0158\n",
            "Epoch 64/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1920 - mae: 0.4990\n",
            "Epoch 64: loss did not improve from 0.18967\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1898 - mae: 0.4950 - lr: 0.0200\n",
            "Epoch 65/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1861 - mae: 0.4884\n",
            "Epoch 65: loss improved from 0.18967 to 0.18509, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1851 - mae: 0.4871 - lr: 0.0251\n",
            "Epoch 66/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1878 - mae: 0.4921\n",
            "Epoch 66: loss did not improve from 0.18509\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1882 - mae: 0.4924 - lr: 0.0316\n",
            "Epoch 67/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1882 - mae: 0.4927\n",
            "Epoch 67: loss did not improve from 0.18509\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1890 - mae: 0.4941 - lr: 0.0398\n",
            "Epoch 68/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1858 - mae: 0.4893\n",
            "Epoch 68: loss did not improve from 0.18509\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1859 - mae: 0.4896 - lr: 0.0501\n",
            "Epoch 69/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1837 - mae: 0.4854\n",
            "Epoch 69: loss improved from 0.18509 to 0.18338, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1834 - mae: 0.4857 - lr: 0.0631\n",
            "Epoch 70/100\n",
            "62/68 [==========================>...] - ETA: 0s - loss: 0.1855 - mae: 0.4902\n",
            "Epoch 70: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1859 - mae: 0.4901 - lr: 0.0794\n",
            "Epoch 71/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1863 - mae: 0.4898\n",
            "Epoch 71: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1860 - mae: 0.4898 - lr: 0.1000\n",
            "Epoch 72/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1881 - mae: 0.4954\n",
            "Epoch 72: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1881 - mae: 0.4954 - lr: 0.1259\n",
            "Epoch 73/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1875 - mae: 0.4902\n",
            "Epoch 73: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1874 - mae: 0.4910 - lr: 0.1585\n",
            "Epoch 74/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1894 - mae: 0.4943\n",
            "Epoch 74: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1898 - mae: 0.4951 - lr: 0.1995\n",
            "Epoch 75/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1906 - mae: 0.4964\n",
            "Epoch 75: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1887 - mae: 0.4948 - lr: 0.2512\n",
            "Epoch 76/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1972 - mae: 0.5050\n",
            "Epoch 76: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1947 - mae: 0.5015 - lr: 0.3162\n",
            "Epoch 77/100\n",
            "62/68 [==========================>...] - ETA: 0s - loss: 0.1903 - mae: 0.4953\n",
            "Epoch 77: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1904 - mae: 0.4947 - lr: 0.3981\n",
            "Epoch 78/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2002 - mae: 0.5105\n",
            "Epoch 78: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2070 - mae: 0.5192 - lr: 0.5012\n",
            "Epoch 79/100\n",
            "62/68 [==========================>...] - ETA: 0s - loss: 0.2247 - mae: 0.5465\n",
            "Epoch 79: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2257 - mae: 0.5469 - lr: 0.6310\n",
            "Epoch 80/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2165 - mae: 0.5348\n",
            "Epoch 80: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2151 - mae: 0.5322 - lr: 0.7943\n",
            "Epoch 81/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2676 - mae: 0.5991\n",
            "Epoch 81: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2626 - mae: 0.5929 - lr: 1.0000\n",
            "Epoch 82/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2394 - mae: 0.5655\n",
            "Epoch 82: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2406 - mae: 0.5675 - lr: 1.2589\n",
            "Epoch 83/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2496 - mae: 0.5796\n",
            "Epoch 83: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2461 - mae: 0.5755 - lr: 1.5849\n",
            "Epoch 84/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 54.5226 - mae: 55.0125\n",
            "Epoch 84: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 55.4310 - mae: 55.9212 - lr: 1.9953\n",
            "Epoch 85/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 82.9726 - mae: 83.4726\n",
            "Epoch 85: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 84.6324 - mae: 85.1323 - lr: 2.5119\n",
            "Epoch 86/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 108.2552 - mae: 108.7525\n",
            "Epoch 86: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 113.9501 - mae: 114.4475 - lr: 3.1623\n",
            "Epoch 87/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 142.7250 - mae: 143.2248\n",
            "Epoch 87: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 139.4094 - mae: 139.9093 - lr: 3.9811\n",
            "Epoch 88/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 158.7941 - mae: 159.2935\n",
            "Epoch 88: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 159.0463 - mae: 159.5456 - lr: 5.0119\n",
            "Epoch 89/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 198.4509 - mae: 198.9509\n",
            "Epoch 89: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 198.1536 - mae: 198.6536 - lr: 6.3096\n",
            "Epoch 90/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 274.3827 - mae: 274.8827\n",
            "Epoch 90: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 280.5057 - mae: 281.0057 - lr: 7.9433\n",
            "Epoch 91/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 211.6345 - mae: 212.1345\n",
            "Epoch 91: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 214.4804 - mae: 214.9804 - lr: 10.0000\n",
            "Epoch 92/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 339.6838 - mae: 340.1838\n",
            "Epoch 92: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 363.1761 - mae: 363.6761 - lr: 12.5893\n",
            "Epoch 93/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 1025.1499 - mae: 1025.6499\n",
            "Epoch 93: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 1017.9042 - mae: 1018.4042 - lr: 15.8489\n",
            "Epoch 94/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 881.3427 - mae: 881.8411\n",
            "Epoch 94: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 863.2609 - mae: 863.7594 - lr: 19.9526\n",
            "Epoch 95/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 826.9169 - mae: 827.4168\n",
            "Epoch 95: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 837.3959 - mae: 837.8958 - lr: 25.1189\n",
            "Epoch 96/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 1098.2220 - mae: 1098.7220\n",
            "Epoch 96: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 1098.2220 - mae: 1098.7220 - lr: 31.6228\n",
            "Epoch 97/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 1523.4918 - mae: 1523.9918\n",
            "Epoch 97: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 1529.3953 - mae: 1529.8953 - lr: 39.8107\n",
            "Epoch 98/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 1669.3344 - mae: 1669.8344\n",
            "Epoch 98: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 1599.0607 - mae: 1599.5607 - lr: 50.1187\n",
            "Epoch 99/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 3587.7180 - mae: 3588.2180\n",
            "Epoch 99: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 3510.9468 - mae: 3511.4468 - lr: 63.0957\n",
            "Epoch 100/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 4779.8745 - mae: 4780.3745\n",
            "Epoch 100: loss did not improve from 0.18338\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 4927.7168 - mae: 4928.2168 - lr: 79.4328\n"
          ]
        }
      ],
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQsIpEOfUVrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "061f8dbe-67fa-496b-e19e-ffbf2b697841"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 0.5])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nous Constatons que notre modèle fait de l'overfiting. pour obtenir des meilleurs résultats, nous allons positionner l'optimiseur à 1e-1**"
      ],
      "metadata": {
        "id": "uAJvRu49WENP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXof-yChVHBZ"
      },
      "source": [
        "# 3. Entrainement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9xIawiy4Se8"
      },
      "outputs": [],
      "source": [
        "# Remise à zéro des états du modèle\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6TYJxqA4T4s"
      },
      "outputs": [],
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU_e8ciiVEUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86bcd8de-ed2c-4549-e725-c8c50d5d0944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     63/Unknown - 2s 11ms/step - loss: 0.1850 - mae: 0.4883\n",
            "Epoch 1: loss improved from inf to 0.18622, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 4s 28ms/step - loss: 0.1862 - mae: 0.4894 - val_loss: 0.1727 - val_mae: 0.4735\n",
            "Epoch 2/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1854 - mae: 0.4885\n",
            "Epoch 2: loss improved from 0.18622 to 0.18525, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1852 - mae: 0.4881 - val_loss: 0.1612 - val_mae: 0.4534\n",
            "Epoch 3/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1887 - mae: 0.4923\n",
            "Epoch 3: loss did not improve from 0.18525\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1873 - mae: 0.4898 - val_loss: 0.1616 - val_mae: 0.4538\n",
            "Epoch 4/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1888 - mae: 0.4954\n",
            "Epoch 4: loss did not improve from 0.18525\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1896 - mae: 0.4968 - val_loss: 0.1821 - val_mae: 0.4830\n",
            "Epoch 5/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1845 - mae: 0.4896\n",
            "Epoch 5: loss improved from 0.18525 to 0.18524, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1852 - mae: 0.4909 - val_loss: 0.1737 - val_mae: 0.4697\n",
            "Epoch 6/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1866 - mae: 0.4900\n",
            "Epoch 6: loss improved from 0.18524 to 0.18500, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1850 - mae: 0.4876 - val_loss: 0.1635 - val_mae: 0.4568\n",
            "Epoch 7/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1809 - mae: 0.4812\n",
            "Epoch 7: loss improved from 0.18500 to 0.18191, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1819 - mae: 0.4827 - val_loss: 0.1616 - val_mae: 0.4555\n",
            "Epoch 8/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1812 - mae: 0.4833\n",
            "Epoch 8: loss improved from 0.18191 to 0.18119, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1812 - mae: 0.4833 - val_loss: 0.1588 - val_mae: 0.4496\n",
            "Epoch 9/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1802 - mae: 0.4820\n",
            "Epoch 9: loss improved from 0.18119 to 0.18110, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1811 - mae: 0.4826 - val_loss: 0.1581 - val_mae: 0.4470\n",
            "Epoch 10/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1894 - mae: 0.4959\n",
            "Epoch 10: loss did not improve from 0.18110\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1890 - mae: 0.4955 - val_loss: 0.1703 - val_mae: 0.4702\n",
            "Epoch 11/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1794 - mae: 0.4788\n",
            "Epoch 11: loss improved from 0.18110 to 0.18032, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1803 - mae: 0.4806 - val_loss: 0.1582 - val_mae: 0.4468\n",
            "Epoch 12/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1794 - mae: 0.4793\n",
            "Epoch 12: loss did not improve from 0.18032\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1806 - mae: 0.4818 - val_loss: 0.1713 - val_mae: 0.4691\n",
            "Epoch 13/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1823 - mae: 0.4826\n",
            "Epoch 13: loss did not improve from 0.18032\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1807 - mae: 0.4806 - val_loss: 0.1578 - val_mae: 0.4488\n",
            "Epoch 14/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1790 - mae: 0.4773\n",
            "Epoch 14: loss improved from 0.18032 to 0.17861, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1786 - mae: 0.4772 - val_loss: 0.1580 - val_mae: 0.4456\n",
            "Epoch 15/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1838 - mae: 0.4880\n",
            "Epoch 15: loss did not improve from 0.17861\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1818 - mae: 0.4852 - val_loss: 0.1597 - val_mae: 0.4519\n",
            "Epoch 16/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4721\n",
            "Epoch 16: loss improved from 0.17861 to 0.17800, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1780 - mae: 0.4753 - val_loss: 0.1608 - val_mae: 0.4498\n",
            "Epoch 17/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1791 - mae: 0.4794\n",
            "Epoch 17: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1791 - mae: 0.4794 - val_loss: 0.1637 - val_mae: 0.4535\n",
            "Epoch 18/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1803 - mae: 0.4809\n",
            "Epoch 18: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1817 - mae: 0.4828 - val_loss: 0.1692 - val_mae: 0.4629\n",
            "Epoch 19/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1782 - mae: 0.4777\n",
            "Epoch 19: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1803 - mae: 0.4823 - val_loss: 0.1632 - val_mae: 0.4559\n",
            "Epoch 20/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1839 - mae: 0.4862\n",
            "Epoch 20: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1812 - mae: 0.4826 - val_loss: 0.1639 - val_mae: 0.4582\n",
            "Epoch 21/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1771 - mae: 0.4742\n",
            "Epoch 21: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1804 - mae: 0.4796 - val_loss: 0.1628 - val_mae: 0.4559\n",
            "Epoch 22/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1783 - mae: 0.4792\n",
            "Epoch 22: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1783 - mae: 0.4792 - val_loss: 0.1826 - val_mae: 0.4850\n",
            "Epoch 23/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1857 - mae: 0.4897\n",
            "Epoch 23: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1842 - mae: 0.4869 - val_loss: 0.1560 - val_mae: 0.4438\n",
            "Epoch 24/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1822 - mae: 0.4831\n",
            "Epoch 24: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1822 - mae: 0.4831 - val_loss: 0.1729 - val_mae: 0.4733\n",
            "Epoch 25/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1854 - mae: 0.4900\n",
            "Epoch 25: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1860 - mae: 0.4907 - val_loss: 0.1600 - val_mae: 0.4476\n",
            "Epoch 26/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1785 - mae: 0.4764\n",
            "Epoch 26: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1781 - mae: 0.4763 - val_loss: 0.1656 - val_mae: 0.4635\n",
            "Epoch 27/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1763 - mae: 0.4750\n",
            "Epoch 27: loss did not improve from 0.17800\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1794 - mae: 0.4795 - val_loss: 0.1591 - val_mae: 0.4495\n",
            "Epoch 28/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1767 - mae: 0.4740\n",
            "Epoch 28: loss improved from 0.17800 to 0.17685, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1769 - mae: 0.4749 - val_loss: 0.1565 - val_mae: 0.4433\n",
            "Epoch 29/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1802 - mae: 0.4806\n",
            "Epoch 29: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1793 - mae: 0.4789 - val_loss: 0.1606 - val_mae: 0.4518\n",
            "Epoch 30/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1854 - mae: 0.4886\n",
            "Epoch 30: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1836 - mae: 0.4847 - val_loss: 0.1681 - val_mae: 0.4612\n",
            "Epoch 31/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1783 - mae: 0.4775\n",
            "Epoch 31: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1798 - mae: 0.4795 - val_loss: 0.1597 - val_mae: 0.4514\n",
            "Epoch 32/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4755\n",
            "Epoch 32: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1777 - mae: 0.4774 - val_loss: 0.1600 - val_mae: 0.4476\n",
            "Epoch 33/500\n",
            "62/68 [==========================>...] - ETA: 0s - loss: 0.1807 - mae: 0.4811\n",
            "Epoch 33: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1814 - mae: 0.4826 - val_loss: 0.1704 - val_mae: 0.4650\n",
            "Epoch 34/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1826 - mae: 0.4833\n",
            "Epoch 34: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1826 - mae: 0.4833 - val_loss: 0.1707 - val_mae: 0.4656\n",
            "Epoch 35/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1847 - mae: 0.4877\n",
            "Epoch 35: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1863 - mae: 0.4899 - val_loss: 0.1595 - val_mae: 0.4478\n",
            "Epoch 36/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1779 - mae: 0.4785\n",
            "Epoch 36: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1794 - mae: 0.4805 - val_loss: 0.1668 - val_mae: 0.4621\n",
            "Epoch 37/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1809 - mae: 0.4843\n",
            "Epoch 37: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1809 - mae: 0.4843 - val_loss: 0.1583 - val_mae: 0.4497\n",
            "Epoch 38/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1793 - mae: 0.4780\n",
            "Epoch 38: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1793 - mae: 0.4780 - val_loss: 0.1596 - val_mae: 0.4510\n",
            "Epoch 39/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1811 - mae: 0.4839\n",
            "Epoch 39: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1795 - mae: 0.4816 - val_loss: 0.1593 - val_mae: 0.4464\n",
            "Epoch 40/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1805 - mae: 0.4812\n",
            "Epoch 40: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1805 - mae: 0.4812 - val_loss: 0.1665 - val_mae: 0.4660\n",
            "Epoch 41/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1799 - mae: 0.4811\n",
            "Epoch 41: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1789 - mae: 0.4786 - val_loss: 0.1824 - val_mae: 0.4837\n",
            "Epoch 42/500\n",
            "62/68 [==========================>...] - ETA: 0s - loss: 0.1803 - mae: 0.4812\n",
            "Epoch 42: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1792 - mae: 0.4802 - val_loss: 0.1572 - val_mae: 0.4456\n",
            "Epoch 43/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1781 - mae: 0.4771\n",
            "Epoch 43: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1774 - mae: 0.4762 - val_loss: 0.1604 - val_mae: 0.4488\n",
            "Epoch 44/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1819 - mae: 0.4802\n",
            "Epoch 44: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1820 - mae: 0.4805 - val_loss: 0.1575 - val_mae: 0.4469\n",
            "Epoch 45/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1797 - mae: 0.4788\n",
            "Epoch 45: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1790 - mae: 0.4783 - val_loss: 0.1584 - val_mae: 0.4476\n",
            "Epoch 46/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1797 - mae: 0.4788\n",
            "Epoch 46: loss did not improve from 0.17685\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1780 - mae: 0.4758 - val_loss: 0.1577 - val_mae: 0.4479\n",
            "Epoch 47/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1768 - mae: 0.4752\n",
            "Epoch 47: loss improved from 0.17685 to 0.17679, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1768 - mae: 0.4752 - val_loss: 0.1629 - val_mae: 0.4562\n",
            "Epoch 48/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1791 - mae: 0.4781\n",
            "Epoch 48: loss did not improve from 0.17679\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1784 - mae: 0.4774 - val_loss: 0.1575 - val_mae: 0.4444\n",
            "Epoch 49/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1789 - mae: 0.4787\n",
            "Epoch 49: loss did not improve from 0.17679\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1787 - mae: 0.4782 - val_loss: 0.1595 - val_mae: 0.4485\n",
            "Epoch 50/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1757 - mae: 0.4740\n",
            "Epoch 50: loss did not improve from 0.17679\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1768 - mae: 0.4748 - val_loss: 0.1600 - val_mae: 0.4480\n",
            "Epoch 51/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1797 - mae: 0.4787\n",
            "Epoch 51: loss did not improve from 0.17679\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1782 - mae: 0.4765 - val_loss: 0.1550 - val_mae: 0.4415\n",
            "Epoch 52/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1842 - mae: 0.4879\n",
            "Epoch 52: loss did not improve from 0.17679\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1830 - mae: 0.4866 - val_loss: 0.1581 - val_mae: 0.4478\n",
            "Epoch 53/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1768 - mae: 0.4754\n",
            "Epoch 53: loss improved from 0.17679 to 0.17568, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1757 - mae: 0.4741 - val_loss: 0.1577 - val_mae: 0.4453\n",
            "Epoch 54/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1778 - mae: 0.4759\n",
            "Epoch 54: loss did not improve from 0.17568\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1776 - mae: 0.4759 - val_loss: 0.1714 - val_mae: 0.4666\n",
            "Epoch 55/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4736\n",
            "Epoch 55: loss did not improve from 0.17568\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1769 - mae: 0.4740 - val_loss: 0.1587 - val_mae: 0.4474\n",
            "Epoch 56/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1771 - mae: 0.4742\n",
            "Epoch 56: loss did not improve from 0.17568\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1780 - mae: 0.4761 - val_loss: 0.1565 - val_mae: 0.4433\n",
            "Epoch 57/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1800 - mae: 0.4807\n",
            "Epoch 57: loss did not improve from 0.17568\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1800 - mae: 0.4807 - val_loss: 0.1547 - val_mae: 0.4422\n",
            "Epoch 58/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1798 - mae: 0.4796\n",
            "Epoch 58: loss did not improve from 0.17568\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1791 - mae: 0.4781 - val_loss: 0.1572 - val_mae: 0.4491\n",
            "Epoch 59/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1798 - mae: 0.4788\n",
            "Epoch 59: loss did not improve from 0.17568\n",
            "68/68 [==============================] - 3s 38ms/step - loss: 0.1783 - mae: 0.4758 - val_loss: 0.1673 - val_mae: 0.4620\n",
            "Epoch 60/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1796 - mae: 0.4792\n",
            "Epoch 60: loss did not improve from 0.17568\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1796 - mae: 0.4792 - val_loss: 0.1658 - val_mae: 0.4613\n",
            "Epoch 61/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1781 - mae: 0.4782\n",
            "Epoch 61: loss did not improve from 0.17568\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1782 - mae: 0.4775 - val_loss: 0.1561 - val_mae: 0.4429\n",
            "Epoch 62/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1810 - mae: 0.4800\n",
            "Epoch 62: loss did not improve from 0.17568\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1800 - mae: 0.4790 - val_loss: 0.1561 - val_mae: 0.4422\n",
            "Epoch 63/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1755 - mae: 0.4740\n",
            "Epoch 63: loss improved from 0.17568 to 0.17536, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1754 - mae: 0.4734 - val_loss: 0.1629 - val_mae: 0.4525\n",
            "Epoch 64/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1768 - mae: 0.4736\n",
            "Epoch 64: loss did not improve from 0.17536\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1762 - mae: 0.4734 - val_loss: 0.1655 - val_mae: 0.4571\n",
            "Epoch 65/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4754\n",
            "Epoch 65: loss did not improve from 0.17536\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1776 - mae: 0.4768 - val_loss: 0.1558 - val_mae: 0.4443\n",
            "Epoch 66/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 66: loss did not improve from 0.17536\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1765 - mae: 0.4750 - val_loss: 0.1607 - val_mae: 0.4510\n",
            "Epoch 67/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1806 - mae: 0.4827\n",
            "Epoch 67: loss did not improve from 0.17536\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1793 - mae: 0.4801 - val_loss: 0.1602 - val_mae: 0.4485\n",
            "Epoch 68/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4747\n",
            "Epoch 68: loss did not improve from 0.17536\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1765 - mae: 0.4748 - val_loss: 0.1558 - val_mae: 0.4441\n",
            "Epoch 69/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1773 - mae: 0.4775\n",
            "Epoch 69: loss did not improve from 0.17536\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1775 - mae: 0.4772 - val_loss: 0.1600 - val_mae: 0.4487\n",
            "Epoch 70/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1793 - mae: 0.4767\n",
            "Epoch 70: loss did not improve from 0.17536\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1776 - mae: 0.4751 - val_loss: 0.1710 - val_mae: 0.4714\n",
            "Epoch 71/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1765 - mae: 0.4747\n",
            "Epoch 71: loss did not improve from 0.17536\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1763 - mae: 0.4737 - val_loss: 0.1542 - val_mae: 0.4425\n",
            "Epoch 72/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4700\n",
            "Epoch 72: loss improved from 0.17536 to 0.17439, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1744 - mae: 0.4716 - val_loss: 0.1549 - val_mae: 0.4441\n",
            "Epoch 73/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1784 - mae: 0.4782\n",
            "Epoch 73: loss did not improve from 0.17439\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1784 - mae: 0.4782 - val_loss: 0.1586 - val_mae: 0.4457\n",
            "Epoch 74/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1783 - mae: 0.4755\n",
            "Epoch 74: loss did not improve from 0.17439\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1777 - mae: 0.4748 - val_loss: 0.1599 - val_mae: 0.4481\n",
            "Epoch 75/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1754 - mae: 0.4729\n",
            "Epoch 75: loss did not improve from 0.17439\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1764 - mae: 0.4736 - val_loss: 0.1576 - val_mae: 0.4482\n",
            "Epoch 76/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1773 - mae: 0.4754\n",
            "Epoch 76: loss did not improve from 0.17439\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1773 - mae: 0.4754 - val_loss: 0.1646 - val_mae: 0.4556\n",
            "Epoch 77/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4693\n",
            "Epoch 77: loss did not improve from 0.17439\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1745 - mae: 0.4700 - val_loss: 0.1656 - val_mae: 0.4611\n",
            "Epoch 78/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1773 - mae: 0.4758\n",
            "Epoch 78: loss did not improve from 0.17439\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1804 - mae: 0.4800 - val_loss: 0.1550 - val_mae: 0.4420\n",
            "Epoch 79/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4702\n",
            "Epoch 79: loss improved from 0.17439 to 0.17322, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1732 - mae: 0.4689 - val_loss: 0.1666 - val_mae: 0.4591\n",
            "Epoch 80/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4713\n",
            "Epoch 80: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1765 - mae: 0.4730 - val_loss: 0.1549 - val_mae: 0.4415\n",
            "Epoch 81/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4729\n",
            "Epoch 81: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1556 - val_mae: 0.4447\n",
            "Epoch 82/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1749 - mae: 0.4705\n",
            "Epoch 82: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1749 - mae: 0.4705 - val_loss: 0.1540 - val_mae: 0.4417\n",
            "Epoch 83/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1736 - mae: 0.4711\n",
            "Epoch 83: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1736 - mae: 0.4711 - val_loss: 0.1557 - val_mae: 0.4443\n",
            "Epoch 84/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1772 - mae: 0.4760\n",
            "Epoch 84: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1763 - mae: 0.4742 - val_loss: 0.1561 - val_mae: 0.4429\n",
            "Epoch 85/500\n",
            "62/68 [==========================>...] - ETA: 0s - loss: 0.1740 - mae: 0.4678\n",
            "Epoch 85: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1732 - mae: 0.4678 - val_loss: 0.1543 - val_mae: 0.4420\n",
            "Epoch 86/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4720\n",
            "Epoch 86: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1746 - mae: 0.4710 - val_loss: 0.1558 - val_mae: 0.4455\n",
            "Epoch 87/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1749 - mae: 0.4719\n",
            "Epoch 87: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1742 - mae: 0.4709 - val_loss: 0.1588 - val_mae: 0.4498\n",
            "Epoch 88/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4735\n",
            "Epoch 88: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1741 - mae: 0.4724 - val_loss: 0.1635 - val_mae: 0.4544\n",
            "Epoch 89/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1784 - mae: 0.4754\n",
            "Epoch 89: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1779 - mae: 0.4755 - val_loss: 0.1561 - val_mae: 0.4446\n",
            "Epoch 90/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4713\n",
            "Epoch 90: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1738 - mae: 0.4689 - val_loss: 0.1545 - val_mae: 0.4412\n",
            "Epoch 91/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1785 - mae: 0.4763\n",
            "Epoch 91: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1794 - mae: 0.4780 - val_loss: 0.1547 - val_mae: 0.4422\n",
            "Epoch 92/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4679\n",
            "Epoch 92: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1736 - mae: 0.4685 - val_loss: 0.1552 - val_mae: 0.4450\n",
            "Epoch 93/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1711 - mae: 0.4665\n",
            "Epoch 93: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1732 - mae: 0.4696 - val_loss: 0.1536 - val_mae: 0.4399\n",
            "Epoch 94/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1751 - mae: 0.4743\n",
            "Epoch 94: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1736 - mae: 0.4708 - val_loss: 0.1553 - val_mae: 0.4438\n",
            "Epoch 95/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1766 - mae: 0.4735\n",
            "Epoch 95: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1774 - mae: 0.4742 - val_loss: 0.1533 - val_mae: 0.4395\n",
            "Epoch 96/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1742 - mae: 0.4692\n",
            "Epoch 96: loss did not improve from 0.17322\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1735 - mae: 0.4685 - val_loss: 0.1538 - val_mae: 0.4406\n",
            "Epoch 97/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1723 - mae: 0.4672\n",
            "Epoch 97: loss improved from 0.17322 to 0.17192, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1719 - mae: 0.4677 - val_loss: 0.1570 - val_mae: 0.4470\n",
            "Epoch 98/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1691 - mae: 0.4621\n",
            "Epoch 98: loss did not improve from 0.17192\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1732 - mae: 0.4676 - val_loss: 0.1551 - val_mae: 0.4420\n",
            "Epoch 99/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1745 - mae: 0.4702\n",
            "Epoch 99: loss did not improve from 0.17192\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1745 - mae: 0.4700 - val_loss: 0.1576 - val_mae: 0.4460\n",
            "Epoch 100/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1767 - mae: 0.4745\n",
            "Epoch 100: loss did not improve from 0.17192\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1742 - mae: 0.4715 - val_loss: 0.1534 - val_mae: 0.4386\n",
            "Epoch 101/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4719\n",
            "Epoch 101: loss did not improve from 0.17192\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1753 - mae: 0.4720 - val_loss: 0.1531 - val_mae: 0.4402\n",
            "Epoch 102/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1759 - mae: 0.4736\n",
            "Epoch 102: loss did not improve from 0.17192\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1752 - mae: 0.4730 - val_loss: 0.1529 - val_mae: 0.4413\n",
            "Epoch 103/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1742 - mae: 0.4717\n",
            "Epoch 103: loss did not improve from 0.17192\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1726 - mae: 0.4689 - val_loss: 0.1542 - val_mae: 0.4410\n",
            "Epoch 104/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4732\n",
            "Epoch 104: loss did not improve from 0.17192\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1752 - mae: 0.4730 - val_loss: 0.1553 - val_mae: 0.4419\n",
            "Epoch 105/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1718 - mae: 0.4679\n",
            "Epoch 105: loss improved from 0.17192 to 0.17185, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1718 - mae: 0.4679 - val_loss: 0.1563 - val_mae: 0.4432\n",
            "Epoch 106/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4698\n",
            "Epoch 106: loss did not improve from 0.17185\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1738 - mae: 0.4695 - val_loss: 0.1550 - val_mae: 0.4426\n",
            "Epoch 107/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.4681\n",
            "Epoch 107: loss did not improve from 0.17185\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1720 - mae: 0.4671 - val_loss: 0.1552 - val_mae: 0.4428\n",
            "Epoch 108/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1740 - mae: 0.4706\n",
            "Epoch 108: loss did not improve from 0.17185\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1740 - mae: 0.4706 - val_loss: 0.1570 - val_mae: 0.4441\n",
            "Epoch 109/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1707 - mae: 0.4646\n",
            "Epoch 109: loss did not improve from 0.17185\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1725 - mae: 0.4676 - val_loss: 0.1549 - val_mae: 0.4415\n",
            "Epoch 110/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1720 - mae: 0.4660\n",
            "Epoch 110: loss improved from 0.17185 to 0.17184, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1718 - mae: 0.4666 - val_loss: 0.1543 - val_mae: 0.4403\n",
            "Epoch 111/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4703\n",
            "Epoch 111: loss did not improve from 0.17184\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1767 - mae: 0.4729 - val_loss: 0.1526 - val_mae: 0.4389\n",
            "Epoch 112/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1711 - mae: 0.4645\n",
            "Epoch 112: loss did not improve from 0.17184\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1723 - mae: 0.4663 - val_loss: 0.1596 - val_mae: 0.4499\n",
            "Epoch 113/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.4657\n",
            "Epoch 113: loss did not improve from 0.17184\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1726 - mae: 0.4671 - val_loss: 0.1603 - val_mae: 0.4500\n",
            "Epoch 114/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4694\n",
            "Epoch 114: loss did not improve from 0.17184\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1724 - mae: 0.4673 - val_loss: 0.1630 - val_mae: 0.4564\n",
            "Epoch 115/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1752 - mae: 0.4704\n",
            "Epoch 115: loss did not improve from 0.17184\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1751 - mae: 0.4719 - val_loss: 0.1551 - val_mae: 0.4417\n",
            "Epoch 116/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4687\n",
            "Epoch 116: loss did not improve from 0.17184\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1731 - mae: 0.4676 - val_loss: 0.1548 - val_mae: 0.4415\n",
            "Epoch 117/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4691\n",
            "Epoch 117: loss did not improve from 0.17184\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1736 - mae: 0.4698 - val_loss: 0.1609 - val_mae: 0.4485\n",
            "Epoch 118/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1721 - mae: 0.4663\n",
            "Epoch 118: loss did not improve from 0.17184\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1724 - mae: 0.4675 - val_loss: 0.1603 - val_mae: 0.4488\n",
            "Epoch 119/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1724 - mae: 0.4687\n",
            "Epoch 119: loss did not improve from 0.17184\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1728 - mae: 0.4696 - val_loss: 0.1620 - val_mae: 0.4573\n",
            "Epoch 120/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1721 - mae: 0.4686\n",
            "Epoch 120: loss improved from 0.17184 to 0.17092, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1709 - mae: 0.4671 - val_loss: 0.1559 - val_mae: 0.4428\n",
            "Epoch 121/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4698\n",
            "Epoch 121: loss did not improve from 0.17092\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1743 - mae: 0.4699 - val_loss: 0.1574 - val_mae: 0.4478\n",
            "Epoch 122/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1710 - mae: 0.4664\n",
            "Epoch 122: loss improved from 0.17092 to 0.16987, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1699 - mae: 0.4649 - val_loss: 0.1547 - val_mae: 0.4401\n",
            "Epoch 123/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1708 - mae: 0.4645\n",
            "Epoch 123: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1711 - mae: 0.4656 - val_loss: 0.1595 - val_mae: 0.4474\n",
            "Epoch 124/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1685 - mae: 0.4629\n",
            "Epoch 124: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1705 - mae: 0.4659 - val_loss: 0.1569 - val_mae: 0.4432\n",
            "Epoch 125/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1742 - mae: 0.4697\n",
            "Epoch 125: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1727 - mae: 0.4675 - val_loss: 0.1653 - val_mae: 0.4557\n",
            "Epoch 126/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4687\n",
            "Epoch 126: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1710 - mae: 0.4659 - val_loss: 0.1561 - val_mae: 0.4431\n",
            "Epoch 127/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1713 - mae: 0.4677\n",
            "Epoch 127: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1712 - mae: 0.4679 - val_loss: 0.1568 - val_mae: 0.4441\n",
            "Epoch 128/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4724\n",
            "Epoch 128: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1745 - mae: 0.4696 - val_loss: 0.1613 - val_mae: 0.4497\n",
            "Epoch 129/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4686\n",
            "Epoch 129: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1725 - mae: 0.4679 - val_loss: 0.1600 - val_mae: 0.4482\n",
            "Epoch 130/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1701 - mae: 0.4626\n",
            "Epoch 130: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1703 - mae: 0.4629 - val_loss: 0.1595 - val_mae: 0.4508\n",
            "Epoch 131/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1715 - mae: 0.4655\n",
            "Epoch 131: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1702 - mae: 0.4643 - val_loss: 0.1575 - val_mae: 0.4461\n",
            "Epoch 132/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1718 - mae: 0.4672\n",
            "Epoch 132: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1709 - mae: 0.4654 - val_loss: 0.1559 - val_mae: 0.4439\n",
            "Epoch 133/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1694 - mae: 0.4639\n",
            "Epoch 133: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1713 - mae: 0.4662 - val_loss: 0.1562 - val_mae: 0.4438\n",
            "Epoch 134/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1696 - mae: 0.4651\n",
            "Epoch 134: loss improved from 0.16987 to 0.16955, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1696 - mae: 0.4651 - val_loss: 0.1610 - val_mae: 0.4526\n",
            "Epoch 135/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1696 - mae: 0.4633\n",
            "Epoch 135: loss did not improve from 0.16955\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1702 - mae: 0.4651 - val_loss: 0.1597 - val_mae: 0.4470\n",
            "Epoch 136/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1702 - mae: 0.4660\n",
            "Epoch 136: loss did not improve from 0.16955\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1706 - mae: 0.4672 - val_loss: 0.1572 - val_mae: 0.4465\n",
            "Epoch 137/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1695 - mae: 0.4634\n",
            "Epoch 137: loss did not improve from 0.16955\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1708 - mae: 0.4647 - val_loss: 0.1599 - val_mae: 0.4478\n",
            "Epoch 138/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1695 - mae: 0.4651\n",
            "Epoch 138: loss did not improve from 0.16955\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1707 - mae: 0.4664 - val_loss: 0.1586 - val_mae: 0.4454\n",
            "Epoch 139/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1709 - mae: 0.4651\n",
            "Epoch 139: loss did not improve from 0.16955\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1698 - mae: 0.4637 - val_loss: 0.1649 - val_mae: 0.4618\n",
            "Epoch 140/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4698\n",
            "Epoch 140: loss did not improve from 0.16955\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1731 - mae: 0.4687 - val_loss: 0.1614 - val_mae: 0.4533\n",
            "Epoch 141/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1674 - mae: 0.4612\n",
            "Epoch 141: loss improved from 0.16955 to 0.16787, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1679 - mae: 0.4618 - val_loss: 0.1582 - val_mae: 0.4475\n",
            "Epoch 142/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4680\n",
            "Epoch 142: loss did not improve from 0.16787\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1734 - mae: 0.4689 - val_loss: 0.1612 - val_mae: 0.4489\n",
            "Epoch 143/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1689 - mae: 0.4628\n",
            "Epoch 143: loss did not improve from 0.16787\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1689 - mae: 0.4628 - val_loss: 0.1571 - val_mae: 0.4474\n",
            "Epoch 144/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1690 - mae: 0.4648\n",
            "Epoch 144: loss did not improve from 0.16787\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1698 - mae: 0.4659 - val_loss: 0.1609 - val_mae: 0.4516\n",
            "Epoch 145/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1695 - mae: 0.4640\n",
            "Epoch 145: loss did not improve from 0.16787\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1690 - mae: 0.4630 - val_loss: 0.1606 - val_mae: 0.4482\n",
            "Epoch 146/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1665 - mae: 0.4588\n",
            "Epoch 146: loss improved from 0.16787 to 0.16654, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1665 - mae: 0.4588 - val_loss: 0.1739 - val_mae: 0.4760\n",
            "Epoch 147/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1725 - mae: 0.4652\n",
            "Epoch 147: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1727 - mae: 0.4658 - val_loss: 0.1601 - val_mae: 0.4498\n",
            "Epoch 148/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1685 - mae: 0.4613\n",
            "Epoch 148: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1683 - mae: 0.4612 - val_loss: 0.1589 - val_mae: 0.4467\n",
            "Epoch 149/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1704 - mae: 0.4657\n",
            "Epoch 149: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1683 - mae: 0.4624 - val_loss: 0.1609 - val_mae: 0.4484\n",
            "Epoch 150/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1727 - mae: 0.4709\n",
            "Epoch 150: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1696 - mae: 0.4654 - val_loss: 0.1616 - val_mae: 0.4503\n",
            "Epoch 151/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1689 - mae: 0.4627\n",
            "Epoch 151: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1689 - mae: 0.4627 - val_loss: 0.1637 - val_mae: 0.4529\n",
            "Epoch 152/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1726 - mae: 0.4690\n",
            "Epoch 152: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1721 - mae: 0.4686 - val_loss: 0.1618 - val_mae: 0.4510\n",
            "Epoch 153/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1666 - mae: 0.4595\n",
            "Epoch 153: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1674 - mae: 0.4608 - val_loss: 0.1604 - val_mae: 0.4488\n",
            "Epoch 154/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1678 - mae: 0.4614\n",
            "Epoch 154: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1669 - mae: 0.4599 - val_loss: 0.1596 - val_mae: 0.4514\n",
            "Epoch 155/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1687 - mae: 0.4617\n",
            "Epoch 155: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1677 - mae: 0.4603 - val_loss: 0.1705 - val_mae: 0.4698\n",
            "Epoch 156/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1680 - mae: 0.4633\n",
            "Epoch 156: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1684 - mae: 0.4643 - val_loss: 0.1620 - val_mae: 0.4518\n",
            "Epoch 157/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1680 - mae: 0.4621\n",
            "Epoch 157: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1680 - mae: 0.4621 - val_loss: 0.1622 - val_mae: 0.4524\n",
            "Epoch 158/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1654 - mae: 0.4546\n",
            "Epoch 158: loss improved from 0.16654 to 0.16610, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1661 - mae: 0.4555 - val_loss: 0.1595 - val_mae: 0.4493\n",
            "Epoch 159/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1681 - mae: 0.4613\n",
            "Epoch 159: loss did not improve from 0.16610\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1685 - mae: 0.4623 - val_loss: 0.1603 - val_mae: 0.4487\n",
            "Epoch 160/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1674 - mae: 0.4627\n",
            "Epoch 160: loss did not improve from 0.16610\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1691 - mae: 0.4648 - val_loss: 0.1612 - val_mae: 0.4505\n",
            "Epoch 161/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1675 - mae: 0.4614\n",
            "Epoch 161: loss did not improve from 0.16610\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1667 - mae: 0.4602 - val_loss: 0.1602 - val_mae: 0.4491\n",
            "Epoch 162/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1667 - mae: 0.4604\n",
            "Epoch 162: loss did not improve from 0.16610\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1664 - mae: 0.4597 - val_loss: 0.1615 - val_mae: 0.4516\n",
            "Epoch 163/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1672 - mae: 0.4617\n",
            "Epoch 163: loss did not improve from 0.16610\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1682 - mae: 0.4615 - val_loss: 0.1613 - val_mae: 0.4499\n",
            "Epoch 164/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1676 - mae: 0.4605\n",
            "Epoch 164: loss did not improve from 0.16610\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1688 - mae: 0.4620 - val_loss: 0.1632 - val_mae: 0.4538\n",
            "Epoch 165/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1650 - mae: 0.4568\n",
            "Epoch 165: loss improved from 0.16610 to 0.16530, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1653 - mae: 0.4584 - val_loss: 0.1653 - val_mae: 0.4556\n",
            "Epoch 166/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1694 - mae: 0.4626\n",
            "Epoch 166: loss did not improve from 0.16530\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1690 - mae: 0.4628 - val_loss: 0.1724 - val_mae: 0.4715\n",
            "Epoch 167/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1659 - mae: 0.4587\n",
            "Epoch 167: loss did not improve from 0.16530\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1664 - mae: 0.4601 - val_loss: 0.1645 - val_mae: 0.4544\n",
            "Epoch 168/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1662 - mae: 0.4616\n",
            "Epoch 168: loss did not improve from 0.16530\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1659 - mae: 0.4613 - val_loss: 0.1725 - val_mae: 0.4713\n",
            "Epoch 169/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1655 - mae: 0.4572\n",
            "Epoch 169: loss improved from 0.16530 to 0.16474, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1647 - mae: 0.4562 - val_loss: 0.1659 - val_mae: 0.4564\n",
            "Epoch 170/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1634 - mae: 0.4576\n",
            "Epoch 170: loss improved from 0.16474 to 0.16398, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1640 - mae: 0.4579 - val_loss: 0.1684 - val_mae: 0.4598\n",
            "Epoch 171/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1649 - mae: 0.4562\n",
            "Epoch 171: loss did not improve from 0.16398\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1667 - mae: 0.4584 - val_loss: 0.1646 - val_mae: 0.4539\n",
            "Epoch 172/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1649 - mae: 0.4563\n",
            "Epoch 172: loss did not improve from 0.16398\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1649 - mae: 0.4563 - val_loss: 0.1627 - val_mae: 0.4512\n",
            "Epoch 173/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1659 - mae: 0.4582\n",
            "Epoch 173: loss did not improve from 0.16398\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1659 - mae: 0.4582 - val_loss: 0.1687 - val_mae: 0.4616\n",
            "Epoch 174/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1652 - mae: 0.4589\n",
            "Epoch 174: loss improved from 0.16398 to 0.16378, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1638 - mae: 0.4561 - val_loss: 0.1659 - val_mae: 0.4560\n",
            "Epoch 175/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1644 - mae: 0.4565\n",
            "Epoch 175: loss improved from 0.16378 to 0.16267, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1627 - mae: 0.4540 - val_loss: 0.1635 - val_mae: 0.4536\n",
            "Epoch 176/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1590 - mae: 0.4503\n",
            "Epoch 176: loss improved from 0.16267 to 0.16167, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1617 - mae: 0.4534 - val_loss: 0.1668 - val_mae: 0.4609\n",
            "Epoch 177/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1645 - mae: 0.4598\n",
            "Epoch 177: loss did not improve from 0.16167\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1645 - mae: 0.4598 - val_loss: 0.1677 - val_mae: 0.4585\n",
            "Epoch 178/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1608 - mae: 0.4508\n",
            "Epoch 178: loss improved from 0.16167 to 0.16026, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1603 - mae: 0.4503 - val_loss: 0.1643 - val_mae: 0.4557\n",
            "Epoch 179/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1619 - mae: 0.4545\n",
            "Epoch 179: loss did not improve from 0.16026\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1620 - mae: 0.4549 - val_loss: 0.1790 - val_mae: 0.4793\n",
            "Epoch 180/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1656 - mae: 0.4599\n",
            "Epoch 180: loss did not improve from 0.16026\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1651 - mae: 0.4595 - val_loss: 0.1658 - val_mae: 0.4577\n",
            "Epoch 181/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1651 - mae: 0.4625\n",
            "Epoch 181: loss did not improve from 0.16026\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1647 - mae: 0.4612 - val_loss: 0.1663 - val_mae: 0.4607\n",
            "Epoch 182/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1659 - mae: 0.4609\n",
            "Epoch 182: loss did not improve from 0.16026\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1660 - mae: 0.4609 - val_loss: 0.1721 - val_mae: 0.4661\n",
            "Epoch 183/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1589 - mae: 0.4498\n",
            "Epoch 183: loss did not improve from 0.16026\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1611 - mae: 0.4514 - val_loss: 0.1765 - val_mae: 0.4777\n",
            "Epoch 184/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1576 - mae: 0.4472\n",
            "Epoch 184: loss improved from 0.16026 to 0.15930, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1593 - mae: 0.4495 - val_loss: 0.1690 - val_mae: 0.4631\n",
            "Epoch 185/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1585 - mae: 0.4487\n",
            "Epoch 185: loss did not improve from 0.15930\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1594 - mae: 0.4497 - val_loss: 0.1688 - val_mae: 0.4638\n",
            "Epoch 186/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1591 - mae: 0.4487\n",
            "Epoch 186: loss improved from 0.15930 to 0.15910, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1591 - mae: 0.4487 - val_loss: 0.1723 - val_mae: 0.4684\n",
            "Epoch 187/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1609 - mae: 0.4545\n",
            "Epoch 187: loss did not improve from 0.15910\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1604 - mae: 0.4528 - val_loss: 0.1700 - val_mae: 0.4629\n",
            "Epoch 188/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1587 - mae: 0.4496\n",
            "Epoch 188: loss improved from 0.15910 to 0.15869, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1587 - mae: 0.4496 - val_loss: 0.1679 - val_mae: 0.4611\n",
            "Epoch 189/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1570 - mae: 0.4478\n",
            "Epoch 189: loss did not improve from 0.15869\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1589 - mae: 0.4505 - val_loss: 0.1735 - val_mae: 0.4703\n",
            "Epoch 190/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1615 - mae: 0.4544\n",
            "Epoch 190: loss did not improve from 0.15869\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1609 - mae: 0.4534 - val_loss: 0.1737 - val_mae: 0.4685\n",
            "Epoch 191/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1601 - mae: 0.4522\n",
            "Epoch 191: loss improved from 0.15869 to 0.15833, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1583 - mae: 0.4493 - val_loss: 0.1694 - val_mae: 0.4598\n",
            "Epoch 192/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1593 - mae: 0.4498\n",
            "Epoch 192: loss did not improve from 0.15833\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1611 - mae: 0.4521 - val_loss: 0.1745 - val_mae: 0.4679\n",
            "Epoch 193/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1613 - mae: 0.4522\n",
            "Epoch 193: loss did not improve from 0.15833\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1626 - mae: 0.4538 - val_loss: 0.1717 - val_mae: 0.4639\n",
            "Epoch 194/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1572 - mae: 0.4465\n",
            "Epoch 194: loss improved from 0.15833 to 0.15777, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1578 - mae: 0.4480 - val_loss: 0.1801 - val_mae: 0.4813\n",
            "Epoch 195/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1572 - mae: 0.4490\n",
            "Epoch 195: loss improved from 0.15777 to 0.15718, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1572 - mae: 0.4490 - val_loss: 0.1823 - val_mae: 0.4837\n",
            "Epoch 196/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1549 - mae: 0.4439\n",
            "Epoch 196: loss improved from 0.15718 to 0.15494, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1549 - mae: 0.4439 - val_loss: 0.1692 - val_mae: 0.4602\n",
            "Epoch 197/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1559 - mae: 0.4456\n",
            "Epoch 197: loss did not improve from 0.15494\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1555 - mae: 0.4448 - val_loss: 0.1741 - val_mae: 0.4682\n",
            "Epoch 198/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1569 - mae: 0.4487\n",
            "Epoch 198: loss did not improve from 0.15494\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1573 - mae: 0.4488 - val_loss: 0.1700 - val_mae: 0.4626\n",
            "Epoch 199/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1536 - mae: 0.4412\n",
            "Epoch 199: loss did not improve from 0.15494\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1558 - mae: 0.4449 - val_loss: 0.1728 - val_mae: 0.4642\n",
            "Epoch 200/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1555 - mae: 0.4461\n",
            "Epoch 200: loss did not improve from 0.15494\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1557 - mae: 0.4466 - val_loss: 0.1719 - val_mae: 0.4678\n",
            "Epoch 201/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1536 - mae: 0.4455\n",
            "Epoch 201: loss did not improve from 0.15494\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1564 - mae: 0.4489 - val_loss: 0.1725 - val_mae: 0.4664\n",
            "Epoch 202/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1549 - mae: 0.4470\n",
            "Epoch 202: loss did not improve from 0.15494\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1559 - mae: 0.4487 - val_loss: 0.1744 - val_mae: 0.4707\n",
            "Epoch 203/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1555 - mae: 0.4460\n",
            "Epoch 203: loss did not improve from 0.15494\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1550 - mae: 0.4450 - val_loss: 0.1778 - val_mae: 0.4753\n",
            "Epoch 204/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1545 - mae: 0.4448\n",
            "Epoch 204: loss did not improve from 0.15494\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1555 - mae: 0.4463 - val_loss: 0.1753 - val_mae: 0.4703\n",
            "Epoch 205/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1563 - mae: 0.4471\n",
            "Epoch 205: loss improved from 0.15494 to 0.15474, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1547 - mae: 0.4446 - val_loss: 0.1797 - val_mae: 0.4749\n",
            "Epoch 206/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1545 - mae: 0.4449\n",
            "Epoch 206: loss improved from 0.15474 to 0.15446, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1545 - mae: 0.4449 - val_loss: 0.1694 - val_mae: 0.4608\n",
            "Epoch 207/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1539 - mae: 0.4430\n",
            "Epoch 207: loss improved from 0.15446 to 0.15386, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1539 - mae: 0.4430 - val_loss: 0.1735 - val_mae: 0.4692\n",
            "Epoch 208/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1512 - mae: 0.4380\n",
            "Epoch 208: loss improved from 0.15386 to 0.15190, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1519 - mae: 0.4390 - val_loss: 0.1747 - val_mae: 0.4733\n",
            "Epoch 209/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1526 - mae: 0.4448\n",
            "Epoch 209: loss improved from 0.15190 to 0.15145, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1515 - mae: 0.4426 - val_loss: 0.1707 - val_mae: 0.4628\n",
            "Epoch 210/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1493 - mae: 0.4359\n",
            "Epoch 210: loss improved from 0.15145 to 0.14904, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1490 - mae: 0.4359 - val_loss: 0.1818 - val_mae: 0.4783\n",
            "Epoch 211/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1524 - mae: 0.4403\n",
            "Epoch 211: loss did not improve from 0.14904\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1523 - mae: 0.4393 - val_loss: 0.1762 - val_mae: 0.4729\n",
            "Epoch 212/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1524 - mae: 0.4409\n",
            "Epoch 212: loss did not improve from 0.14904\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1530 - mae: 0.4417 - val_loss: 0.1802 - val_mae: 0.4780\n",
            "Epoch 213/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1499 - mae: 0.4377\n",
            "Epoch 213: loss did not improve from 0.14904\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1499 - mae: 0.4377 - val_loss: 0.1797 - val_mae: 0.4800\n",
            "Epoch 214/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1484 - mae: 0.4354\n",
            "Epoch 214: loss did not improve from 0.14904\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1501 - mae: 0.4381 - val_loss: 0.1848 - val_mae: 0.4859\n",
            "Epoch 215/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1477 - mae: 0.4344\n",
            "Epoch 215: loss improved from 0.14904 to 0.14859, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1486 - mae: 0.4353 - val_loss: 0.1788 - val_mae: 0.4744\n",
            "Epoch 216/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1456 - mae: 0.4278\n",
            "Epoch 216: loss improved from 0.14859 to 0.14788, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1479 - mae: 0.4320 - val_loss: 0.1811 - val_mae: 0.4806\n",
            "Epoch 217/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1451 - mae: 0.4299\n",
            "Epoch 217: loss improved from 0.14788 to 0.14542, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1454 - mae: 0.4306 - val_loss: 0.1790 - val_mae: 0.4737\n",
            "Epoch 218/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1488 - mae: 0.4339\n",
            "Epoch 218: loss did not improve from 0.14542\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1485 - mae: 0.4332 - val_loss: 0.1787 - val_mae: 0.4734\n",
            "Epoch 219/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1484 - mae: 0.4323\n",
            "Epoch 219: loss did not improve from 0.14542\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1484 - mae: 0.4323 - val_loss: 0.1819 - val_mae: 0.4827\n",
            "Epoch 220/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1443 - mae: 0.4284\n",
            "Epoch 220: loss improved from 0.14542 to 0.14393, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1439 - mae: 0.4275 - val_loss: 0.1761 - val_mae: 0.4741\n",
            "Epoch 221/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1492 - mae: 0.4370\n",
            "Epoch 221: loss did not improve from 0.14393\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1482 - mae: 0.4352 - val_loss: 0.1822 - val_mae: 0.4822\n",
            "Epoch 222/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1471 - mae: 0.4319\n",
            "Epoch 222: loss did not improve from 0.14393\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1465 - mae: 0.4310 - val_loss: 0.1880 - val_mae: 0.4919\n",
            "Epoch 223/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1457 - mae: 0.4309\n",
            "Epoch 223: loss did not improve from 0.14393\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1458 - mae: 0.4307 - val_loss: 0.1878 - val_mae: 0.4883\n",
            "Epoch 224/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1432 - mae: 0.4252\n",
            "Epoch 224: loss improved from 0.14393 to 0.14303, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1430 - mae: 0.4244 - val_loss: 0.1844 - val_mae: 0.4829\n",
            "Epoch 225/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1447 - mae: 0.4288\n",
            "Epoch 225: loss did not improve from 0.14303\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1450 - mae: 0.4293 - val_loss: 0.1897 - val_mae: 0.4924\n",
            "Epoch 226/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1454 - mae: 0.4279\n",
            "Epoch 226: loss did not improve from 0.14303\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1454 - mae: 0.4280 - val_loss: 0.1816 - val_mae: 0.4819\n",
            "Epoch 227/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1418 - mae: 0.4249\n",
            "Epoch 227: loss did not improve from 0.14303\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1432 - mae: 0.4270 - val_loss: 0.1833 - val_mae: 0.4815\n",
            "Epoch 228/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1408 - mae: 0.4209\n",
            "Epoch 228: loss did not improve from 0.14303\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1438 - mae: 0.4258 - val_loss: 0.1822 - val_mae: 0.4821\n",
            "Epoch 229/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1451 - mae: 0.4289\n",
            "Epoch 229: loss did not improve from 0.14303\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1442 - mae: 0.4277 - val_loss: 0.1778 - val_mae: 0.4774\n",
            "Epoch 230/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1398 - mae: 0.4194\n",
            "Epoch 230: loss improved from 0.14303 to 0.13994, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1399 - mae: 0.4204 - val_loss: 0.1788 - val_mae: 0.4750\n",
            "Epoch 231/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1416 - mae: 0.4221\n",
            "Epoch 231: loss did not improve from 0.13994\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1410 - mae: 0.4206 - val_loss: 0.1775 - val_mae: 0.4740\n",
            "Epoch 232/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1359 - mae: 0.4166\n",
            "Epoch 232: loss improved from 0.13994 to 0.13639, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1364 - mae: 0.4172 - val_loss: 0.1904 - val_mae: 0.4950\n",
            "Epoch 233/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1412 - mae: 0.4251\n",
            "Epoch 233: loss did not improve from 0.13639\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1404 - mae: 0.4233 - val_loss: 0.1858 - val_mae: 0.4829\n",
            "Epoch 234/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1378 - mae: 0.4178\n",
            "Epoch 234: loss did not improve from 0.13639\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1378 - mae: 0.4178 - val_loss: 0.1854 - val_mae: 0.4869\n",
            "Epoch 235/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1404 - mae: 0.4203\n",
            "Epoch 235: loss did not improve from 0.13639\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1395 - mae: 0.4187 - val_loss: 0.1901 - val_mae: 0.4939\n",
            "Epoch 236/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1385 - mae: 0.4203\n",
            "Epoch 236: loss did not improve from 0.13639\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1376 - mae: 0.4190 - val_loss: 0.1878 - val_mae: 0.4893\n",
            "Epoch 237/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1355 - mae: 0.4148\n",
            "Epoch 237: loss improved from 0.13639 to 0.13553, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1355 - mae: 0.4148 - val_loss: 0.1956 - val_mae: 0.5038\n",
            "Epoch 238/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1369 - mae: 0.4148\n",
            "Epoch 238: loss did not improve from 0.13553\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1367 - mae: 0.4148 - val_loss: 0.1944 - val_mae: 0.5022\n",
            "Epoch 239/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1380 - mae: 0.4164\n",
            "Epoch 239: loss did not improve from 0.13553\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1383 - mae: 0.4162 - val_loss: 0.1980 - val_mae: 0.5089\n",
            "Epoch 240/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1306 - mae: 0.4052\n",
            "Epoch 240: loss improved from 0.13553 to 0.13312, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1331 - mae: 0.4099 - val_loss: 0.1818 - val_mae: 0.4830\n",
            "Epoch 241/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1335 - mae: 0.4105\n",
            "Epoch 241: loss improved from 0.13312 to 0.13307, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1331 - mae: 0.4098 - val_loss: 0.1937 - val_mae: 0.5018\n",
            "Epoch 242/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1314 - mae: 0.4050\n",
            "Epoch 242: loss improved from 0.13307 to 0.13232, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1323 - mae: 0.4066 - val_loss: 0.1934 - val_mae: 0.4963\n",
            "Epoch 243/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1316 - mae: 0.4071\n",
            "Epoch 243: loss improved from 0.13232 to 0.13199, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1320 - mae: 0.4066 - val_loss: 0.2017 - val_mae: 0.5123\n",
            "Epoch 244/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1341 - mae: 0.4137\n",
            "Epoch 244: loss did not improve from 0.13199\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1330 - mae: 0.4113 - val_loss: 0.1972 - val_mae: 0.5047\n",
            "Epoch 245/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1309 - mae: 0.4074\n",
            "Epoch 245: loss improved from 0.13199 to 0.13085, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1309 - mae: 0.4074 - val_loss: 0.2047 - val_mae: 0.5175\n",
            "Epoch 246/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1337 - mae: 0.4111\n",
            "Epoch 246: loss did not improve from 0.13085\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1334 - mae: 0.4105 - val_loss: 0.2086 - val_mae: 0.5262\n",
            "Epoch 247/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1328 - mae: 0.4102\n",
            "Epoch 247: loss did not improve from 0.13085\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1322 - mae: 0.4091 - val_loss: 0.1918 - val_mae: 0.4983\n",
            "Epoch 248/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1245 - mae: 0.3968\n",
            "Epoch 248: loss improved from 0.13085 to 0.12575, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1257 - mae: 0.3987 - val_loss: 0.1977 - val_mae: 0.5062\n",
            "Epoch 249/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1290 - mae: 0.4050\n",
            "Epoch 249: loss did not improve from 0.12575\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1286 - mae: 0.4046 - val_loss: 0.2060 - val_mae: 0.5167\n",
            "Epoch 250/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1270 - mae: 0.3993\n",
            "Epoch 250: loss did not improve from 0.12575\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1275 - mae: 0.3996 - val_loss: 0.2084 - val_mae: 0.5214\n",
            "Epoch 251/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1266 - mae: 0.3995\n",
            "Epoch 251: loss did not improve from 0.12575\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1260 - mae: 0.3983 - val_loss: 0.2045 - val_mae: 0.5139\n",
            "Epoch 252/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1218 - mae: 0.3919\n",
            "Epoch 252: loss improved from 0.12575 to 0.12112, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1211 - mae: 0.3912 - val_loss: 0.2156 - val_mae: 0.5305\n",
            "Epoch 253/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1226 - mae: 0.3906\n",
            "Epoch 253: loss did not improve from 0.12112\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1229 - mae: 0.3909 - val_loss: 0.2138 - val_mae: 0.5275\n",
            "Epoch 254/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1290 - mae: 0.4061\n",
            "Epoch 254: loss did not improve from 0.12112\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1296 - mae: 0.4067 - val_loss: 0.1936 - val_mae: 0.4980\n",
            "Epoch 255/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1191 - mae: 0.3865\n",
            "Epoch 255: loss improved from 0.12112 to 0.11974, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1197 - mae: 0.3872 - val_loss: 0.2091 - val_mae: 0.5231\n",
            "Epoch 256/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1222 - mae: 0.3915\n",
            "Epoch 256: loss did not improve from 0.11974\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1229 - mae: 0.3927 - val_loss: 0.2040 - val_mae: 0.5110\n",
            "Epoch 257/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1225 - mae: 0.3929\n",
            "Epoch 257: loss did not improve from 0.11974\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1218 - mae: 0.3919 - val_loss: 0.2052 - val_mae: 0.5158\n",
            "Epoch 258/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1152 - mae: 0.3800\n",
            "Epoch 258: loss improved from 0.11974 to 0.11520, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1152 - mae: 0.3800 - val_loss: 0.2155 - val_mae: 0.5290\n",
            "Epoch 259/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1172 - mae: 0.3852\n",
            "Epoch 259: loss did not improve from 0.11520\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1172 - mae: 0.3852 - val_loss: 0.2095 - val_mae: 0.5219\n",
            "Epoch 260/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1131 - mae: 0.3796\n",
            "Epoch 260: loss improved from 0.11520 to 0.11352, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1135 - mae: 0.3801 - val_loss: 0.2112 - val_mae: 0.5276\n",
            "Epoch 261/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1105 - mae: 0.3708\n",
            "Epoch 261: loss improved from 0.11352 to 0.11071, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1107 - mae: 0.3713 - val_loss: 0.2254 - val_mae: 0.5439\n",
            "Epoch 262/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1098 - mae: 0.3694\n",
            "Epoch 262: loss improved from 0.11071 to 0.10892, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1089 - mae: 0.3675 - val_loss: 0.2169 - val_mae: 0.5335\n",
            "Epoch 263/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1083 - mae: 0.3702\n",
            "Epoch 263: loss improved from 0.10892 to 0.10841, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1084 - mae: 0.3707 - val_loss: 0.2193 - val_mae: 0.5367\n",
            "Epoch 264/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1097 - mae: 0.3675\n",
            "Epoch 264: loss did not improve from 0.10841\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1099 - mae: 0.3683 - val_loss: 0.2149 - val_mae: 0.5325\n",
            "Epoch 265/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1070 - mae: 0.3647\n",
            "Epoch 265: loss improved from 0.10841 to 0.10782, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1078 - mae: 0.3662 - val_loss: 0.2074 - val_mae: 0.5241\n",
            "Epoch 266/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1034 - mae: 0.3571\n",
            "Epoch 266: loss improved from 0.10782 to 0.10338, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1034 - mae: 0.3571 - val_loss: 0.2137 - val_mae: 0.5314\n",
            "Epoch 267/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1110 - mae: 0.3695\n",
            "Epoch 267: loss did not improve from 0.10338\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1105 - mae: 0.3687 - val_loss: 0.2172 - val_mae: 0.5356\n",
            "Epoch 268/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1070 - mae: 0.3683\n",
            "Epoch 268: loss did not improve from 0.10338\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1071 - mae: 0.3683 - val_loss: 0.2391 - val_mae: 0.5624\n",
            "Epoch 269/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1099 - mae: 0.3721\n",
            "Epoch 269: loss did not improve from 0.10338\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1101 - mae: 0.3724 - val_loss: 0.2190 - val_mae: 0.5382\n",
            "Epoch 270/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0985 - mae: 0.3488\n",
            "Epoch 270: loss improved from 0.10338 to 0.09848, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0985 - mae: 0.3488 - val_loss: 0.2350 - val_mae: 0.5584\n",
            "Epoch 271/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0972 - mae: 0.3469\n",
            "Epoch 271: loss improved from 0.09848 to 0.09719, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0972 - mae: 0.3458 - val_loss: 0.2237 - val_mae: 0.5445\n",
            "Epoch 272/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0968 - mae: 0.3489\n",
            "Epoch 272: loss did not improve from 0.09719\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0973 - mae: 0.3497 - val_loss: 0.2499 - val_mae: 0.5803\n",
            "Epoch 273/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1007 - mae: 0.3536\n",
            "Epoch 273: loss did not improve from 0.09719\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1007 - mae: 0.3536 - val_loss: 0.2376 - val_mae: 0.5608\n",
            "Epoch 274/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0975 - mae: 0.3488\n",
            "Epoch 274: loss improved from 0.09719 to 0.09659, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0966 - mae: 0.3475 - val_loss: 0.2266 - val_mae: 0.5473\n",
            "Epoch 275/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0925 - mae: 0.3393\n",
            "Epoch 275: loss improved from 0.09659 to 0.09181, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0918 - mae: 0.3382 - val_loss: 0.2354 - val_mae: 0.5627\n",
            "Epoch 276/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0974 - mae: 0.3502\n",
            "Epoch 276: loss did not improve from 0.09181\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0974 - mae: 0.3502 - val_loss: 0.2279 - val_mae: 0.5486\n",
            "Epoch 277/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0899 - mae: 0.3388\n",
            "Epoch 277: loss improved from 0.09181 to 0.08987, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0899 - mae: 0.3386 - val_loss: 0.2371 - val_mae: 0.5621\n",
            "Epoch 278/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0903 - mae: 0.3357\n",
            "Epoch 278: loss did not improve from 0.08987\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0910 - mae: 0.3365 - val_loss: 0.2447 - val_mae: 0.5726\n",
            "Epoch 279/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0879 - mae: 0.3313\n",
            "Epoch 279: loss improved from 0.08987 to 0.08793, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0879 - mae: 0.3313 - val_loss: 0.2522 - val_mae: 0.5801\n",
            "Epoch 280/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0866 - mae: 0.3298\n",
            "Epoch 280: loss improved from 0.08793 to 0.08696, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0870 - mae: 0.3308 - val_loss: 0.2476 - val_mae: 0.5744\n",
            "Epoch 281/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0872 - mae: 0.3305\n",
            "Epoch 281: loss did not improve from 0.08696\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0887 - mae: 0.3332 - val_loss: 0.2377 - val_mae: 0.5635\n",
            "Epoch 282/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0851 - mae: 0.3257\n",
            "Epoch 282: loss improved from 0.08696 to 0.08461, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0846 - mae: 0.3250 - val_loss: 0.2449 - val_mae: 0.5731\n",
            "Epoch 283/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0862 - mae: 0.3285\n",
            "Epoch 283: loss did not improve from 0.08461\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0868 - mae: 0.3300 - val_loss: 0.2483 - val_mae: 0.5732\n",
            "Epoch 284/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0868 - mae: 0.3327\n",
            "Epoch 284: loss did not improve from 0.08461\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0864 - mae: 0.3321 - val_loss: 0.2718 - val_mae: 0.6092\n",
            "Epoch 285/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0918 - mae: 0.3363\n",
            "Epoch 285: loss did not improve from 0.08461\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0914 - mae: 0.3361 - val_loss: 0.2442 - val_mae: 0.5688\n",
            "Epoch 286/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0794 - mae: 0.3131\n",
            "Epoch 286: loss improved from 0.08461 to 0.07944, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0794 - mae: 0.3131 - val_loss: 0.2541 - val_mae: 0.5831\n",
            "Epoch 287/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0750 - mae: 0.3040\n",
            "Epoch 287: loss improved from 0.07944 to 0.07576, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0758 - mae: 0.3061 - val_loss: 0.2584 - val_mae: 0.5876\n",
            "Epoch 288/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0821 - mae: 0.3192\n",
            "Epoch 288: loss did not improve from 0.07576\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0825 - mae: 0.3198 - val_loss: 0.2475 - val_mae: 0.5780\n",
            "Epoch 289/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0756 - mae: 0.3065\n",
            "Epoch 289: loss did not improve from 0.07576\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0764 - mae: 0.3085 - val_loss: 0.2482 - val_mae: 0.5789\n",
            "Epoch 290/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0856 - mae: 0.3257\n",
            "Epoch 290: loss did not improve from 0.07576\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0865 - mae: 0.3275 - val_loss: 0.2429 - val_mae: 0.5725\n",
            "Epoch 291/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0812 - mae: 0.3158\n",
            "Epoch 291: loss did not improve from 0.07576\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0812 - mae: 0.3158 - val_loss: 0.2453 - val_mae: 0.5733\n",
            "Epoch 292/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0777 - mae: 0.3122\n",
            "Epoch 292: loss did not improve from 0.07576\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0777 - mae: 0.3122 - val_loss: 0.2644 - val_mae: 0.6009\n",
            "Epoch 293/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0846 - mae: 0.3234\n",
            "Epoch 293: loss did not improve from 0.07576\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0861 - mae: 0.3263 - val_loss: 0.2441 - val_mae: 0.5723\n",
            "Epoch 294/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0743 - mae: 0.3029\n",
            "Epoch 294: loss improved from 0.07576 to 0.07410, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0741 - mae: 0.3026 - val_loss: 0.2558 - val_mae: 0.5872\n",
            "Epoch 295/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0690 - mae: 0.2921\n",
            "Epoch 295: loss improved from 0.07410 to 0.06847, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0685 - mae: 0.2908 - val_loss: 0.2717 - val_mae: 0.6094\n",
            "Epoch 296/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0701 - mae: 0.2965\n",
            "Epoch 296: loss did not improve from 0.06847\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0705 - mae: 0.2972 - val_loss: 0.2645 - val_mae: 0.5984\n",
            "Epoch 297/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0743 - mae: 0.3002\n",
            "Epoch 297: loss did not improve from 0.06847\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0743 - mae: 0.3002 - val_loss: 0.2580 - val_mae: 0.5910\n",
            "Epoch 298/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0724 - mae: 0.2994\n",
            "Epoch 298: loss did not improve from 0.06847\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0730 - mae: 0.3006 - val_loss: 0.2528 - val_mae: 0.5833\n",
            "Epoch 299/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0674 - mae: 0.2868\n",
            "Epoch 299: loss improved from 0.06847 to 0.06795, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0679 - mae: 0.2876 - val_loss: 0.2682 - val_mae: 0.6039\n",
            "Epoch 300/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0778 - mae: 0.3104\n",
            "Epoch 300: loss did not improve from 0.06795\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0777 - mae: 0.3103 - val_loss: 0.2688 - val_mae: 0.6067\n",
            "Epoch 301/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0729 - mae: 0.2991\n",
            "Epoch 301: loss did not improve from 0.06795\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0729 - mae: 0.2995 - val_loss: 0.2656 - val_mae: 0.5983\n",
            "Epoch 302/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0667 - mae: 0.2889\n",
            "Epoch 302: loss improved from 0.06795 to 0.06665, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0667 - mae: 0.2889 - val_loss: 0.2663 - val_mae: 0.6012\n",
            "Epoch 303/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0693 - mae: 0.2931\n",
            "Epoch 303: loss did not improve from 0.06665\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0693 - mae: 0.2931 - val_loss: 0.2533 - val_mae: 0.5791\n",
            "Epoch 304/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0690 - mae: 0.2924\n",
            "Epoch 304: loss did not improve from 0.06665\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0684 - mae: 0.2912 - val_loss: 0.2711 - val_mae: 0.6068\n",
            "Epoch 305/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0611 - mae: 0.2730\n",
            "Epoch 305: loss improved from 0.06665 to 0.06093, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0609 - mae: 0.2727 - val_loss: 0.2642 - val_mae: 0.5998\n",
            "Epoch 306/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0642 - mae: 0.2798\n",
            "Epoch 306: loss did not improve from 0.06093\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0639 - mae: 0.2793 - val_loss: 0.2681 - val_mae: 0.6045\n",
            "Epoch 307/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0604 - mae: 0.2739\n",
            "Epoch 307: loss improved from 0.06093 to 0.06040, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0604 - mae: 0.2739 - val_loss: 0.2722 - val_mae: 0.6093\n",
            "Epoch 308/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0686 - mae: 0.2882\n",
            "Epoch 308: loss did not improve from 0.06040\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0685 - mae: 0.2880 - val_loss: 0.2603 - val_mae: 0.5915\n",
            "Epoch 309/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0597 - mae: 0.2711\n",
            "Epoch 309: loss did not improve from 0.06040\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0605 - mae: 0.2732 - val_loss: 0.2631 - val_mae: 0.5995\n",
            "Epoch 310/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0557 - mae: 0.2632\n",
            "Epoch 310: loss improved from 0.06040 to 0.05525, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0553 - mae: 0.2620 - val_loss: 0.2775 - val_mae: 0.6149\n",
            "Epoch 311/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0583 - mae: 0.2656\n",
            "Epoch 311: loss did not improve from 0.05525\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0583 - mae: 0.2655 - val_loss: 0.2699 - val_mae: 0.6076\n",
            "Epoch 312/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0553 - mae: 0.2591\n",
            "Epoch 312: loss improved from 0.05525 to 0.05472, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0547 - mae: 0.2579 - val_loss: 0.2715 - val_mae: 0.6083\n",
            "Epoch 313/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0488 - mae: 0.2445\n",
            "Epoch 313: loss improved from 0.05472 to 0.04853, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0485 - mae: 0.2439 - val_loss: 0.2758 - val_mae: 0.6153\n",
            "Epoch 314/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0542 - mae: 0.2570\n",
            "Epoch 314: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0542 - mae: 0.2570 - val_loss: 0.2812 - val_mae: 0.6270\n",
            "Epoch 315/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0552 - mae: 0.2603\n",
            "Epoch 315: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0552 - mae: 0.2603 - val_loss: 0.2828 - val_mae: 0.6260\n",
            "Epoch 316/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0542 - mae: 0.2574\n",
            "Epoch 316: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0546 - mae: 0.2584 - val_loss: 0.2832 - val_mae: 0.6203\n",
            "Epoch 317/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0491 - mae: 0.2463\n",
            "Epoch 317: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0494 - mae: 0.2464 - val_loss: 0.2754 - val_mae: 0.6124\n",
            "Epoch 318/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0497 - mae: 0.2487\n",
            "Epoch 318: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.0495 - mae: 0.2479 - val_loss: 0.2807 - val_mae: 0.6189\n",
            "Epoch 319/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0516 - mae: 0.2496\n",
            "Epoch 319: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0521 - mae: 0.2504 - val_loss: 0.2787 - val_mae: 0.6143\n",
            "Epoch 320/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1100 - mae: 0.3550\n",
            "Epoch 320: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1100 - mae: 0.3561 - val_loss: 0.2153 - val_mae: 0.5342\n",
            "Epoch 321/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1108 - mae: 0.3648\n",
            "Epoch 321: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1090 - mae: 0.3628 - val_loss: 0.2347 - val_mae: 0.5621\n",
            "Epoch 322/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0757 - mae: 0.3017\n",
            "Epoch 322: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0755 - mae: 0.3017 - val_loss: 0.2642 - val_mae: 0.5991\n",
            "Epoch 323/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0579 - mae: 0.2643\n",
            "Epoch 323: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0577 - mae: 0.2633 - val_loss: 0.2712 - val_mae: 0.6086\n",
            "Epoch 324/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0488 - mae: 0.2453\n",
            "Epoch 324: loss did not improve from 0.04853\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0497 - mae: 0.2478 - val_loss: 0.2726 - val_mae: 0.6120\n",
            "Epoch 325/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0456 - mae: 0.2349\n",
            "Epoch 325: loss improved from 0.04853 to 0.04532, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0453 - mae: 0.2343 - val_loss: 0.2751 - val_mae: 0.6107\n",
            "Epoch 326/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0427 - mae: 0.2301\n",
            "Epoch 326: loss improved from 0.04532 to 0.04304, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0430 - mae: 0.2310 - val_loss: 0.2944 - val_mae: 0.6353\n",
            "Epoch 327/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0446 - mae: 0.2359\n",
            "Epoch 327: loss did not improve from 0.04304\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0446 - mae: 0.2360 - val_loss: 0.2896 - val_mae: 0.6314\n",
            "Epoch 328/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0409 - mae: 0.2217\n",
            "Epoch 328: loss improved from 0.04304 to 0.04086, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0409 - mae: 0.2217 - val_loss: 0.2847 - val_mae: 0.6221\n",
            "Epoch 329/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0407 - mae: 0.2234\n",
            "Epoch 329: loss did not improve from 0.04086\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0422 - mae: 0.2267 - val_loss: 0.2983 - val_mae: 0.6414\n",
            "Epoch 330/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0411 - mae: 0.2246\n",
            "Epoch 330: loss did not improve from 0.04086\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0413 - mae: 0.2251 - val_loss: 0.2834 - val_mae: 0.6215\n",
            "Epoch 331/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0449 - mae: 0.2337\n",
            "Epoch 331: loss did not improve from 0.04086\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0445 - mae: 0.2322 - val_loss: 0.2847 - val_mae: 0.6223\n",
            "Epoch 332/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0364 - mae: 0.2127\n",
            "Epoch 332: loss improved from 0.04086 to 0.03618, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0362 - mae: 0.2118 - val_loss: 0.2892 - val_mae: 0.6291\n",
            "Epoch 333/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0431 - mae: 0.2285\n",
            "Epoch 333: loss did not improve from 0.03618\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0437 - mae: 0.2301 - val_loss: 0.2923 - val_mae: 0.6351\n",
            "Epoch 334/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0414 - mae: 0.2252\n",
            "Epoch 334: loss did not improve from 0.03618\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0410 - mae: 0.2237 - val_loss: 0.2976 - val_mae: 0.6418\n",
            "Epoch 335/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0343 - mae: 0.2057\n",
            "Epoch 335: loss improved from 0.03618 to 0.03415, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0342 - mae: 0.2051 - val_loss: 0.3001 - val_mae: 0.6430\n",
            "Epoch 336/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0354 - mae: 0.2072\n",
            "Epoch 336: loss did not improve from 0.03415\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0350 - mae: 0.2068 - val_loss: 0.3077 - val_mae: 0.6537\n",
            "Epoch 337/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0330 - mae: 0.2003\n",
            "Epoch 337: loss improved from 0.03415 to 0.03275, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0328 - mae: 0.1994 - val_loss: 0.2933 - val_mae: 0.6367\n",
            "Epoch 338/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0330 - mae: 0.2013\n",
            "Epoch 338: loss did not improve from 0.03275\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0331 - mae: 0.2014 - val_loss: 0.3084 - val_mae: 0.6554\n",
            "Epoch 339/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0310 - mae: 0.1935\n",
            "Epoch 339: loss improved from 0.03275 to 0.03105, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0310 - mae: 0.1941 - val_loss: 0.3141 - val_mae: 0.6631\n",
            "Epoch 340/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0312 - mae: 0.1956\n",
            "Epoch 340: loss did not improve from 0.03105\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.0312 - mae: 0.1956 - val_loss: 0.2993 - val_mae: 0.6450\n",
            "Epoch 341/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0320 - mae: 0.1946\n",
            "Epoch 341: loss did not improve from 0.03105\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0320 - mae: 0.1946 - val_loss: 0.2960 - val_mae: 0.6394\n",
            "Epoch 342/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0312 - mae: 0.1972\n",
            "Epoch 342: loss did not improve from 0.03105\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0311 - mae: 0.1965 - val_loss: 0.3080 - val_mae: 0.6560\n",
            "Epoch 343/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0293 - mae: 0.1891\n",
            "Epoch 343: loss improved from 0.03105 to 0.02962, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0296 - mae: 0.1901 - val_loss: 0.3018 - val_mae: 0.6445\n",
            "Epoch 344/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0315 - mae: 0.1931\n",
            "Epoch 344: loss did not improve from 0.02962\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0318 - mae: 0.1942 - val_loss: 0.3172 - val_mae: 0.6653\n",
            "Epoch 345/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0334 - mae: 0.2010\n",
            "Epoch 345: loss did not improve from 0.02962\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0329 - mae: 0.1997 - val_loss: 0.2928 - val_mae: 0.6322\n",
            "Epoch 346/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0311 - mae: 0.1956\n",
            "Epoch 346: loss did not improve from 0.02962\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0309 - mae: 0.1951 - val_loss: 0.3039 - val_mae: 0.6515\n",
            "Epoch 347/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0268 - mae: 0.1802\n",
            "Epoch 347: loss improved from 0.02962 to 0.02667, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0267 - mae: 0.1800 - val_loss: 0.3033 - val_mae: 0.6500\n",
            "Epoch 348/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.1852\n",
            "Epoch 348: loss did not improve from 0.02667\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0285 - mae: 0.1849 - val_loss: 0.3006 - val_mae: 0.6490\n",
            "Epoch 349/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0306 - mae: 0.1919\n",
            "Epoch 349: loss did not improve from 0.02667\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0306 - mae: 0.1919 - val_loss: 0.3026 - val_mae: 0.6475\n",
            "Epoch 350/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0313 - mae: 0.1953\n",
            "Epoch 350: loss did not improve from 0.02667\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0313 - mae: 0.1957 - val_loss: 0.3022 - val_mae: 0.6493\n",
            "Epoch 351/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.1799\n",
            "Epoch 351: loss did not improve from 0.02667\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0273 - mae: 0.1808 - val_loss: 0.2978 - val_mae: 0.6407\n",
            "Epoch 352/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0275 - mae: 0.1835\n",
            "Epoch 352: loss did not improve from 0.02667\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0272 - mae: 0.1827 - val_loss: 0.3027 - val_mae: 0.6508\n",
            "Epoch 353/500\n",
            "62/68 [==========================>...] - ETA: 0s - loss: 0.0246 - mae: 0.1741\n",
            "Epoch 353: loss improved from 0.02667 to 0.02505, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0250 - mae: 0.1743 - val_loss: 0.3027 - val_mae: 0.6508\n",
            "Epoch 354/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0258 - mae: 0.1770\n",
            "Epoch 354: loss did not improve from 0.02505\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0257 - mae: 0.1772 - val_loss: 0.3115 - val_mae: 0.6595\n",
            "Epoch 355/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0250 - mae: 0.1735\n",
            "Epoch 355: loss did not improve from 0.02505\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0255 - mae: 0.1750 - val_loss: 0.3151 - val_mae: 0.6638\n",
            "Epoch 356/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0300 - mae: 0.1894\n",
            "Epoch 356: loss did not improve from 0.02505\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0300 - mae: 0.1898 - val_loss: 0.3053 - val_mae: 0.6499\n",
            "Epoch 357/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.1732\n",
            "Epoch 357: loss improved from 0.02505 to 0.02493, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0249 - mae: 0.1732 - val_loss: 0.3134 - val_mae: 0.6621\n",
            "Epoch 358/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0250 - mae: 0.1726\n",
            "Epoch 358: loss did not improve from 0.02493\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0251 - mae: 0.1727 - val_loss: 0.2996 - val_mae: 0.6444\n",
            "Epoch 359/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.1791\n",
            "Epoch 359: loss did not improve from 0.02493\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0272 - mae: 0.1810 - val_loss: 0.3122 - val_mae: 0.6625\n",
            "Epoch 360/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.2055\n",
            "Epoch 360: loss did not improve from 0.02493\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0353 - mae: 0.2046 - val_loss: 0.3084 - val_mae: 0.6547\n",
            "Epoch 361/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0217 - mae: 0.1630\n",
            "Epoch 361: loss improved from 0.02493 to 0.02249, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0225 - mae: 0.1655 - val_loss: 0.3080 - val_mae: 0.6547\n",
            "Epoch 362/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0235 - mae: 0.1710\n",
            "Epoch 362: loss did not improve from 0.02249\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0241 - mae: 0.1725 - val_loss: 0.2980 - val_mae: 0.6419\n",
            "Epoch 363/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.1691\n",
            "Epoch 363: loss did not improve from 0.02249\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0238 - mae: 0.1689 - val_loss: 0.3088 - val_mae: 0.6537\n",
            "Epoch 364/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.1799\n",
            "Epoch 364: loss did not improve from 0.02249\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0279 - mae: 0.1799 - val_loss: 0.3029 - val_mae: 0.6488\n",
            "Epoch 365/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.1826\n",
            "Epoch 365: loss did not improve from 0.02249\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0272 - mae: 0.1831 - val_loss: 0.3096 - val_mae: 0.6568\n",
            "Epoch 366/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.1673\n",
            "Epoch 366: loss did not improve from 0.02249\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0231 - mae: 0.1678 - val_loss: 0.3092 - val_mae: 0.6556\n",
            "Epoch 367/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.1520\n",
            "Epoch 367: loss improved from 0.02249 to 0.01907, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0191 - mae: 0.1522 - val_loss: 0.3068 - val_mae: 0.6544\n",
            "Epoch 368/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0187 - mae: 0.1493\n",
            "Epoch 368: loss improved from 0.01907 to 0.01886, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0189 - mae: 0.1500 - val_loss: 0.3087 - val_mae: 0.6571\n",
            "Epoch 369/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0171 - mae: 0.1437\n",
            "Epoch 369: loss improved from 0.01886 to 0.01710, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0171 - mae: 0.1438 - val_loss: 0.3131 - val_mae: 0.6600\n",
            "Epoch 370/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0181 - mae: 0.1470\n",
            "Epoch 370: loss did not improve from 0.01710\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0184 - mae: 0.1478 - val_loss: 0.3100 - val_mae: 0.6545\n",
            "Epoch 371/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0225 - mae: 0.1630\n",
            "Epoch 371: loss did not improve from 0.01710\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0224 - mae: 0.1632 - val_loss: 0.3104 - val_mae: 0.6574\n",
            "Epoch 372/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0169 - mae: 0.1444\n",
            "Epoch 372: loss improved from 0.01710 to 0.01674, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0167 - mae: 0.1441 - val_loss: 0.3034 - val_mae: 0.6488\n",
            "Epoch 373/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0151 - mae: 0.1357\n",
            "Epoch 373: loss improved from 0.01674 to 0.01495, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0149 - mae: 0.1348 - val_loss: 0.2986 - val_mae: 0.6424\n",
            "Epoch 374/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0156 - mae: 0.1381\n",
            "Epoch 374: loss did not improve from 0.01495\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0156 - mae: 0.1379 - val_loss: 0.3126 - val_mae: 0.6596\n",
            "Epoch 375/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0158 - mae: 0.1375\n",
            "Epoch 375: loss did not improve from 0.01495\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0158 - mae: 0.1375 - val_loss: 0.3117 - val_mae: 0.6598\n",
            "Epoch 376/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0191 - mae: 0.1540\n",
            "Epoch 376: loss did not improve from 0.01495\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0192 - mae: 0.1544 - val_loss: 0.3178 - val_mae: 0.6688\n",
            "Epoch 377/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0164 - mae: 0.1424\n",
            "Epoch 377: loss did not improve from 0.01495\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0164 - mae: 0.1422 - val_loss: 0.3197 - val_mae: 0.6711\n",
            "Epoch 378/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0139 - mae: 0.1314\n",
            "Epoch 378: loss improved from 0.01495 to 0.01393, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0139 - mae: 0.1315 - val_loss: 0.3142 - val_mae: 0.6636\n",
            "Epoch 379/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0153 - mae: 0.1352\n",
            "Epoch 379: loss did not improve from 0.01393\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0152 - mae: 0.1345 - val_loss: 0.3100 - val_mae: 0.6572\n",
            "Epoch 380/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0140 - mae: 0.1304\n",
            "Epoch 380: loss did not improve from 0.01393\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0140 - mae: 0.1304 - val_loss: 0.3215 - val_mae: 0.6708\n",
            "Epoch 381/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.1225\n",
            "Epoch 381: loss improved from 0.01393 to 0.01220, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0122 - mae: 0.1225 - val_loss: 0.3053 - val_mae: 0.6521\n",
            "Epoch 382/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0114 - mae: 0.1182\n",
            "Epoch 382: loss improved from 0.01220 to 0.01143, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0114 - mae: 0.1182 - val_loss: 0.3184 - val_mae: 0.6692\n",
            "Epoch 383/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0183 - mae: 0.1438\n",
            "Epoch 383: loss did not improve from 0.01143\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0188 - mae: 0.1452 - val_loss: 0.3132 - val_mae: 0.6614\n",
            "Epoch 384/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0206 - mae: 0.1562\n",
            "Epoch 384: loss did not improve from 0.01143\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0205 - mae: 0.1559 - val_loss: 0.3089 - val_mae: 0.6543\n",
            "Epoch 385/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0165 - mae: 0.1403\n",
            "Epoch 385: loss did not improve from 0.01143\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0165 - mae: 0.1405 - val_loss: 0.3016 - val_mae: 0.6478\n",
            "Epoch 386/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0151 - mae: 0.1361\n",
            "Epoch 386: loss did not improve from 0.01143\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0151 - mae: 0.1356 - val_loss: 0.3145 - val_mae: 0.6611\n",
            "Epoch 387/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0138 - mae: 0.1298\n",
            "Epoch 387: loss did not improve from 0.01143\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0138 - mae: 0.1298 - val_loss: 0.3101 - val_mae: 0.6554\n",
            "Epoch 388/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0120 - mae: 0.1220\n",
            "Epoch 388: loss did not improve from 0.01143\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0120 - mae: 0.1220 - val_loss: 0.3094 - val_mae: 0.6550\n",
            "Epoch 389/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.1189\n",
            "Epoch 389: loss did not improve from 0.01143\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0116 - mae: 0.1191 - val_loss: 0.3179 - val_mae: 0.6673\n",
            "Epoch 390/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0118 - mae: 0.1203\n",
            "Epoch 390: loss did not improve from 0.01143\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.0117 - mae: 0.1195 - val_loss: 0.3150 - val_mae: 0.6629\n",
            "Epoch 391/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.1135\n",
            "Epoch 391: loss improved from 0.01143 to 0.01069, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0107 - mae: 0.1138 - val_loss: 0.3168 - val_mae: 0.6636\n",
            "Epoch 392/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.1132\n",
            "Epoch 392: loss improved from 0.01069 to 0.01059, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0106 - mae: 0.1132 - val_loss: 0.3189 - val_mae: 0.6687\n",
            "Epoch 393/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0115 - mae: 0.1180\n",
            "Epoch 393: loss did not improve from 0.01059\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0115 - mae: 0.1183 - val_loss: 0.3136 - val_mae: 0.6647\n",
            "Epoch 394/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0119 - mae: 0.1204\n",
            "Epoch 394: loss did not improve from 0.01059\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0118 - mae: 0.1201 - val_loss: 0.3158 - val_mae: 0.6640\n",
            "Epoch 395/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0117 - mae: 0.1195\n",
            "Epoch 395: loss did not improve from 0.01059\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0118 - mae: 0.1196 - val_loss: 0.3206 - val_mae: 0.6701\n",
            "Epoch 396/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0093 - mae: 0.1064\n",
            "Epoch 396: loss improved from 0.01059 to 0.00934, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0093 - mae: 0.1064 - val_loss: 0.3183 - val_mae: 0.6659\n",
            "Epoch 397/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0082 - mae: 0.1000\n",
            "Epoch 397: loss improved from 0.00934 to 0.00809, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0993 - val_loss: 0.3208 - val_mae: 0.6711\n",
            "Epoch 398/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0081 - mae: 0.1003\n",
            "Epoch 398: loss did not improve from 0.00809\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.1003 - val_loss: 0.3213 - val_mae: 0.6729\n",
            "Epoch 399/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0077 - mae: 0.0976\n",
            "Epoch 399: loss improved from 0.00809 to 0.00769, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0977 - val_loss: 0.3204 - val_mae: 0.6687\n",
            "Epoch 400/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.1126\n",
            "Epoch 400: loss did not improve from 0.00769\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0104 - mae: 0.1126 - val_loss: 0.3164 - val_mae: 0.6659\n",
            "Epoch 401/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0085 - mae: 0.1016\n",
            "Epoch 401: loss did not improve from 0.00769\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.1022 - val_loss: 0.3182 - val_mae: 0.6684\n",
            "Epoch 402/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0074 - mae: 0.0954\n",
            "Epoch 402: loss improved from 0.00769 to 0.00739, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0074 - mae: 0.0954 - val_loss: 0.3137 - val_mae: 0.6609\n",
            "Epoch 403/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0080 - mae: 0.0987\n",
            "Epoch 403: loss did not improve from 0.00739\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0082 - mae: 0.0991 - val_loss: 0.3222 - val_mae: 0.6735\n",
            "Epoch 404/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0962\n",
            "Epoch 404: loss did not improve from 0.00739\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0077 - mae: 0.0961 - val_loss: 0.3250 - val_mae: 0.6751\n",
            "Epoch 405/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0943\n",
            "Epoch 405: loss improved from 0.00739 to 0.00737, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0074 - mae: 0.0946 - val_loss: 0.3225 - val_mae: 0.6748\n",
            "Epoch 406/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0069 - mae: 0.0914\n",
            "Epoch 406: loss improved from 0.00737 to 0.00689, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.0069 - mae: 0.0914 - val_loss: 0.3245 - val_mae: 0.6775\n",
            "Epoch 407/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0088 - mae: 0.1021\n",
            "Epoch 407: loss did not improve from 0.00689\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0091 - mae: 0.1032 - val_loss: 0.3210 - val_mae: 0.6691\n",
            "Epoch 408/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.1107\n",
            "Epoch 408: loss did not improve from 0.00689\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0104 - mae: 0.1105 - val_loss: 0.3186 - val_mae: 0.6710\n",
            "Epoch 409/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0088 - mae: 0.1033\n",
            "Epoch 409: loss did not improve from 0.00689\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0088 - mae: 0.1033 - val_loss: 0.3154 - val_mae: 0.6649\n",
            "Epoch 410/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0079 - mae: 0.0976\n",
            "Epoch 410: loss did not improve from 0.00689\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0079 - mae: 0.0976 - val_loss: 0.3197 - val_mae: 0.6711\n",
            "Epoch 411/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0922\n",
            "Epoch 411: loss did not improve from 0.00689\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0070 - mae: 0.0923 - val_loss: 0.3242 - val_mae: 0.6761\n",
            "Epoch 412/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0063 - mae: 0.0877\n",
            "Epoch 412: loss improved from 0.00689 to 0.00628, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0063 - mae: 0.0875 - val_loss: 0.3216 - val_mae: 0.6728\n",
            "Epoch 413/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0062 - mae: 0.0863\n",
            "Epoch 413: loss improved from 0.00628 to 0.00625, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0062 - mae: 0.0865 - val_loss: 0.3182 - val_mae: 0.6683\n",
            "Epoch 414/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0832\n",
            "Epoch 414: loss improved from 0.00625 to 0.00565, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0057 - mae: 0.0835 - val_loss: 0.3218 - val_mae: 0.6738\n",
            "Epoch 415/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0074 - mae: 0.0955\n",
            "Epoch 415: loss did not improve from 0.00565\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0073 - mae: 0.0950 - val_loss: 0.3188 - val_mae: 0.6690\n",
            "Epoch 416/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0832\n",
            "Epoch 416: loss did not improve from 0.00565\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0057 - mae: 0.0834 - val_loss: 0.3195 - val_mae: 0.6693\n",
            "Epoch 417/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0047 - mae: 0.0763\n",
            "Epoch 417: loss improved from 0.00565 to 0.00475, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0047 - mae: 0.0765 - val_loss: 0.3226 - val_mae: 0.6721\n",
            "Epoch 418/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0047 - mae: 0.0761\n",
            "Epoch 418: loss improved from 0.00475 to 0.00470, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0047 - mae: 0.0761 - val_loss: 0.3232 - val_mae: 0.6762\n",
            "Epoch 419/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0050 - mae: 0.0776\n",
            "Epoch 419: loss did not improve from 0.00470\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0050 - mae: 0.0779 - val_loss: 0.3205 - val_mae: 0.6710\n",
            "Epoch 420/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0047 - mae: 0.0746\n",
            "Epoch 420: loss did not improve from 0.00470\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0047 - mae: 0.0754 - val_loss: 0.3250 - val_mae: 0.6759\n",
            "Epoch 421/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0880\n",
            "Epoch 421: loss did not improve from 0.00470\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0066 - mae: 0.0875 - val_loss: 0.3197 - val_mae: 0.6696\n",
            "Epoch 422/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0053 - mae: 0.0802\n",
            "Epoch 422: loss did not improve from 0.00470\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0053 - mae: 0.0801 - val_loss: 0.3202 - val_mae: 0.6703\n",
            "Epoch 423/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0050 - mae: 0.0777\n",
            "Epoch 423: loss did not improve from 0.00470\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.0050 - mae: 0.0777 - val_loss: 0.3209 - val_mae: 0.6694\n",
            "Epoch 424/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0049 - mae: 0.0769\n",
            "Epoch 424: loss did not improve from 0.00470\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0049 - mae: 0.0769 - val_loss: 0.3260 - val_mae: 0.6776\n",
            "Epoch 425/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0794\n",
            "Epoch 425: loss did not improve from 0.00470\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0052 - mae: 0.0794 - val_loss: 0.3285 - val_mae: 0.6814\n",
            "Epoch 426/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0042 - mae: 0.0720\n",
            "Epoch 426: loss improved from 0.00470 to 0.00417, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0042 - mae: 0.0717 - val_loss: 0.3233 - val_mae: 0.6752\n",
            "Epoch 427/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0038 - mae: 0.0679\n",
            "Epoch 427: loss improved from 0.00417 to 0.00383, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0038 - mae: 0.0678 - val_loss: 0.3250 - val_mae: 0.6760\n",
            "Epoch 428/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0036 - mae: 0.0657\n",
            "Epoch 428: loss improved from 0.00383 to 0.00357, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0036 - mae: 0.0660 - val_loss: 0.3274 - val_mae: 0.6786\n",
            "Epoch 429/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0036 - mae: 0.0652\n",
            "Epoch 429: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0037 - mae: 0.0661 - val_loss: 0.3244 - val_mae: 0.6767\n",
            "Epoch 430/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0041 - mae: 0.0692\n",
            "Epoch 430: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0041 - mae: 0.0692 - val_loss: 0.3234 - val_mae: 0.6761\n",
            "Epoch 431/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0039 - mae: 0.0686\n",
            "Epoch 431: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0039 - mae: 0.0686 - val_loss: 0.3226 - val_mae: 0.6738\n",
            "Epoch 432/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0043 - mae: 0.0724\n",
            "Epoch 432: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.0043 - mae: 0.0722 - val_loss: 0.3265 - val_mae: 0.6793\n",
            "Epoch 433/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0036 - mae: 0.0647\n",
            "Epoch 433: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0036 - mae: 0.0654 - val_loss: 0.3212 - val_mae: 0.6722\n",
            "Epoch 434/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0041 - mae: 0.0699\n",
            "Epoch 434: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0041 - mae: 0.0698 - val_loss: 0.3257 - val_mae: 0.6784\n",
            "Epoch 435/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0040 - mae: 0.0699\n",
            "Epoch 435: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0040 - mae: 0.0699 - val_loss: 0.3242 - val_mae: 0.6772\n",
            "Epoch 436/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0041 - mae: 0.0700\n",
            "Epoch 436: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0041 - mae: 0.0700 - val_loss: 0.3262 - val_mae: 0.6788\n",
            "Epoch 437/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0041 - mae: 0.0706\n",
            "Epoch 437: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0041 - mae: 0.0706 - val_loss: 0.3215 - val_mae: 0.6739\n",
            "Epoch 438/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0039 - mae: 0.0686\n",
            "Epoch 438: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0039 - mae: 0.0685 - val_loss: 0.3248 - val_mae: 0.6755\n",
            "Epoch 439/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0044 - mae: 0.0711\n",
            "Epoch 439: loss did not improve from 0.00357\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0044 - mae: 0.0714 - val_loss: 0.3252 - val_mae: 0.6768\n",
            "Epoch 440/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0035 - mae: 0.0659\n",
            "Epoch 440: loss improved from 0.00357 to 0.00350, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.0035 - mae: 0.0658 - val_loss: 0.3218 - val_mae: 0.6712\n",
            "Epoch 441/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0038 - mae: 0.0683\n",
            "Epoch 441: loss did not improve from 0.00350\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0038 - mae: 0.0684 - val_loss: 0.3256 - val_mae: 0.6751\n",
            "Epoch 442/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0042 - mae: 0.0707\n",
            "Epoch 442: loss did not improve from 0.00350\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0041 - mae: 0.0701 - val_loss: 0.3198 - val_mae: 0.6693\n",
            "Epoch 443/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0034 - mae: 0.0638\n",
            "Epoch 443: loss improved from 0.00350 to 0.00342, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0034 - mae: 0.0634 - val_loss: 0.3229 - val_mae: 0.6731\n",
            "Epoch 444/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0080 - mae: 0.0894\n",
            "Epoch 444: loss did not improve from 0.00342\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0079 - mae: 0.0894 - val_loss: 0.3214 - val_mae: 0.6719\n",
            "Epoch 445/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0915\n",
            "Epoch 445: loss did not improve from 0.00342\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0070 - mae: 0.0912 - val_loss: 0.3268 - val_mae: 0.6790\n",
            "Epoch 446/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0073 - mae: 0.0939\n",
            "Epoch 446: loss did not improve from 0.00342\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0072 - mae: 0.0929 - val_loss: 0.3196 - val_mae: 0.6685\n",
            "Epoch 447/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0825\n",
            "Epoch 447: loss did not improve from 0.00342\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0055 - mae: 0.0825 - val_loss: 0.3282 - val_mae: 0.6820\n",
            "Epoch 448/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0050 - mae: 0.0780\n",
            "Epoch 448: loss did not improve from 0.00342\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0050 - mae: 0.0780 - val_loss: 0.3216 - val_mae: 0.6721\n",
            "Epoch 449/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0039 - mae: 0.0681\n",
            "Epoch 449: loss did not improve from 0.00342\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0038 - mae: 0.0678 - val_loss: 0.3246 - val_mae: 0.6764\n",
            "Epoch 450/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0033 - mae: 0.0626\n",
            "Epoch 450: loss improved from 0.00342 to 0.00325, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0032 - mae: 0.0625 - val_loss: 0.3236 - val_mae: 0.6742\n",
            "Epoch 451/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0035 - mae: 0.0642\n",
            "Epoch 451: loss did not improve from 0.00325\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0035 - mae: 0.0640 - val_loss: 0.3295 - val_mae: 0.6835\n",
            "Epoch 452/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0034 - mae: 0.0639\n",
            "Epoch 452: loss did not improve from 0.00325\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0033 - mae: 0.0638 - val_loss: 0.3253 - val_mae: 0.6784\n",
            "Epoch 453/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0027 - mae: 0.0578\n",
            "Epoch 453: loss improved from 0.00325 to 0.00274, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.0027 - mae: 0.0578 - val_loss: 0.3258 - val_mae: 0.6782\n",
            "Epoch 454/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0029 - mae: 0.0589\n",
            "Epoch 454: loss did not improve from 0.00274\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0029 - mae: 0.0590 - val_loss: 0.3218 - val_mae: 0.6739\n",
            "Epoch 455/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0030 - mae: 0.0603\n",
            "Epoch 455: loss did not improve from 0.00274\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0030 - mae: 0.0603 - val_loss: 0.3233 - val_mae: 0.6764\n",
            "Epoch 456/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0031 - mae: 0.0605\n",
            "Epoch 456: loss did not improve from 0.00274\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0030 - mae: 0.0595 - val_loss: 0.3245 - val_mae: 0.6750\n",
            "Epoch 457/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0025 - mae: 0.0549\n",
            "Epoch 457: loss improved from 0.00274 to 0.00248, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0025 - mae: 0.0549 - val_loss: 0.3254 - val_mae: 0.6767\n",
            "Epoch 458/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0028 - mae: 0.0578\n",
            "Epoch 458: loss did not improve from 0.00248\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0028 - mae: 0.0578 - val_loss: 0.3221 - val_mae: 0.6723\n",
            "Epoch 459/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0024 - mae: 0.0528\n",
            "Epoch 459: loss improved from 0.00248 to 0.00236, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.0024 - mae: 0.0526 - val_loss: 0.3263 - val_mae: 0.6779\n",
            "Epoch 460/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0022 - mae: 0.0520\n",
            "Epoch 460: loss improved from 0.00236 to 0.00225, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0022 - mae: 0.0520 - val_loss: 0.3259 - val_mae: 0.6769\n",
            "Epoch 461/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0020 - mae: 0.0493\n",
            "Epoch 461: loss improved from 0.00225 to 0.00200, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0020 - mae: 0.0493 - val_loss: 0.3275 - val_mae: 0.6812\n",
            "Epoch 462/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0021 - mae: 0.0504\n",
            "Epoch 462: loss did not improve from 0.00200\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0021 - mae: 0.0502 - val_loss: 0.3264 - val_mae: 0.6770\n",
            "Epoch 463/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0021 - mae: 0.0503\n",
            "Epoch 463: loss did not improve from 0.00200\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0021 - mae: 0.0505 - val_loss: 0.3210 - val_mae: 0.6718\n",
            "Epoch 464/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0497\n",
            "Epoch 464: loss did not improve from 0.00200\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0020 - mae: 0.0496 - val_loss: 0.3264 - val_mae: 0.6771\n",
            "Epoch 465/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0019 - mae: 0.0470\n",
            "Epoch 465: loss improved from 0.00200 to 0.00186, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0019 - mae: 0.0469 - val_loss: 0.3270 - val_mae: 0.6788\n",
            "Epoch 466/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0018 - mae: 0.0462\n",
            "Epoch 466: loss improved from 0.00186 to 0.00176, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0018 - mae: 0.0462 - val_loss: 0.3261 - val_mae: 0.6769\n",
            "Epoch 467/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0019 - mae: 0.0483\n",
            "Epoch 467: loss did not improve from 0.00176\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0019 - mae: 0.0482 - val_loss: 0.3248 - val_mae: 0.6757\n",
            "Epoch 468/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0018 - mae: 0.0458\n",
            "Epoch 468: loss improved from 0.00176 to 0.00176, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0018 - mae: 0.0458 - val_loss: 0.3266 - val_mae: 0.6776\n",
            "Epoch 469/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0444\n",
            "Epoch 469: loss improved from 0.00176 to 0.00161, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0016 - mae: 0.0444 - val_loss: 0.3268 - val_mae: 0.6781\n",
            "Epoch 470/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0017 - mae: 0.0445\n",
            "Epoch 470: loss did not improve from 0.00161\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.0016 - mae: 0.0444 - val_loss: 0.3294 - val_mae: 0.6818\n",
            "Epoch 471/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0015 - mae: 0.0425\n",
            "Epoch 471: loss improved from 0.00161 to 0.00150, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0015 - mae: 0.0425 - val_loss: 0.3253 - val_mae: 0.6767\n",
            "Epoch 472/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0440\n",
            "Epoch 472: loss did not improve from 0.00150\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0438 - val_loss: 0.3274 - val_mae: 0.6795\n",
            "Epoch 473/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0015 - mae: 0.0430\n",
            "Epoch 473: loss did not improve from 0.00150\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0015 - mae: 0.0430 - val_loss: 0.3254 - val_mae: 0.6772\n",
            "Epoch 474/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0017 - mae: 0.0444\n",
            "Epoch 474: loss did not improve from 0.00150\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0017 - mae: 0.0446 - val_loss: 0.3287 - val_mae: 0.6814\n",
            "Epoch 475/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0017 - mae: 0.0451\n",
            "Epoch 475: loss did not improve from 0.00150\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0017 - mae: 0.0451 - val_loss: 0.3261 - val_mae: 0.6791\n",
            "Epoch 476/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0446\n",
            "Epoch 476: loss did not improve from 0.00150\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0016 - mae: 0.0447 - val_loss: 0.3289 - val_mae: 0.6818\n",
            "Epoch 477/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0017 - mae: 0.0439\n",
            "Epoch 477: loss did not improve from 0.00150\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0016 - mae: 0.0436 - val_loss: 0.3281 - val_mae: 0.6813\n",
            "Epoch 478/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0015 - mae: 0.0423\n",
            "Epoch 478: loss did not improve from 0.00150\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0015 - mae: 0.0426 - val_loss: 0.3240 - val_mae: 0.6753\n",
            "Epoch 479/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0017 - mae: 0.0456\n",
            "Epoch 479: loss did not improve from 0.00150\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0017 - mae: 0.0457 - val_loss: 0.3256 - val_mae: 0.6767\n",
            "Epoch 480/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0438\n",
            "Epoch 480: loss did not improve from 0.00150\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0435 - val_loss: 0.3289 - val_mae: 0.6811\n",
            "Epoch 481/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0015 - mae: 0.0422\n",
            "Epoch 481: loss improved from 0.00150 to 0.00148, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0015 - mae: 0.0420 - val_loss: 0.3289 - val_mae: 0.6823\n",
            "Epoch 482/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0398\n",
            "Epoch 482: loss improved from 0.00148 to 0.00138, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0014 - mae: 0.0401 - val_loss: 0.3278 - val_mae: 0.6807\n",
            "Epoch 483/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0022 - mae: 0.0487\n",
            "Epoch 483: loss did not improve from 0.00138\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0023 - mae: 0.0489 - val_loss: 0.3275 - val_mae: 0.6787\n",
            "Epoch 484/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.0022 - mae: 0.0503\n",
            "Epoch 484: loss did not improve from 0.00138\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0021 - mae: 0.0500 - val_loss: 0.3249 - val_mae: 0.6761\n",
            "Epoch 485/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0434\n",
            "Epoch 485: loss did not improve from 0.00138\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0434 - val_loss: 0.3295 - val_mae: 0.6822\n",
            "Epoch 486/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0017 - mae: 0.0454\n",
            "Epoch 486: loss did not improve from 0.00138\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0018 - mae: 0.0457 - val_loss: 0.3281 - val_mae: 0.6800\n",
            "Epoch 487/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0496\n",
            "Epoch 487: loss did not improve from 0.00138\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0020 - mae: 0.0496 - val_loss: 0.3263 - val_mae: 0.6773\n",
            "Epoch 488/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0434\n",
            "Epoch 488: loss did not improve from 0.00138\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0016 - mae: 0.0434 - val_loss: 0.3281 - val_mae: 0.6801\n",
            "Epoch 489/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0013 - mae: 0.0401\n",
            "Epoch 489: loss improved from 0.00138 to 0.00133, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0013 - mae: 0.0401 - val_loss: 0.3282 - val_mae: 0.6802\n",
            "Epoch 490/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0013 - mae: 0.0389\n",
            "Epoch 490: loss improved from 0.00133 to 0.00125, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0012 - mae: 0.0388 - val_loss: 0.3271 - val_mae: 0.6778\n",
            "Epoch 491/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0013 - mae: 0.0389\n",
            "Epoch 491: loss did not improve from 0.00125\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0013 - mae: 0.0389 - val_loss: 0.3251 - val_mae: 0.6762\n",
            "Epoch 492/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0011 - mae: 0.0368\n",
            "Epoch 492: loss improved from 0.00125 to 0.00113, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0011 - mae: 0.0368 - val_loss: 0.3280 - val_mae: 0.6789\n",
            "Epoch 493/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.0011 - mae: 0.0366\n",
            "Epoch 493: loss did not improve from 0.00113\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0011 - mae: 0.0367 - val_loss: 0.3292 - val_mae: 0.6817\n",
            "Epoch 494/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.0011 - mae: 0.0363\n",
            "Epoch 494: loss improved from 0.00113 to 0.00110, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0011 - mae: 0.0362 - val_loss: 0.3296 - val_mae: 0.6829\n",
            "Epoch 495/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0011 - mae: 0.0365\n",
            "Epoch 495: loss did not improve from 0.00110\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.0011 - mae: 0.0365 - val_loss: 0.3276 - val_mae: 0.6795\n",
            "Epoch 496/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 9.4715e-04 - mae: 0.0339\n",
            "Epoch 496: loss improved from 0.00110 to 0.00095, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 9.4515e-04 - mae: 0.0338 - val_loss: 0.3278 - val_mae: 0.6794\n",
            "Epoch 497/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 9.9516e-04 - mae: 0.0346\n",
            "Epoch 497: loss did not improve from 0.00095\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 9.9337e-04 - mae: 0.0345 - val_loss: 0.3292 - val_mae: 0.6812\n",
            "Epoch 498/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.0011 - mae: 0.0369\n",
            "Epoch 498: loss did not improve from 0.00095\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0012 - mae: 0.0371 - val_loss: 0.3314 - val_mae: 0.6842\n",
            "Epoch 499/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.0013 - mae: 0.0391\n",
            "Epoch 499: loss did not improve from 0.00095\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0013 - mae: 0.0391 - val_loss: 0.3293 - val_mae: 0.6832\n",
            "Epoch 500/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.0011 - mae: 0.0359\n",
            "Epoch 500: loss did not improve from 0.00095\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.0011 - mae: 0.0359 - val_loss: 0.3250 - val_mae: 0.6765\n",
            "Step time : 0.019\n",
            "Total time : 636.234\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-1,momentum=0.9) # on a positionné l'optimiseur à 1e-1\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_entrainement.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnuFhohqZIHK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "9ec891f8-4e94-45e0-d6f6-9e3766cbfa19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction de la période\")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IL est clair que notre modèle n'est pas bon puiqu'il surapprend à une allure exponentielle. Notre essayerons une autre modèle plus adapté en diminuant le taux d'apprentissage lorsque le nombre de période augmente.**"
      ],
      "metadata": {
        "id": "WLOF-yQTfD2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UcaqI7Qvu4iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJKluFmm4kSL"
      },
      "source": [
        "# 4. Adaptation du taux d'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-Yrk8TB620S"
      },
      "outputs": [],
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**On commence par une taux d'apprentissage de 0,1 et on fait en sorte que pour chaque période supplémentaire il soit diminuer de 1%.**"
      ],
      "metadata": {
        "id": "bZTSCjYtgMgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNVIYw744shF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9511a1-cbec-42f6-dc57-587d53638d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "     67/Unknown - 3s 12ms/step - loss: 0.2345 - mae: 0.5589\n",
            "Epoch 1: loss improved from inf to 0.23454, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 5s 35ms/step - loss: 0.2345 - mae: 0.5592 - val_loss: 0.1861 - val_mae: 0.4852\n",
            "Epoch 2/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.2003 - mae: 0.5097\n",
            "Epoch 2: loss improved from 0.23454 to 0.20047, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.2005 - mae: 0.5104 - val_loss: 0.1732 - val_mae: 0.4674\n",
            "Epoch 3/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1932 - mae: 0.4993\n",
            "Epoch 3: loss improved from 0.20047 to 0.19415, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1942 - mae: 0.5008 - val_loss: 0.1747 - val_mae: 0.4734\n",
            "Epoch 4/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1868 - mae: 0.4904\n",
            "Epoch 4: loss improved from 0.19415 to 0.18622, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1862 - mae: 0.4894 - val_loss: 0.1728 - val_mae: 0.4686\n",
            "Epoch 5/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1886 - mae: 0.4946\n",
            "Epoch 5: loss did not improve from 0.18622\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1884 - mae: 0.4940 - val_loss: 0.1834 - val_mae: 0.4858\n",
            "Epoch 6/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1856 - mae: 0.4889\n",
            "Epoch 6: loss improved from 0.18622 to 0.18551, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1855 - mae: 0.4887 - val_loss: 0.1649 - val_mae: 0.4603\n",
            "Epoch 7/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1860 - mae: 0.4896\n",
            "Epoch 7: loss did not improve from 0.18551\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1860 - mae: 0.4896 - val_loss: 0.1572 - val_mae: 0.4458\n",
            "Epoch 8/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1844 - mae: 0.4875\n",
            "Epoch 8: loss improved from 0.18551 to 0.18436, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1844 - mae: 0.4875 - val_loss: 0.1691 - val_mae: 0.4649\n",
            "Epoch 9/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1806 - mae: 0.4806\n",
            "Epoch 9: loss improved from 0.18436 to 0.18079, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1808 - mae: 0.4810 - val_loss: 0.1734 - val_mae: 0.4701\n",
            "Epoch 10/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1875 - mae: 0.4914\n",
            "Epoch 10: loss did not improve from 0.18079\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1840 - mae: 0.4861 - val_loss: 0.1594 - val_mae: 0.4514\n",
            "Epoch 11/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1796 - mae: 0.4807\n",
            "Epoch 11: loss improved from 0.18079 to 0.17829, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1783 - mae: 0.4785 - val_loss: 0.1631 - val_mae: 0.4545\n",
            "Epoch 12/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1845 - mae: 0.4890\n",
            "Epoch 12: loss did not improve from 0.17829\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1845 - mae: 0.4890 - val_loss: 0.1574 - val_mae: 0.4451\n",
            "Epoch 13/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1789 - mae: 0.4784\n",
            "Epoch 13: loss did not improve from 0.17829\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1789 - mae: 0.4784 - val_loss: 0.1578 - val_mae: 0.4461\n",
            "Epoch 14/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1790 - mae: 0.4791\n",
            "Epoch 14: loss did not improve from 0.17829\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1793 - mae: 0.4790 - val_loss: 0.1619 - val_mae: 0.4540\n",
            "Epoch 15/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1794 - mae: 0.4811\n",
            "Epoch 15: loss did not improve from 0.17829\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1794 - mae: 0.4810 - val_loss: 0.1573 - val_mae: 0.4448\n",
            "Epoch 16/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1770 - mae: 0.4759\n",
            "Epoch 16: loss improved from 0.17829 to 0.17703, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1770 - mae: 0.4759 - val_loss: 0.1585 - val_mae: 0.4493\n",
            "Epoch 17/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1773 - mae: 0.4783\n",
            "Epoch 17: loss did not improve from 0.17703\n",
            "68/68 [==============================] - 2s 19ms/step - loss: 0.1778 - mae: 0.4792 - val_loss: 0.1593 - val_mae: 0.4480\n",
            "Epoch 18/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1766 - mae: 0.4782\n",
            "Epoch 18: loss did not improve from 0.17703\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1777 - mae: 0.4783 - val_loss: 0.1618 - val_mae: 0.4518\n",
            "Epoch 19/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1785 - mae: 0.4785\n",
            "Epoch 19: loss did not improve from 0.17703\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1785 - mae: 0.4785 - val_loss: 0.1588 - val_mae: 0.4479\n",
            "Epoch 20/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1763 - mae: 0.4762\n",
            "Epoch 20: loss did not improve from 0.17703\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1771 - mae: 0.4767 - val_loss: 0.1591 - val_mae: 0.4479\n",
            "Epoch 21/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4775\n",
            "Epoch 21: loss improved from 0.17703 to 0.17686, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1769 - mae: 0.4775 - val_loss: 0.1593 - val_mae: 0.4483\n",
            "Epoch 22/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1780 - mae: 0.4796\n",
            "Epoch 22: loss did not improve from 0.17686\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1785 - mae: 0.4810 - val_loss: 0.1594 - val_mae: 0.4476\n",
            "Epoch 23/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4755\n",
            "Epoch 23: loss improved from 0.17686 to 0.17632, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1763 - mae: 0.4755 - val_loss: 0.1555 - val_mae: 0.4431\n",
            "Epoch 24/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1774 - mae: 0.4783\n",
            "Epoch 24: loss did not improve from 0.17632\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1770 - mae: 0.4778 - val_loss: 0.1565 - val_mae: 0.4449\n",
            "Epoch 25/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1764 - mae: 0.4755\n",
            "Epoch 25: loss did not improve from 0.17632\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1764 - mae: 0.4755 - val_loss: 0.1553 - val_mae: 0.4420\n",
            "Epoch 26/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4742\n",
            "Epoch 26: loss improved from 0.17632 to 0.17542, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1754 - mae: 0.4731 - val_loss: 0.1570 - val_mae: 0.4450\n",
            "Epoch 27/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4788\n",
            "Epoch 27: loss did not improve from 0.17542\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1761 - mae: 0.4774 - val_loss: 0.1568 - val_mae: 0.4437\n",
            "Epoch 28/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4734\n",
            "Epoch 28: loss did not improve from 0.17542\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1759 - mae: 0.4744 - val_loss: 0.1553 - val_mae: 0.4438\n",
            "Epoch 29/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4715\n",
            "Epoch 29: loss improved from 0.17542 to 0.17392, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1739 - mae: 0.4716 - val_loss: 0.1588 - val_mae: 0.4487\n",
            "Epoch 30/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1722 - mae: 0.4685\n",
            "Epoch 30: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1755 - mae: 0.4739 - val_loss: 0.1551 - val_mae: 0.4421\n",
            "Epoch 31/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4756\n",
            "Epoch 31: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1761 - mae: 0.4754 - val_loss: 0.1581 - val_mae: 0.4488\n",
            "Epoch 32/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4755\n",
            "Epoch 32: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1567 - val_mae: 0.4464\n",
            "Epoch 33/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4743\n",
            "Epoch 33: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1559 - val_mae: 0.4434\n",
            "Epoch 34/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4773\n",
            "Epoch 34: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1772 - mae: 0.4781 - val_loss: 0.1551 - val_mae: 0.4420\n",
            "Epoch 35/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.4743\n",
            "Epoch 35: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1765 - mae: 0.4771 - val_loss: 0.1557 - val_mae: 0.4444\n",
            "Epoch 36/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4737\n",
            "Epoch 36: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1760 - mae: 0.4737 - val_loss: 0.1559 - val_mae: 0.4427\n",
            "Epoch 37/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4745\n",
            "Epoch 37: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1767 - mae: 0.4753 - val_loss: 0.1548 - val_mae: 0.4426\n",
            "Epoch 38/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4752\n",
            "Epoch 38: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1555 - val_mae: 0.4433\n",
            "Epoch 39/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1778 - mae: 0.4764\n",
            "Epoch 39: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 2s 19ms/step - loss: 0.1778 - mae: 0.4764 - val_loss: 0.1596 - val_mae: 0.4494\n",
            "Epoch 40/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1751 - mae: 0.4733\n",
            "Epoch 40: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1751 - mae: 0.4733 - val_loss: 0.1576 - val_mae: 0.4465\n",
            "Epoch 41/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1747 - mae: 0.4740\n",
            "Epoch 41: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1747 - mae: 0.4740 - val_loss: 0.1569 - val_mae: 0.4451\n",
            "Epoch 42/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1744 - mae: 0.4720\n",
            "Epoch 42: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1743 - mae: 0.4719 - val_loss: 0.1559 - val_mae: 0.4423\n",
            "Epoch 43/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4737\n",
            "Epoch 43: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1751 - mae: 0.4729 - val_loss: 0.1562 - val_mae: 0.4440\n",
            "Epoch 44/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1764 - mae: 0.4745\n",
            "Epoch 44: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1744 - mae: 0.4713 - val_loss: 0.1585 - val_mae: 0.4489\n",
            "Epoch 45/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1753 - mae: 0.4737\n",
            "Epoch 45: loss did not improve from 0.17392\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1753 - mae: 0.4737 - val_loss: 0.1556 - val_mae: 0.4426\n",
            "Epoch 46/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4711\n",
            "Epoch 46: loss improved from 0.17392 to 0.17367, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1737 - mae: 0.4707 - val_loss: 0.1580 - val_mae: 0.4457\n",
            "Epoch 47/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1745 - mae: 0.4732\n",
            "Epoch 47: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1745 - mae: 0.4732 - val_loss: 0.1530 - val_mae: 0.4381\n",
            "Epoch 48/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1734 - mae: 0.4698\n",
            "Epoch 48: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1743 - mae: 0.4718 - val_loss: 0.1558 - val_mae: 0.4432\n",
            "Epoch 49/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4705\n",
            "Epoch 49: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1739 - mae: 0.4716 - val_loss: 0.1557 - val_mae: 0.4419\n",
            "Epoch 50/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1748 - mae: 0.4728\n",
            "Epoch 50: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 2s 19ms/step - loss: 0.1748 - mae: 0.4728 - val_loss: 0.1552 - val_mae: 0.4432\n",
            "Epoch 51/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1770 - mae: 0.4755\n",
            "Epoch 51: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1751 - mae: 0.4725 - val_loss: 0.1539 - val_mae: 0.4412\n",
            "Epoch 52/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1769 - mae: 0.4764\n",
            "Epoch 52: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1748 - mae: 0.4736 - val_loss: 0.1566 - val_mae: 0.4453\n",
            "Epoch 53/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1750 - mae: 0.4748\n",
            "Epoch 53: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1750 - mae: 0.4748 - val_loss: 0.1544 - val_mae: 0.4424\n",
            "Epoch 54/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4714\n",
            "Epoch 54: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1743 - mae: 0.4711 - val_loss: 0.1557 - val_mae: 0.4445\n",
            "Epoch 55/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4727\n",
            "Epoch 55: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1741 - mae: 0.4730 - val_loss: 0.1563 - val_mae: 0.4439\n",
            "Epoch 56/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4709\n",
            "Epoch 56: loss did not improve from 0.17367\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1743 - mae: 0.4714 - val_loss: 0.1542 - val_mae: 0.4418\n",
            "Epoch 57/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1764 - mae: 0.4747\n",
            "Epoch 57: loss improved from 0.17367 to 0.17337, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1734 - mae: 0.4708 - val_loss: 0.1576 - val_mae: 0.4458\n",
            "Epoch 58/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4699\n",
            "Epoch 58: loss did not improve from 0.17337\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1740 - mae: 0.4701 - val_loss: 0.1583 - val_mae: 0.4474\n",
            "Epoch 59/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1723 - mae: 0.4685\n",
            "Epoch 59: loss did not improve from 0.17337\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1739 - mae: 0.4719 - val_loss: 0.1560 - val_mae: 0.4434\n",
            "Epoch 60/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1738 - mae: 0.4720\n",
            "Epoch 60: loss did not improve from 0.17337\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1752 - mae: 0.4738 - val_loss: 0.1579 - val_mae: 0.4463\n",
            "Epoch 61/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4721\n",
            "Epoch 61: loss did not improve from 0.17337\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1743 - mae: 0.4718 - val_loss: 0.1540 - val_mae: 0.4412\n",
            "Epoch 62/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1757 - mae: 0.4736\n",
            "Epoch 62: loss did not improve from 0.17337\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1743 - mae: 0.4713 - val_loss: 0.1565 - val_mae: 0.4442\n",
            "Epoch 63/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1743 - mae: 0.4723\n",
            "Epoch 63: loss did not improve from 0.17337\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1750 - mae: 0.4731 - val_loss: 0.1560 - val_mae: 0.4448\n",
            "Epoch 64/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4743\n",
            "Epoch 64: loss did not improve from 0.17337\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1543 - val_mae: 0.4418\n",
            "Epoch 65/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4720\n",
            "Epoch 65: loss did not improve from 0.17337\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1746 - mae: 0.4728 - val_loss: 0.1559 - val_mae: 0.4435\n",
            "Epoch 66/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4724\n",
            "Epoch 66: loss did not improve from 0.17337\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1742 - mae: 0.4719 - val_loss: 0.1543 - val_mae: 0.4427\n",
            "Epoch 67/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4700\n",
            "Epoch 67: loss improved from 0.17337 to 0.17319, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1732 - mae: 0.4708 - val_loss: 0.1537 - val_mae: 0.4409\n",
            "Epoch 68/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1741 - mae: 0.4720\n",
            "Epoch 68: loss did not improve from 0.17319\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1733 - mae: 0.4705 - val_loss: 0.1564 - val_mae: 0.4438\n",
            "Epoch 69/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1718 - mae: 0.4683\n",
            "Epoch 69: loss did not improve from 0.17319\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1733 - mae: 0.4708 - val_loss: 0.1558 - val_mae: 0.4439\n",
            "Epoch 70/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4710\n",
            "Epoch 70: loss did not improve from 0.17319\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1737 - mae: 0.4706 - val_loss: 0.1562 - val_mae: 0.4432\n",
            "Epoch 71/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1730 - mae: 0.4712\n",
            "Epoch 71: loss did not improve from 0.17319\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1733 - mae: 0.4713 - val_loss: 0.1557 - val_mae: 0.4458\n",
            "Epoch 72/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1738 - mae: 0.4736\n",
            "Epoch 72: loss improved from 0.17319 to 0.17311, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1731 - mae: 0.4716 - val_loss: 0.1577 - val_mae: 0.4460\n",
            "Epoch 73/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4720\n",
            "Epoch 73: loss did not improve from 0.17311\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1742 - mae: 0.4719 - val_loss: 0.1549 - val_mae: 0.4420\n",
            "Epoch 74/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4683\n",
            "Epoch 74: loss did not improve from 0.17311\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1563 - val_mae: 0.4441\n",
            "Epoch 75/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4703\n",
            "Epoch 75: loss did not improve from 0.17311\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1733 - mae: 0.4705 - val_loss: 0.1554 - val_mae: 0.4434\n",
            "Epoch 76/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1726 - mae: 0.4696\n",
            "Epoch 76: loss did not improve from 0.17311\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1731 - mae: 0.4708 - val_loss: 0.1543 - val_mae: 0.4405\n",
            "Epoch 77/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1763 - mae: 0.4758\n",
            "Epoch 77: loss did not improve from 0.17311\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1735 - mae: 0.4713 - val_loss: 0.1560 - val_mae: 0.4426\n",
            "Epoch 78/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1740 - mae: 0.4723\n",
            "Epoch 78: loss did not improve from 0.17311\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1741 - mae: 0.4721 - val_loss: 0.1544 - val_mae: 0.4406\n",
            "Epoch 79/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.4688\n",
            "Epoch 79: loss improved from 0.17311 to 0.17263, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1726 - mae: 0.4698 - val_loss: 0.1560 - val_mae: 0.4433\n",
            "Epoch 80/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4706\n",
            "Epoch 80: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1734 - mae: 0.4706 - val_loss: 0.1555 - val_mae: 0.4437\n",
            "Epoch 81/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1726 - mae: 0.4696\n",
            "Epoch 81: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1726 - mae: 0.4696 - val_loss: 0.1548 - val_mae: 0.4427\n",
            "Epoch 82/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1719 - mae: 0.4689\n",
            "Epoch 82: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1732 - mae: 0.4709 - val_loss: 0.1520 - val_mae: 0.4389\n",
            "Epoch 83/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4730\n",
            "Epoch 83: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1738 - mae: 0.4710 - val_loss: 0.1537 - val_mae: 0.4413\n",
            "Epoch 84/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.4709\n",
            "Epoch 84: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1738 - mae: 0.4713 - val_loss: 0.1548 - val_mae: 0.4420\n",
            "Epoch 85/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4714\n",
            "Epoch 85: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1726 - mae: 0.4704 - val_loss: 0.1574 - val_mae: 0.4458\n",
            "Epoch 86/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4714\n",
            "Epoch 86: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1735 - mae: 0.4702 - val_loss: 0.1560 - val_mae: 0.4446\n",
            "Epoch 87/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1755 - mae: 0.4737\n",
            "Epoch 87: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1732 - mae: 0.4710 - val_loss: 0.1564 - val_mae: 0.4453\n",
            "Epoch 88/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4692\n",
            "Epoch 88: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1734 - mae: 0.4696 - val_loss: 0.1538 - val_mae: 0.4395\n",
            "Epoch 89/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.4694\n",
            "Epoch 89: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4710 - val_loss: 0.1546 - val_mae: 0.4407\n",
            "Epoch 90/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1736 - mae: 0.4713\n",
            "Epoch 90: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1730 - mae: 0.4700 - val_loss: 0.1535 - val_mae: 0.4397\n",
            "Epoch 91/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1719 - mae: 0.4670\n",
            "Epoch 91: loss did not improve from 0.17263\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1735 - mae: 0.4705 - val_loss: 0.1558 - val_mae: 0.4436\n",
            "Epoch 92/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1722 - mae: 0.4682\n",
            "Epoch 92: loss improved from 0.17263 to 0.17254, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1725 - mae: 0.4692 - val_loss: 0.1537 - val_mae: 0.4410\n",
            "Epoch 93/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4707\n",
            "Epoch 93: loss did not improve from 0.17254\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1733 - mae: 0.4707 - val_loss: 0.1560 - val_mae: 0.4434\n",
            "Epoch 94/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4683\n",
            "Epoch 94: loss improved from 0.17254 to 0.17225, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1723 - mae: 0.4683 - val_loss: 0.1550 - val_mae: 0.4434\n",
            "Epoch 95/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.4697\n",
            "Epoch 95: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1728 - mae: 0.4698 - val_loss: 0.1534 - val_mae: 0.4409\n",
            "Epoch 96/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4699\n",
            "Epoch 96: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1732 - mae: 0.4703 - val_loss: 0.1525 - val_mae: 0.4384\n",
            "Epoch 97/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4695\n",
            "Epoch 97: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1733 - mae: 0.4702 - val_loss: 0.1538 - val_mae: 0.4403\n",
            "Epoch 98/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4726\n",
            "Epoch 98: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1731 - mae: 0.4702 - val_loss: 0.1532 - val_mae: 0.4409\n",
            "Epoch 99/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1726 - mae: 0.4701\n",
            "Epoch 99: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1726 - mae: 0.4701 - val_loss: 0.1543 - val_mae: 0.4413\n",
            "Epoch 100/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4686\n",
            "Epoch 100: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1731 - mae: 0.4693 - val_loss: 0.1534 - val_mae: 0.4401\n",
            "Epoch 101/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1724 - mae: 0.4689\n",
            "Epoch 101: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1724 - mae: 0.4689 - val_loss: 0.1550 - val_mae: 0.4415\n",
            "Epoch 102/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4714\n",
            "Epoch 102: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1733 - mae: 0.4698 - val_loss: 0.1535 - val_mae: 0.4399\n",
            "Epoch 103/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4732\n",
            "Epoch 103: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1723 - mae: 0.4690 - val_loss: 0.1552 - val_mae: 0.4423\n",
            "Epoch 104/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4712\n",
            "Epoch 104: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1731 - mae: 0.4698 - val_loss: 0.1544 - val_mae: 0.4413\n",
            "Epoch 105/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4670\n",
            "Epoch 105: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1724 - mae: 0.4684 - val_loss: 0.1549 - val_mae: 0.4431\n",
            "Epoch 106/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4710\n",
            "Epoch 106: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1725 - mae: 0.4703 - val_loss: 0.1534 - val_mae: 0.4408\n",
            "Epoch 107/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1714 - mae: 0.4686\n",
            "Epoch 107: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1728 - mae: 0.4700 - val_loss: 0.1540 - val_mae: 0.4405\n",
            "Epoch 108/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1714 - mae: 0.4671\n",
            "Epoch 108: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1733 - mae: 0.4697 - val_loss: 0.1527 - val_mae: 0.4398\n",
            "Epoch 109/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1719 - mae: 0.4682\n",
            "Epoch 109: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1730 - mae: 0.4700 - val_loss: 0.1546 - val_mae: 0.4412\n",
            "Epoch 110/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4699\n",
            "Epoch 110: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1726 - mae: 0.4693 - val_loss: 0.1595 - val_mae: 0.4480\n",
            "Epoch 111/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1726 - mae: 0.4686\n",
            "Epoch 111: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1726 - mae: 0.4689 - val_loss: 0.1546 - val_mae: 0.4412\n",
            "Epoch 112/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4697\n",
            "Epoch 112: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1727 - mae: 0.4694 - val_loss: 0.1580 - val_mae: 0.4455\n",
            "Epoch 113/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4689\n",
            "Epoch 113: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1729 - mae: 0.4695 - val_loss: 0.1539 - val_mae: 0.4406\n",
            "Epoch 114/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1703 - mae: 0.4665\n",
            "Epoch 114: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1726 - mae: 0.4696 - val_loss: 0.1576 - val_mae: 0.4459\n",
            "Epoch 115/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1728 - mae: 0.4692\n",
            "Epoch 115: loss did not improve from 0.17225\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1728 - mae: 0.4692 - val_loss: 0.1552 - val_mae: 0.4429\n",
            "Epoch 116/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1714 - mae: 0.4675\n",
            "Epoch 116: loss improved from 0.17225 to 0.17136, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1714 - mae: 0.4675 - val_loss: 0.1534 - val_mae: 0.4394\n",
            "Epoch 117/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4712\n",
            "Epoch 117: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1728 - mae: 0.4700 - val_loss: 0.1548 - val_mae: 0.4418\n",
            "Epoch 118/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1724 - mae: 0.4690\n",
            "Epoch 118: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4690 - val_loss: 0.1540 - val_mae: 0.4403\n",
            "Epoch 119/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1724 - mae: 0.4686\n",
            "Epoch 119: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1714 - mae: 0.4669 - val_loss: 0.1544 - val_mae: 0.4414\n",
            "Epoch 120/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4709\n",
            "Epoch 120: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1734 - mae: 0.4708 - val_loss: 0.1535 - val_mae: 0.4400\n",
            "Epoch 121/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4686\n",
            "Epoch 121: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1722 - mae: 0.4681 - val_loss: 0.1543 - val_mae: 0.4399\n",
            "Epoch 122/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4714\n",
            "Epoch 122: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1725 - mae: 0.4688 - val_loss: 0.1558 - val_mae: 0.4451\n",
            "Epoch 123/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4695\n",
            "Epoch 123: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1723 - mae: 0.4695 - val_loss: 0.1534 - val_mae: 0.4390\n",
            "Epoch 124/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4700\n",
            "Epoch 124: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1723 - mae: 0.4696 - val_loss: 0.1561 - val_mae: 0.4434\n",
            "Epoch 125/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1705 - mae: 0.4657\n",
            "Epoch 125: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1722 - mae: 0.4680 - val_loss: 0.1538 - val_mae: 0.4403\n",
            "Epoch 126/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1726 - mae: 0.4693\n",
            "Epoch 126: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1726 - mae: 0.4693 - val_loss: 0.1543 - val_mae: 0.4405\n",
            "Epoch 127/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4714\n",
            "Epoch 127: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1720 - mae: 0.4681 - val_loss: 0.1560 - val_mae: 0.4435\n",
            "Epoch 128/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1727 - mae: 0.4694\n",
            "Epoch 128: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1719 - mae: 0.4683 - val_loss: 0.1554 - val_mae: 0.4436\n",
            "Epoch 129/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1717 - mae: 0.4683\n",
            "Epoch 129: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1717 - mae: 0.4683 - val_loss: 0.1541 - val_mae: 0.4399\n",
            "Epoch 130/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1732 - mae: 0.4705\n",
            "Epoch 130: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1718 - mae: 0.4681 - val_loss: 0.1537 - val_mae: 0.4407\n",
            "Epoch 131/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1719 - mae: 0.4694\n",
            "Epoch 131: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1722 - mae: 0.4691 - val_loss: 0.1558 - val_mae: 0.4432\n",
            "Epoch 132/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4685\n",
            "Epoch 132: loss did not improve from 0.17136\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4676 - val_loss: 0.1569 - val_mae: 0.4441\n",
            "Epoch 133/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4705\n",
            "Epoch 133: loss improved from 0.17136 to 0.17091, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 19ms/step - loss: 0.1709 - mae: 0.4671 - val_loss: 0.1541 - val_mae: 0.4403\n",
            "Epoch 134/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1726 - mae: 0.4690\n",
            "Epoch 134: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1722 - mae: 0.4679 - val_loss: 0.1538 - val_mae: 0.4402\n",
            "Epoch 135/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1709 - mae: 0.4660\n",
            "Epoch 135: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1717 - mae: 0.4671 - val_loss: 0.1537 - val_mae: 0.4398\n",
            "Epoch 136/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1709 - mae: 0.4667\n",
            "Epoch 136: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1715 - mae: 0.4671 - val_loss: 0.1554 - val_mae: 0.4420\n",
            "Epoch 137/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1717 - mae: 0.4681\n",
            "Epoch 137: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1717 - mae: 0.4681 - val_loss: 0.1544 - val_mae: 0.4418\n",
            "Epoch 138/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1706 - mae: 0.4655\n",
            "Epoch 138: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1718 - mae: 0.4684 - val_loss: 0.1551 - val_mae: 0.4425\n",
            "Epoch 139/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1719 - mae: 0.4686\n",
            "Epoch 139: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1719 - mae: 0.4686 - val_loss: 0.1549 - val_mae: 0.4411\n",
            "Epoch 140/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4680\n",
            "Epoch 140: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1724 - mae: 0.4695 - val_loss: 0.1544 - val_mae: 0.4404\n",
            "Epoch 141/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1734 - mae: 0.4723\n",
            "Epoch 141: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1724 - mae: 0.4690 - val_loss: 0.1540 - val_mae: 0.4407\n",
            "Epoch 142/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1717 - mae: 0.4665\n",
            "Epoch 142: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1719 - mae: 0.4683 - val_loss: 0.1549 - val_mae: 0.4406\n",
            "Epoch 143/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4675\n",
            "Epoch 143: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1722 - mae: 0.4680 - val_loss: 0.1539 - val_mae: 0.4393\n",
            "Epoch 144/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1720 - mae: 0.4680\n",
            "Epoch 144: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1720 - mae: 0.4680 - val_loss: 0.1543 - val_mae: 0.4410\n",
            "Epoch 145/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1708 - mae: 0.4672\n",
            "Epoch 145: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1709 - mae: 0.4673 - val_loss: 0.1567 - val_mae: 0.4461\n",
            "Epoch 146/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4725\n",
            "Epoch 146: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1719 - mae: 0.4685 - val_loss: 0.1556 - val_mae: 0.4428\n",
            "Epoch 147/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4685\n",
            "Epoch 147: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1717 - mae: 0.4681 - val_loss: 0.1554 - val_mae: 0.4423\n",
            "Epoch 148/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4689\n",
            "Epoch 148: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1715 - mae: 0.4670 - val_loss: 0.1539 - val_mae: 0.4403\n",
            "Epoch 149/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1714 - mae: 0.4664\n",
            "Epoch 149: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1714 - mae: 0.4664 - val_loss: 0.1547 - val_mae: 0.4422\n",
            "Epoch 150/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1713 - mae: 0.4678\n",
            "Epoch 150: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1713 - mae: 0.4678 - val_loss: 0.1544 - val_mae: 0.4401\n",
            "Epoch 151/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1726 - mae: 0.4693\n",
            "Epoch 151: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1717 - mae: 0.4680 - val_loss: 0.1553 - val_mae: 0.4431\n",
            "Epoch 152/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4699\n",
            "Epoch 152: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1716 - mae: 0.4678 - val_loss: 0.1548 - val_mae: 0.4408\n",
            "Epoch 153/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1718 - mae: 0.4679\n",
            "Epoch 153: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1718 - mae: 0.4682 - val_loss: 0.1553 - val_mae: 0.4414\n",
            "Epoch 154/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4682\n",
            "Epoch 154: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1724 - mae: 0.4687 - val_loss: 0.1552 - val_mae: 0.4425\n",
            "Epoch 155/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4693\n",
            "Epoch 155: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1711 - mae: 0.4664 - val_loss: 0.1545 - val_mae: 0.4417\n",
            "Epoch 156/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4701\n",
            "Epoch 156: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1718 - mae: 0.4672 - val_loss: 0.1539 - val_mae: 0.4404\n",
            "Epoch 157/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1719 - mae: 0.4683\n",
            "Epoch 157: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1719 - mae: 0.4683 - val_loss: 0.1554 - val_mae: 0.4417\n",
            "Epoch 158/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1714 - mae: 0.4668\n",
            "Epoch 158: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1714 - mae: 0.4668 - val_loss: 0.1545 - val_mae: 0.4409\n",
            "Epoch 159/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1713 - mae: 0.4675\n",
            "Epoch 159: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1713 - mae: 0.4675 - val_loss: 0.1543 - val_mae: 0.4408\n",
            "Epoch 160/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1714 - mae: 0.4680\n",
            "Epoch 160: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1714 - mae: 0.4680 - val_loss: 0.1539 - val_mae: 0.4401\n",
            "Epoch 161/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1734 - mae: 0.4702\n",
            "Epoch 161: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1712 - mae: 0.4669 - val_loss: 0.1553 - val_mae: 0.4423\n",
            "Epoch 162/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1733 - mae: 0.4693\n",
            "Epoch 162: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1713 - mae: 0.4663 - val_loss: 0.1544 - val_mae: 0.4409\n",
            "Epoch 163/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1722 - mae: 0.4684\n",
            "Epoch 163: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1716 - mae: 0.4679 - val_loss: 0.1544 - val_mae: 0.4416\n",
            "Epoch 164/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1711 - mae: 0.4676\n",
            "Epoch 164: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1716 - mae: 0.4677 - val_loss: 0.1543 - val_mae: 0.4405\n",
            "Epoch 165/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1718 - mae: 0.4682\n",
            "Epoch 165: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1718 - mae: 0.4681 - val_loss: 0.1553 - val_mae: 0.4431\n",
            "Epoch 166/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1705 - mae: 0.4663\n",
            "Epoch 166: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1714 - mae: 0.4675 - val_loss: 0.1549 - val_mae: 0.4422\n",
            "Epoch 167/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1713 - mae: 0.4673\n",
            "Epoch 167: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1713 - mae: 0.4673 - val_loss: 0.1539 - val_mae: 0.4405\n",
            "Epoch 168/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1710 - mae: 0.4661\n",
            "Epoch 168: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1714 - mae: 0.4671 - val_loss: 0.1542 - val_mae: 0.4412\n",
            "Epoch 169/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1704 - mae: 0.4659\n",
            "Epoch 169: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1712 - mae: 0.4669 - val_loss: 0.1541 - val_mae: 0.4405\n",
            "Epoch 170/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1719 - mae: 0.4684\n",
            "Epoch 170: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1713 - mae: 0.4674 - val_loss: 0.1533 - val_mae: 0.4391\n",
            "Epoch 171/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1706 - mae: 0.4664\n",
            "Epoch 171: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1711 - mae: 0.4668 - val_loss: 0.1543 - val_mae: 0.4414\n",
            "Epoch 172/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1714 - mae: 0.4677\n",
            "Epoch 172: loss did not improve from 0.17091\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1714 - mae: 0.4677 - val_loss: 0.1539 - val_mae: 0.4393\n",
            "Epoch 173/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1682 - mae: 0.4645\n",
            "Epoch 173: loss improved from 0.17091 to 0.17077, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1708 - mae: 0.4669 - val_loss: 0.1543 - val_mae: 0.4409\n",
            "Epoch 174/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1723 - mae: 0.4685\n",
            "Epoch 174: loss improved from 0.17077 to 0.17041, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1704 - mae: 0.4651 - val_loss: 0.1540 - val_mae: 0.4402\n",
            "Epoch 175/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1716 - mae: 0.4683\n",
            "Epoch 175: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1716 - mae: 0.4683 - val_loss: 0.1564 - val_mae: 0.4434\n",
            "Epoch 176/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1715 - mae: 0.4673\n",
            "Epoch 176: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1715 - mae: 0.4673 - val_loss: 0.1550 - val_mae: 0.4415\n",
            "Epoch 177/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1709 - mae: 0.4665\n",
            "Epoch 177: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1709 - mae: 0.4665 - val_loss: 0.1542 - val_mae: 0.4400\n",
            "Epoch 178/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1708 - mae: 0.4653\n",
            "Epoch 178: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1710 - mae: 0.4662 - val_loss: 0.1546 - val_mae: 0.4407\n",
            "Epoch 179/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4687\n",
            "Epoch 179: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1712 - mae: 0.4664 - val_loss: 0.1547 - val_mae: 0.4407\n",
            "Epoch 180/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1712 - mae: 0.4676\n",
            "Epoch 180: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1712 - mae: 0.4676 - val_loss: 0.1528 - val_mae: 0.4388\n",
            "Epoch 181/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1713 - mae: 0.4669\n",
            "Epoch 181: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1717 - mae: 0.4677 - val_loss: 0.1542 - val_mae: 0.4409\n",
            "Epoch 182/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4695\n",
            "Epoch 182: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1719 - mae: 0.4680 - val_loss: 0.1530 - val_mae: 0.4388\n",
            "Epoch 183/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1723 - mae: 0.4689\n",
            "Epoch 183: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1713 - mae: 0.4667 - val_loss: 0.1537 - val_mae: 0.4394\n",
            "Epoch 184/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1702 - mae: 0.4648\n",
            "Epoch 184: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1708 - mae: 0.4662 - val_loss: 0.1546 - val_mae: 0.4411\n",
            "Epoch 185/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1717 - mae: 0.4665\n",
            "Epoch 185: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1705 - mae: 0.4655 - val_loss: 0.1536 - val_mae: 0.4400\n",
            "Epoch 186/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1711 - mae: 0.4677\n",
            "Epoch 186: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1711 - mae: 0.4673 - val_loss: 0.1549 - val_mae: 0.4416\n",
            "Epoch 187/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1718 - mae: 0.4672\n",
            "Epoch 187: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1706 - mae: 0.4663 - val_loss: 0.1548 - val_mae: 0.4410\n",
            "Epoch 188/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1711 - mae: 0.4670\n",
            "Epoch 188: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1711 - mae: 0.4670 - val_loss: 0.1532 - val_mae: 0.4394\n",
            "Epoch 189/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1696 - mae: 0.4642\n",
            "Epoch 189: loss did not improve from 0.17041\n",
            "68/68 [==============================] - 2s 19ms/step - loss: 0.1707 - mae: 0.4658 - val_loss: 0.1540 - val_mae: 0.4407\n",
            "Epoch 190/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1699 - mae: 0.4657\n",
            "Epoch 190: loss improved from 0.17041 to 0.17040, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1704 - mae: 0.4660 - val_loss: 0.1540 - val_mae: 0.4404\n",
            "Epoch 191/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1713 - mae: 0.4675\n",
            "Epoch 191: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1706 - mae: 0.4659 - val_loss: 0.1538 - val_mae: 0.4404\n",
            "Epoch 192/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1701 - mae: 0.4653\n",
            "Epoch 192: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1709 - mae: 0.4661 - val_loss: 0.1545 - val_mae: 0.4402\n",
            "Epoch 193/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4690\n",
            "Epoch 193: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1710 - mae: 0.4662 - val_loss: 0.1545 - val_mae: 0.4412\n",
            "Epoch 194/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1725 - mae: 0.4686\n",
            "Epoch 194: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1706 - mae: 0.4654 - val_loss: 0.1548 - val_mae: 0.4405\n",
            "Epoch 195/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.4681\n",
            "Epoch 195: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1711 - mae: 0.4665 - val_loss: 0.1539 - val_mae: 0.4402\n",
            "Epoch 196/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1713 - mae: 0.4671\n",
            "Epoch 196: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1710 - mae: 0.4665 - val_loss: 0.1538 - val_mae: 0.4406\n",
            "Epoch 197/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4695\n",
            "Epoch 197: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1709 - mae: 0.4666 - val_loss: 0.1548 - val_mae: 0.4417\n",
            "Epoch 198/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1711 - mae: 0.4670\n",
            "Epoch 198: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1711 - mae: 0.4670 - val_loss: 0.1544 - val_mae: 0.4407\n",
            "Epoch 199/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1709 - mae: 0.4669\n",
            "Epoch 199: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1711 - mae: 0.4670 - val_loss: 0.1546 - val_mae: 0.4415\n",
            "Epoch 200/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4675\n",
            "Epoch 200: loss did not improve from 0.17040\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1713 - mae: 0.4668 - val_loss: 0.1547 - val_mae: 0.4404\n",
            "Epoch 201/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1686 - mae: 0.4624\n",
            "Epoch 201: loss improved from 0.17040 to 0.17012, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1701 - mae: 0.4652 - val_loss: 0.1531 - val_mae: 0.4387\n",
            "Epoch 202/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1662 - mae: 0.4593\n",
            "Epoch 202: loss improved from 0.17012 to 0.17004, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1700 - mae: 0.4657 - val_loss: 0.1519 - val_mae: 0.4374\n",
            "Epoch 203/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1707 - mae: 0.4660\n",
            "Epoch 203: loss did not improve from 0.17004\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1710 - mae: 0.4666 - val_loss: 0.1551 - val_mae: 0.4421\n",
            "Epoch 204/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1706 - mae: 0.4654\n",
            "Epoch 204: loss did not improve from 0.17004\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1706 - mae: 0.4654 - val_loss: 0.1544 - val_mae: 0.4409\n",
            "Epoch 205/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1699 - mae: 0.4648\n",
            "Epoch 205: loss improved from 0.17004 to 0.16987, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1699 - mae: 0.4653 - val_loss: 0.1562 - val_mae: 0.4433\n",
            "Epoch 206/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1701 - mae: 0.4657\n",
            "Epoch 206: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1706 - mae: 0.4664 - val_loss: 0.1534 - val_mae: 0.4388\n",
            "Epoch 207/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1707 - mae: 0.4661\n",
            "Epoch 207: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1708 - mae: 0.4656 - val_loss: 0.1548 - val_mae: 0.4419\n",
            "Epoch 208/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1701 - mae: 0.4651\n",
            "Epoch 208: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1703 - mae: 0.4656 - val_loss: 0.1558 - val_mae: 0.4424\n",
            "Epoch 209/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1707 - mae: 0.4664\n",
            "Epoch 209: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1707 - mae: 0.4664 - val_loss: 0.1528 - val_mae: 0.4387\n",
            "Epoch 210/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4678\n",
            "Epoch 210: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1705 - mae: 0.4657 - val_loss: 0.1542 - val_mae: 0.4408\n",
            "Epoch 211/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1699 - mae: 0.4655\n",
            "Epoch 211: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1699 - mae: 0.4655 - val_loss: 0.1539 - val_mae: 0.4398\n",
            "Epoch 212/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1711 - mae: 0.4664\n",
            "Epoch 212: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1705 - mae: 0.4649 - val_loss: 0.1546 - val_mae: 0.4417\n",
            "Epoch 213/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.4676\n",
            "Epoch 213: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1711 - mae: 0.4672 - val_loss: 0.1553 - val_mae: 0.4424\n",
            "Epoch 214/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1691 - mae: 0.4631\n",
            "Epoch 214: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1703 - mae: 0.4653 - val_loss: 0.1536 - val_mae: 0.4395\n",
            "Epoch 215/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1698 - mae: 0.4657\n",
            "Epoch 215: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1704 - mae: 0.4659 - val_loss: 0.1543 - val_mae: 0.4398\n",
            "Epoch 216/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1705 - mae: 0.4648\n",
            "Epoch 216: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1705 - mae: 0.4648 - val_loss: 0.1543 - val_mae: 0.4408\n",
            "Epoch 217/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1712 - mae: 0.4667\n",
            "Epoch 217: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1708 - mae: 0.4661 - val_loss: 0.1541 - val_mae: 0.4403\n",
            "Epoch 218/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1707 - mae: 0.4665\n",
            "Epoch 218: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1705 - mae: 0.4663 - val_loss: 0.1545 - val_mae: 0.4410\n",
            "Epoch 219/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1706 - mae: 0.4665\n",
            "Epoch 219: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1706 - mae: 0.4659 - val_loss: 0.1542 - val_mae: 0.4411\n",
            "Epoch 220/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1690 - mae: 0.4639\n",
            "Epoch 220: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1702 - mae: 0.4656 - val_loss: 0.1548 - val_mae: 0.4416\n",
            "Epoch 221/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1702 - mae: 0.4650\n",
            "Epoch 221: loss did not improve from 0.16987\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1710 - mae: 0.4665 - val_loss: 0.1548 - val_mae: 0.4408\n",
            "Epoch 222/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1706 - mae: 0.4663\n",
            "Epoch 222: loss improved from 0.16987 to 0.16980, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1698 - mae: 0.4649 - val_loss: 0.1551 - val_mae: 0.4417\n",
            "Epoch 223/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1678 - mae: 0.4606\n",
            "Epoch 223: loss did not improve from 0.16980\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1706 - mae: 0.4654 - val_loss: 0.1543 - val_mae: 0.4399\n",
            "Epoch 224/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1692 - mae: 0.4633\n",
            "Epoch 224: loss did not improve from 0.16980\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1704 - mae: 0.4655 - val_loss: 0.1544 - val_mae: 0.4413\n",
            "Epoch 225/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1702 - mae: 0.4649\n",
            "Epoch 225: loss did not improve from 0.16980\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1703 - mae: 0.4651 - val_loss: 0.1552 - val_mae: 0.4411\n",
            "Epoch 226/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1718 - mae: 0.4678\n",
            "Epoch 226: loss did not improve from 0.16980\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1706 - mae: 0.4662 - val_loss: 0.1547 - val_mae: 0.4409\n",
            "Epoch 227/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1696 - mae: 0.4648\n",
            "Epoch 227: loss improved from 0.16980 to 0.16960, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1696 - mae: 0.4648 - val_loss: 0.1568 - val_mae: 0.4437\n",
            "Epoch 228/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1702 - mae: 0.4652\n",
            "Epoch 228: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1701 - mae: 0.4656 - val_loss: 0.1542 - val_mae: 0.4403\n",
            "Epoch 229/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1706 - mae: 0.4655\n",
            "Epoch 229: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1706 - mae: 0.4655 - val_loss: 0.1553 - val_mae: 0.4425\n",
            "Epoch 230/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1705 - mae: 0.4658\n",
            "Epoch 230: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1703 - mae: 0.4655 - val_loss: 0.1541 - val_mae: 0.4411\n",
            "Epoch 231/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1688 - mae: 0.4627\n",
            "Epoch 231: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1699 - mae: 0.4655 - val_loss: 0.1540 - val_mae: 0.4402\n",
            "Epoch 232/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1699 - mae: 0.4647\n",
            "Epoch 232: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1705 - mae: 0.4653 - val_loss: 0.1548 - val_mae: 0.4414\n",
            "Epoch 233/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1690 - mae: 0.4648\n",
            "Epoch 233: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1701 - mae: 0.4658 - val_loss: 0.1540 - val_mae: 0.4397\n",
            "Epoch 234/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1693 - mae: 0.4645\n",
            "Epoch 234: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1705 - mae: 0.4659 - val_loss: 0.1546 - val_mae: 0.4399\n",
            "Epoch 235/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1699 - mae: 0.4646\n",
            "Epoch 235: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1697 - mae: 0.4643 - val_loss: 0.1566 - val_mae: 0.4448\n",
            "Epoch 236/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1706 - mae: 0.4676\n",
            "Epoch 236: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1708 - mae: 0.4671 - val_loss: 0.1544 - val_mae: 0.4405\n",
            "Epoch 237/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1689 - mae: 0.4632\n",
            "Epoch 237: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1700 - mae: 0.4644 - val_loss: 0.1537 - val_mae: 0.4403\n",
            "Epoch 238/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1701 - mae: 0.4652\n",
            "Epoch 238: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1700 - mae: 0.4657 - val_loss: 0.1541 - val_mae: 0.4405\n",
            "Epoch 239/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1710 - mae: 0.4660\n",
            "Epoch 239: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1704 - mae: 0.4654 - val_loss: 0.1544 - val_mae: 0.4413\n",
            "Epoch 240/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1705 - mae: 0.4664\n",
            "Epoch 240: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1697 - mae: 0.4645 - val_loss: 0.1543 - val_mae: 0.4400\n",
            "Epoch 241/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1709 - mae: 0.4666\n",
            "Epoch 241: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1709 - mae: 0.4666 - val_loss: 0.1536 - val_mae: 0.4391\n",
            "Epoch 242/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1687 - mae: 0.4625\n",
            "Epoch 242: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1701 - mae: 0.4646 - val_loss: 0.1547 - val_mae: 0.4408\n",
            "Epoch 243/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1697 - mae: 0.4630\n",
            "Epoch 243: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1699 - mae: 0.4646 - val_loss: 0.1547 - val_mae: 0.4409\n",
            "Epoch 244/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1708 - mae: 0.4665\n",
            "Epoch 244: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1701 - mae: 0.4648 - val_loss: 0.1523 - val_mae: 0.4369\n",
            "Epoch 245/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1702 - mae: 0.4652\n",
            "Epoch 245: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1702 - mae: 0.4652 - val_loss: 0.1546 - val_mae: 0.4410\n",
            "Epoch 246/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1679 - mae: 0.4624\n",
            "Epoch 246: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1701 - mae: 0.4648 - val_loss: 0.1560 - val_mae: 0.4427\n",
            "Epoch 247/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1699 - mae: 0.4646\n",
            "Epoch 247: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1699 - mae: 0.4646 - val_loss: 0.1537 - val_mae: 0.4394\n",
            "Epoch 248/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1695 - mae: 0.4642\n",
            "Epoch 248: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1699 - mae: 0.4650 - val_loss: 0.1542 - val_mae: 0.4398\n",
            "Epoch 249/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1705 - mae: 0.4649\n",
            "Epoch 249: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1698 - mae: 0.4644 - val_loss: 0.1553 - val_mae: 0.4416\n",
            "Epoch 250/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1693 - mae: 0.4644\n",
            "Epoch 250: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1699 - mae: 0.4645 - val_loss: 0.1547 - val_mae: 0.4398\n",
            "Epoch 251/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1703 - mae: 0.4650\n",
            "Epoch 251: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1703 - mae: 0.4648 - val_loss: 0.1542 - val_mae: 0.4396\n",
            "Epoch 252/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1686 - mae: 0.4617\n",
            "Epoch 252: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1699 - mae: 0.4642 - val_loss: 0.1549 - val_mae: 0.4413\n",
            "Epoch 253/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1699 - mae: 0.4647\n",
            "Epoch 253: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1699 - mae: 0.4647 - val_loss: 0.1532 - val_mae: 0.4387\n",
            "Epoch 254/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1697 - mae: 0.4643\n",
            "Epoch 254: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1697 - mae: 0.4643 - val_loss: 0.1542 - val_mae: 0.4406\n",
            "Epoch 255/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1696 - mae: 0.4633\n",
            "Epoch 255: loss did not improve from 0.16960\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1696 - mae: 0.4633 - val_loss: 0.1525 - val_mae: 0.4370\n",
            "Epoch 256/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1693 - mae: 0.4636\n",
            "Epoch 256: loss improved from 0.16960 to 0.16905, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1691 - mae: 0.4640 - val_loss: 0.1552 - val_mae: 0.4407\n",
            "Epoch 257/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1692 - mae: 0.4633\n",
            "Epoch 257: loss did not improve from 0.16905\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1700 - mae: 0.4639 - val_loss: 0.1546 - val_mae: 0.4396\n",
            "Epoch 258/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1694 - mae: 0.4626\n",
            "Epoch 258: loss did not improve from 0.16905\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1702 - mae: 0.4645 - val_loss: 0.1548 - val_mae: 0.4402\n",
            "Epoch 259/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1704 - mae: 0.4654\n",
            "Epoch 259: loss did not improve from 0.16905\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1698 - mae: 0.4644 - val_loss: 0.1547 - val_mae: 0.4401\n",
            "Epoch 260/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1683 - mae: 0.4618\n",
            "Epoch 260: loss did not improve from 0.16905\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1696 - mae: 0.4639 - val_loss: 0.1534 - val_mae: 0.4385\n",
            "Epoch 261/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1697 - mae: 0.4650\n",
            "Epoch 261: loss did not improve from 0.16905\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1698 - mae: 0.4653 - val_loss: 0.1538 - val_mae: 0.4400\n",
            "Epoch 262/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1700 - mae: 0.4647\n",
            "Epoch 262: loss improved from 0.16905 to 0.16894, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1689 - mae: 0.4633 - val_loss: 0.1538 - val_mae: 0.4398\n",
            "Epoch 263/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1697 - mae: 0.4639\n",
            "Epoch 263: loss did not improve from 0.16894\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1696 - mae: 0.4637 - val_loss: 0.1555 - val_mae: 0.4429\n",
            "Epoch 264/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1676 - mae: 0.4617\n",
            "Epoch 264: loss did not improve from 0.16894\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1696 - mae: 0.4644 - val_loss: 0.1524 - val_mae: 0.4377\n",
            "Epoch 265/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1696 - mae: 0.4639\n",
            "Epoch 265: loss did not improve from 0.16894\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1693 - mae: 0.4635 - val_loss: 0.1546 - val_mae: 0.4402\n",
            "Epoch 266/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1698 - mae: 0.4645\n",
            "Epoch 266: loss did not improve from 0.16894\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1698 - mae: 0.4640 - val_loss: 0.1545 - val_mae: 0.4409\n",
            "Epoch 267/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1714 - mae: 0.4655\n",
            "Epoch 267: loss improved from 0.16894 to 0.16875, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1688 - mae: 0.4620 - val_loss: 0.1547 - val_mae: 0.4408\n",
            "Epoch 268/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1681 - mae: 0.4612\n",
            "Epoch 268: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1691 - mae: 0.4630 - val_loss: 0.1532 - val_mae: 0.4397\n",
            "Epoch 269/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1690 - mae: 0.4643\n",
            "Epoch 269: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1698 - mae: 0.4655 - val_loss: 0.1547 - val_mae: 0.4409\n",
            "Epoch 270/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1702 - mae: 0.4642\n",
            "Epoch 270: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1694 - mae: 0.4632 - val_loss: 0.1532 - val_mae: 0.4387\n",
            "Epoch 271/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1685 - mae: 0.4636\n",
            "Epoch 271: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1692 - mae: 0.4640 - val_loss: 0.1536 - val_mae: 0.4394\n",
            "Epoch 272/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1690 - mae: 0.4632\n",
            "Epoch 272: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1690 - mae: 0.4632 - val_loss: 0.1545 - val_mae: 0.4402\n",
            "Epoch 273/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1698 - mae: 0.4659\n",
            "Epoch 273: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1697 - mae: 0.4655 - val_loss: 0.1542 - val_mae: 0.4398\n",
            "Epoch 274/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1698 - mae: 0.4644\n",
            "Epoch 274: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1697 - mae: 0.4643 - val_loss: 0.1544 - val_mae: 0.4400\n",
            "Epoch 275/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1688 - mae: 0.4628\n",
            "Epoch 275: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1693 - mae: 0.4634 - val_loss: 0.1529 - val_mae: 0.4385\n",
            "Epoch 276/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1690 - mae: 0.4626\n",
            "Epoch 276: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1694 - mae: 0.4629 - val_loss: 0.1554 - val_mae: 0.4429\n",
            "Epoch 277/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1690 - mae: 0.4642\n",
            "Epoch 277: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1696 - mae: 0.4650 - val_loss: 0.1554 - val_mae: 0.4416\n",
            "Epoch 278/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1696 - mae: 0.4645\n",
            "Epoch 278: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1696 - mae: 0.4645 - val_loss: 0.1558 - val_mae: 0.4423\n",
            "Epoch 279/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1691 - mae: 0.4628\n",
            "Epoch 279: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1693 - mae: 0.4628 - val_loss: 0.1558 - val_mae: 0.4431\n",
            "Epoch 280/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1691 - mae: 0.4640\n",
            "Epoch 280: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1691 - mae: 0.4640 - val_loss: 0.1532 - val_mae: 0.4384\n",
            "Epoch 281/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1709 - mae: 0.4651\n",
            "Epoch 281: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1695 - mae: 0.4635 - val_loss: 0.1550 - val_mae: 0.4415\n",
            "Epoch 282/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1690 - mae: 0.4636\n",
            "Epoch 282: loss improved from 0.16875 to 0.16875, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1687 - mae: 0.4634 - val_loss: 0.1543 - val_mae: 0.4413\n",
            "Epoch 283/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1699 - mae: 0.4646\n",
            "Epoch 283: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1692 - mae: 0.4633 - val_loss: 0.1536 - val_mae: 0.4389\n",
            "Epoch 284/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1695 - mae: 0.4631\n",
            "Epoch 284: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1693 - mae: 0.4638 - val_loss: 0.1541 - val_mae: 0.4399\n",
            "Epoch 285/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1695 - mae: 0.4640\n",
            "Epoch 285: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1690 - mae: 0.4631 - val_loss: 0.1547 - val_mae: 0.4402\n",
            "Epoch 286/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1690 - mae: 0.4638\n",
            "Epoch 286: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1694 - mae: 0.4644 - val_loss: 0.1552 - val_mae: 0.4408\n",
            "Epoch 287/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1681 - mae: 0.4609\n",
            "Epoch 287: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1691 - mae: 0.4632 - val_loss: 0.1540 - val_mae: 0.4402\n",
            "Epoch 288/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1707 - mae: 0.4667\n",
            "Epoch 288: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1691 - mae: 0.4636 - val_loss: 0.1541 - val_mae: 0.4395\n",
            "Epoch 289/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1698 - mae: 0.4645\n",
            "Epoch 289: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1696 - mae: 0.4634 - val_loss: 0.1545 - val_mae: 0.4408\n",
            "Epoch 290/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1690 - mae: 0.4632\n",
            "Epoch 290: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1690 - mae: 0.4629 - val_loss: 0.1539 - val_mae: 0.4392\n",
            "Epoch 291/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1692 - mae: 0.4643\n",
            "Epoch 291: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1692 - mae: 0.4643 - val_loss: 0.1528 - val_mae: 0.4386\n",
            "Epoch 292/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1694 - mae: 0.4648\n",
            "Epoch 292: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1697 - mae: 0.4646 - val_loss: 0.1539 - val_mae: 0.4392\n",
            "Epoch 293/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1689 - mae: 0.4626\n",
            "Epoch 293: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1689 - mae: 0.4626 - val_loss: 0.1526 - val_mae: 0.4381\n",
            "Epoch 294/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1700 - mae: 0.4648\n",
            "Epoch 294: loss did not improve from 0.16875\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1700 - mae: 0.4648 - val_loss: 0.1539 - val_mae: 0.4403\n",
            "Epoch 295/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1685 - mae: 0.4628\n",
            "Epoch 295: loss improved from 0.16875 to 0.16850, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1685 - mae: 0.4628 - val_loss: 0.1556 - val_mae: 0.4417\n",
            "Epoch 296/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1679 - mae: 0.4622\n",
            "Epoch 296: loss did not improve from 0.16850\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1698 - mae: 0.4647 - val_loss: 0.1557 - val_mae: 0.4417\n",
            "Epoch 297/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4670\n",
            "Epoch 297: loss did not improve from 0.16850\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1698 - mae: 0.4633 - val_loss: 0.1543 - val_mae: 0.4402\n",
            "Epoch 298/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1693 - mae: 0.4634\n",
            "Epoch 298: loss did not improve from 0.16850\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1694 - mae: 0.4641 - val_loss: 0.1544 - val_mae: 0.4405\n",
            "Epoch 299/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1689 - mae: 0.4630\n",
            "Epoch 299: loss did not improve from 0.16850\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1691 - mae: 0.4636 - val_loss: 0.1548 - val_mae: 0.4419\n",
            "Epoch 300/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1682 - mae: 0.4618\n",
            "Epoch 300: loss did not improve from 0.16850\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1697 - mae: 0.4646 - val_loss: 0.1545 - val_mae: 0.4403\n",
            "Epoch 301/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1687 - mae: 0.4627\n",
            "Epoch 301: loss did not improve from 0.16850\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1692 - mae: 0.4631 - val_loss: 0.1546 - val_mae: 0.4406\n",
            "Epoch 302/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1693 - mae: 0.4638\n",
            "Epoch 302: loss did not improve from 0.16850\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1691 - mae: 0.4634 - val_loss: 0.1541 - val_mae: 0.4400\n",
            "Epoch 303/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1700 - mae: 0.4650\n",
            "Epoch 303: loss did not improve from 0.16850\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1695 - mae: 0.4639 - val_loss: 0.1553 - val_mae: 0.4415\n",
            "Epoch 304/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1708 - mae: 0.4650\n",
            "Epoch 304: loss did not improve from 0.16850\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1693 - mae: 0.4632 - val_loss: 0.1532 - val_mae: 0.4396\n",
            "Epoch 305/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1689 - mae: 0.4641\n",
            "Epoch 305: loss improved from 0.16850 to 0.16845, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1685 - mae: 0.4636 - val_loss: 0.1550 - val_mae: 0.4406\n",
            "Epoch 306/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1691 - mae: 0.4625\n",
            "Epoch 306: loss did not improve from 0.16845\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1688 - mae: 0.4632 - val_loss: 0.1530 - val_mae: 0.4378\n",
            "Epoch 307/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1684 - mae: 0.4617\n",
            "Epoch 307: loss did not improve from 0.16845\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1694 - mae: 0.4632 - val_loss: 0.1550 - val_mae: 0.4414\n",
            "Epoch 308/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1683 - mae: 0.4618\n",
            "Epoch 308: loss did not improve from 0.16845\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1689 - mae: 0.4627 - val_loss: 0.1544 - val_mae: 0.4407\n",
            "Epoch 309/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1695 - mae: 0.4634\n",
            "Epoch 309: loss did not improve from 0.16845\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1688 - mae: 0.4623 - val_loss: 0.1540 - val_mae: 0.4404\n",
            "Epoch 310/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1685 - mae: 0.4625\n",
            "Epoch 310: loss improved from 0.16845 to 0.16845, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1684 - mae: 0.4626 - val_loss: 0.1553 - val_mae: 0.4416\n",
            "Epoch 311/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1685 - mae: 0.4627\n",
            "Epoch 311: loss did not improve from 0.16845\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1690 - mae: 0.4631 - val_loss: 0.1545 - val_mae: 0.4405\n",
            "Epoch 312/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1672 - mae: 0.4609\n",
            "Epoch 312: loss improved from 0.16845 to 0.16805, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1681 - mae: 0.4621 - val_loss: 0.1542 - val_mae: 0.4407\n",
            "Epoch 313/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1688 - mae: 0.4625\n",
            "Epoch 313: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1688 - mae: 0.4625 - val_loss: 0.1542 - val_mae: 0.4403\n",
            "Epoch 314/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1689 - mae: 0.4620\n",
            "Epoch 314: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1686 - mae: 0.4622 - val_loss: 0.1542 - val_mae: 0.4404\n",
            "Epoch 315/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4665\n",
            "Epoch 315: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1687 - mae: 0.4624 - val_loss: 0.1562 - val_mae: 0.4428\n",
            "Epoch 316/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1677 - mae: 0.4621\n",
            "Epoch 316: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1692 - mae: 0.4642 - val_loss: 0.1549 - val_mae: 0.4406\n",
            "Epoch 317/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1689 - mae: 0.4629\n",
            "Epoch 317: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1689 - mae: 0.4630 - val_loss: 0.1542 - val_mae: 0.4399\n",
            "Epoch 318/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1666 - mae: 0.4602\n",
            "Epoch 318: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1688 - mae: 0.4625 - val_loss: 0.1555 - val_mae: 0.4418\n",
            "Epoch 319/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1688 - mae: 0.4644\n",
            "Epoch 319: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1687 - mae: 0.4643 - val_loss: 0.1563 - val_mae: 0.4427\n",
            "Epoch 320/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1693 - mae: 0.4630\n",
            "Epoch 320: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1693 - mae: 0.4630 - val_loss: 0.1546 - val_mae: 0.4408\n",
            "Epoch 321/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1696 - mae: 0.4635\n",
            "Epoch 321: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.1685 - mae: 0.4622 - val_loss: 0.1546 - val_mae: 0.4405\n",
            "Epoch 322/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1691 - mae: 0.4633\n",
            "Epoch 322: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1691 - mae: 0.4633 - val_loss: 0.1545 - val_mae: 0.4410\n",
            "Epoch 323/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1680 - mae: 0.4623\n",
            "Epoch 323: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1688 - mae: 0.4636 - val_loss: 0.1551 - val_mae: 0.4410\n",
            "Epoch 324/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1662 - mae: 0.4593\n",
            "Epoch 324: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1687 - mae: 0.4628 - val_loss: 0.1527 - val_mae: 0.4381\n",
            "Epoch 325/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1670 - mae: 0.4612\n",
            "Epoch 325: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1691 - mae: 0.4634 - val_loss: 0.1547 - val_mae: 0.4398\n",
            "Epoch 326/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1684 - mae: 0.4622\n",
            "Epoch 326: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1685 - mae: 0.4621 - val_loss: 0.1542 - val_mae: 0.4405\n",
            "Epoch 327/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1689 - mae: 0.4628\n",
            "Epoch 327: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1681 - mae: 0.4618 - val_loss: 0.1555 - val_mae: 0.4414\n",
            "Epoch 328/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1688 - mae: 0.4625\n",
            "Epoch 328: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1688 - mae: 0.4625 - val_loss: 0.1547 - val_mae: 0.4408\n",
            "Epoch 329/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1681 - mae: 0.4613\n",
            "Epoch 329: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1688 - mae: 0.4620 - val_loss: 0.1545 - val_mae: 0.4402\n",
            "Epoch 330/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1684 - mae: 0.4624\n",
            "Epoch 330: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1681 - mae: 0.4626 - val_loss: 0.1534 - val_mae: 0.4380\n",
            "Epoch 331/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1697 - mae: 0.4650\n",
            "Epoch 331: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1686 - mae: 0.4629 - val_loss: 0.1549 - val_mae: 0.4410\n",
            "Epoch 332/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1673 - mae: 0.4591\n",
            "Epoch 332: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1683 - mae: 0.4613 - val_loss: 0.1542 - val_mae: 0.4409\n",
            "Epoch 333/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1697 - mae: 0.4646\n",
            "Epoch 333: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1690 - mae: 0.4635 - val_loss: 0.1539 - val_mae: 0.4394\n",
            "Epoch 334/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1689 - mae: 0.4630\n",
            "Epoch 334: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1689 - mae: 0.4630 - val_loss: 0.1540 - val_mae: 0.4396\n",
            "Epoch 335/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1692 - mae: 0.4630\n",
            "Epoch 335: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1684 - mae: 0.4621 - val_loss: 0.1554 - val_mae: 0.4416\n",
            "Epoch 336/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1691 - mae: 0.4629\n",
            "Epoch 336: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1684 - mae: 0.4623 - val_loss: 0.1552 - val_mae: 0.4427\n",
            "Epoch 337/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1685 - mae: 0.4625\n",
            "Epoch 337: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1685 - mae: 0.4625 - val_loss: 0.1544 - val_mae: 0.4403\n",
            "Epoch 338/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1690 - mae: 0.4632\n",
            "Epoch 338: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1684 - mae: 0.4635 - val_loss: 0.1554 - val_mae: 0.4415\n",
            "Epoch 339/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1689 - mae: 0.4621\n",
            "Epoch 339: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1687 - mae: 0.4624 - val_loss: 0.1555 - val_mae: 0.4417\n",
            "Epoch 340/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1687 - mae: 0.4633\n",
            "Epoch 340: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.1683 - mae: 0.4624 - val_loss: 0.1544 - val_mae: 0.4408\n",
            "Epoch 341/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1685 - mae: 0.4624\n",
            "Epoch 341: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1685 - mae: 0.4624 - val_loss: 0.1545 - val_mae: 0.4407\n",
            "Epoch 342/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1682 - mae: 0.4619\n",
            "Epoch 342: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1683 - mae: 0.4624 - val_loss: 0.1558 - val_mae: 0.4426\n",
            "Epoch 343/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1688 - mae: 0.4638\n",
            "Epoch 343: loss did not improve from 0.16805\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1688 - mae: 0.4638 - val_loss: 0.1535 - val_mae: 0.4398\n",
            "Epoch 344/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1691 - mae: 0.4631\n",
            "Epoch 344: loss improved from 0.16805 to 0.16801, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1680 - mae: 0.4616 - val_loss: 0.1552 - val_mae: 0.4414\n",
            "Epoch 345/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1690 - mae: 0.4639\n",
            "Epoch 345: loss did not improve from 0.16801\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1684 - mae: 0.4627 - val_loss: 0.1549 - val_mae: 0.4408\n",
            "Epoch 346/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1689 - mae: 0.4630\n",
            "Epoch 346: loss did not improve from 0.16801\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1686 - mae: 0.4625 - val_loss: 0.1541 - val_mae: 0.4403\n",
            "Epoch 347/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1684 - mae: 0.4623\n",
            "Epoch 347: loss did not improve from 0.16801\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1684 - mae: 0.4623 - val_loss: 0.1546 - val_mae: 0.4410\n",
            "Epoch 348/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1679 - mae: 0.4611\n",
            "Epoch 348: loss improved from 0.16801 to 0.16747, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1675 - mae: 0.4605 - val_loss: 0.1556 - val_mae: 0.4432\n",
            "Epoch 349/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1677 - mae: 0.4614\n",
            "Epoch 349: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1683 - mae: 0.4627 - val_loss: 0.1544 - val_mae: 0.4400\n",
            "Epoch 350/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1684 - mae: 0.4616\n",
            "Epoch 350: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1684 - mae: 0.4616 - val_loss: 0.1551 - val_mae: 0.4420\n",
            "Epoch 351/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1691 - mae: 0.4643\n",
            "Epoch 351: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1685 - mae: 0.4630 - val_loss: 0.1548 - val_mae: 0.4409\n",
            "Epoch 352/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1685 - mae: 0.4617\n",
            "Epoch 352: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1680 - mae: 0.4608 - val_loss: 0.1543 - val_mae: 0.4404\n",
            "Epoch 353/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1688 - mae: 0.4627\n",
            "Epoch 353: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1688 - mae: 0.4627 - val_loss: 0.1566 - val_mae: 0.4437\n",
            "Epoch 354/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1684 - mae: 0.4624\n",
            "Epoch 354: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1685 - mae: 0.4629 - val_loss: 0.1549 - val_mae: 0.4410\n",
            "Epoch 355/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1692 - mae: 0.4632\n",
            "Epoch 355: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1686 - mae: 0.4618 - val_loss: 0.1542 - val_mae: 0.4407\n",
            "Epoch 356/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1671 - mae: 0.4617\n",
            "Epoch 356: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1679 - mae: 0.4621 - val_loss: 0.1552 - val_mae: 0.4413\n",
            "Epoch 357/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1691 - mae: 0.4630\n",
            "Epoch 357: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1681 - mae: 0.4618 - val_loss: 0.1545 - val_mae: 0.4406\n",
            "Epoch 358/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1678 - mae: 0.4616\n",
            "Epoch 358: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1683 - mae: 0.4624 - val_loss: 0.1564 - val_mae: 0.4429\n",
            "Epoch 359/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1682 - mae: 0.4614\n",
            "Epoch 359: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1682 - mae: 0.4614 - val_loss: 0.1539 - val_mae: 0.4405\n",
            "Epoch 360/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1685 - mae: 0.4622\n",
            "Epoch 360: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1685 - mae: 0.4622 - val_loss: 0.1538 - val_mae: 0.4391\n",
            "Epoch 361/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1678 - mae: 0.4608\n",
            "Epoch 361: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1678 - mae: 0.4608 - val_loss: 0.1543 - val_mae: 0.4404\n",
            "Epoch 362/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1687 - mae: 0.4625\n",
            "Epoch 362: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1682 - mae: 0.4620 - val_loss: 0.1562 - val_mae: 0.4433\n",
            "Epoch 363/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1688 - mae: 0.4630\n",
            "Epoch 363: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1680 - mae: 0.4616 - val_loss: 0.1553 - val_mae: 0.4414\n",
            "Epoch 364/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1680 - mae: 0.4614\n",
            "Epoch 364: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1686 - mae: 0.4626 - val_loss: 0.1553 - val_mae: 0.4417\n",
            "Epoch 365/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1682 - mae: 0.4622\n",
            "Epoch 365: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1679 - mae: 0.4619 - val_loss: 0.1560 - val_mae: 0.4432\n",
            "Epoch 366/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1685 - mae: 0.4623\n",
            "Epoch 366: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1679 - mae: 0.4619 - val_loss: 0.1562 - val_mae: 0.4422\n",
            "Epoch 367/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1688 - mae: 0.4631\n",
            "Epoch 367: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1680 - mae: 0.4618 - val_loss: 0.1543 - val_mae: 0.4398\n",
            "Epoch 368/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1686 - mae: 0.4620\n",
            "Epoch 368: loss did not improve from 0.16747\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1681 - mae: 0.4614 - val_loss: 0.1552 - val_mae: 0.4418\n",
            "Epoch 369/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1688 - mae: 0.4633\n",
            "Epoch 369: loss improved from 0.16747 to 0.16714, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1671 - mae: 0.4605 - val_loss: 0.1547 - val_mae: 0.4402\n",
            "Epoch 370/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1673 - mae: 0.4592\n",
            "Epoch 370: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1676 - mae: 0.4604 - val_loss: 0.1565 - val_mae: 0.4438\n",
            "Epoch 371/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1684 - mae: 0.4631\n",
            "Epoch 371: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1673 - mae: 0.4611 - val_loss: 0.1559 - val_mae: 0.4432\n",
            "Epoch 372/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1673 - mae: 0.4603\n",
            "Epoch 372: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1683 - mae: 0.4617 - val_loss: 0.1559 - val_mae: 0.4421\n",
            "Epoch 373/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1678 - mae: 0.4607\n",
            "Epoch 373: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1678 - mae: 0.4610 - val_loss: 0.1546 - val_mae: 0.4407\n",
            "Epoch 374/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1672 - mae: 0.4607\n",
            "Epoch 374: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1676 - mae: 0.4617 - val_loss: 0.1548 - val_mae: 0.4409\n",
            "Epoch 375/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1679 - mae: 0.4619\n",
            "Epoch 375: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1681 - mae: 0.4621 - val_loss: 0.1541 - val_mae: 0.4401\n",
            "Epoch 376/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1676 - mae: 0.4603\n",
            "Epoch 376: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1679 - mae: 0.4607 - val_loss: 0.1543 - val_mae: 0.4404\n",
            "Epoch 377/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1683 - mae: 0.4626\n",
            "Epoch 377: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1683 - mae: 0.4626 - val_loss: 0.1549 - val_mae: 0.4412\n",
            "Epoch 378/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1671 - mae: 0.4601\n",
            "Epoch 378: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1681 - mae: 0.4619 - val_loss: 0.1556 - val_mae: 0.4424\n",
            "Epoch 379/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1668 - mae: 0.4601\n",
            "Epoch 379: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1678 - mae: 0.4616 - val_loss: 0.1560 - val_mae: 0.4424\n",
            "Epoch 380/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1669 - mae: 0.4606\n",
            "Epoch 380: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1672 - mae: 0.4607 - val_loss: 0.1554 - val_mae: 0.4419\n",
            "Epoch 381/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1673 - mae: 0.4626\n",
            "Epoch 381: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1676 - mae: 0.4621 - val_loss: 0.1564 - val_mae: 0.4438\n",
            "Epoch 382/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1677 - mae: 0.4613\n",
            "Epoch 382: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1677 - mae: 0.4613 - val_loss: 0.1551 - val_mae: 0.4413\n",
            "Epoch 383/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1677 - mae: 0.4604\n",
            "Epoch 383: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1678 - mae: 0.4606 - val_loss: 0.1556 - val_mae: 0.4418\n",
            "Epoch 384/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1686 - mae: 0.4613\n",
            "Epoch 384: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1679 - mae: 0.4608 - val_loss: 0.1562 - val_mae: 0.4435\n",
            "Epoch 385/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1681 - mae: 0.4621\n",
            "Epoch 385: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1680 - mae: 0.4618 - val_loss: 0.1546 - val_mae: 0.4405\n",
            "Epoch 386/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1669 - mae: 0.4610\n",
            "Epoch 386: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1679 - mae: 0.4615 - val_loss: 0.1559 - val_mae: 0.4432\n",
            "Epoch 387/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1680 - mae: 0.4616\n",
            "Epoch 387: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1680 - mae: 0.4616 - val_loss: 0.1550 - val_mae: 0.4408\n",
            "Epoch 388/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1687 - mae: 0.4638\n",
            "Epoch 388: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1675 - mae: 0.4617 - val_loss: 0.1560 - val_mae: 0.4430\n",
            "Epoch 389/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1677 - mae: 0.4616\n",
            "Epoch 389: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1678 - mae: 0.4614 - val_loss: 0.1540 - val_mae: 0.4409\n",
            "Epoch 390/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1685 - mae: 0.4624\n",
            "Epoch 390: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1678 - mae: 0.4613 - val_loss: 0.1554 - val_mae: 0.4422\n",
            "Epoch 391/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1675 - mae: 0.4613\n",
            "Epoch 391: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1681 - mae: 0.4621 - val_loss: 0.1559 - val_mae: 0.4435\n",
            "Epoch 392/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1668 - mae: 0.4605\n",
            "Epoch 392: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1679 - mae: 0.4619 - val_loss: 0.1562 - val_mae: 0.4438\n",
            "Epoch 393/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1674 - mae: 0.4608\n",
            "Epoch 393: loss did not improve from 0.16714\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1674 - mae: 0.4608 - val_loss: 0.1561 - val_mae: 0.4428\n",
            "Epoch 394/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1673 - mae: 0.4603\n",
            "Epoch 394: loss improved from 0.16714 to 0.16695, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.1670 - mae: 0.4600 - val_loss: 0.1551 - val_mae: 0.4412\n",
            "Epoch 395/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1688 - mae: 0.4623\n",
            "Epoch 395: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1675 - mae: 0.4605 - val_loss: 0.1540 - val_mae: 0.4401\n",
            "Epoch 396/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1688 - mae: 0.4623\n",
            "Epoch 396: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1683 - mae: 0.4618 - val_loss: 0.1549 - val_mae: 0.4411\n",
            "Epoch 397/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1687 - mae: 0.4640\n",
            "Epoch 397: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1677 - mae: 0.4613 - val_loss: 0.1552 - val_mae: 0.4414\n",
            "Epoch 398/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1676 - mae: 0.4610\n",
            "Epoch 398: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1676 - mae: 0.4610 - val_loss: 0.1549 - val_mae: 0.4414\n",
            "Epoch 399/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1680 - mae: 0.4616\n",
            "Epoch 399: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1675 - mae: 0.4611 - val_loss: 0.1549 - val_mae: 0.4411\n",
            "Epoch 400/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1680 - mae: 0.4614\n",
            "Epoch 400: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1672 - mae: 0.4605 - val_loss: 0.1544 - val_mae: 0.4405\n",
            "Epoch 401/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1679 - mae: 0.4612\n",
            "Epoch 401: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.1679 - mae: 0.4612 - val_loss: 0.1546 - val_mae: 0.4408\n",
            "Epoch 402/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1673 - mae: 0.4603\n",
            "Epoch 402: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1673 - mae: 0.4603 - val_loss: 0.1556 - val_mae: 0.4433\n",
            "Epoch 403/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1668 - mae: 0.4603\n",
            "Epoch 403: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1676 - mae: 0.4617 - val_loss: 0.1540 - val_mae: 0.4391\n",
            "Epoch 404/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1675 - mae: 0.4613\n",
            "Epoch 404: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1675 - mae: 0.4613 - val_loss: 0.1542 - val_mae: 0.4404\n",
            "Epoch 405/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1671 - mae: 0.4605\n",
            "Epoch 405: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1676 - mae: 0.4612 - val_loss: 0.1554 - val_mae: 0.4426\n",
            "Epoch 406/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1688 - mae: 0.4624\n",
            "Epoch 406: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1675 - mae: 0.4612 - val_loss: 0.1549 - val_mae: 0.4415\n",
            "Epoch 407/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1673 - mae: 0.4605\n",
            "Epoch 407: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1673 - mae: 0.4605 - val_loss: 0.1545 - val_mae: 0.4404\n",
            "Epoch 408/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1676 - mae: 0.4606\n",
            "Epoch 408: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1679 - mae: 0.4609 - val_loss: 0.1554 - val_mae: 0.4427\n",
            "Epoch 409/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1671 - mae: 0.4604\n",
            "Epoch 409: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1670 - mae: 0.4603 - val_loss: 0.1560 - val_mae: 0.4427\n",
            "Epoch 410/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1676 - mae: 0.4609\n",
            "Epoch 410: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1676 - mae: 0.4609 - val_loss: 0.1546 - val_mae: 0.4409\n",
            "Epoch 411/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1679 - mae: 0.4611\n",
            "Epoch 411: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1681 - mae: 0.4622 - val_loss: 0.1543 - val_mae: 0.4404\n",
            "Epoch 412/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1678 - mae: 0.4618\n",
            "Epoch 412: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1678 - mae: 0.4618 - val_loss: 0.1554 - val_mae: 0.4419\n",
            "Epoch 413/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1655 - mae: 0.4571\n",
            "Epoch 413: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1677 - mae: 0.4600 - val_loss: 0.1558 - val_mae: 0.4421\n",
            "Epoch 414/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1670 - mae: 0.4619\n",
            "Epoch 414: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1677 - mae: 0.4618 - val_loss: 0.1562 - val_mae: 0.4430\n",
            "Epoch 415/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1692 - mae: 0.4632\n",
            "Epoch 415: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1673 - mae: 0.4600 - val_loss: 0.1568 - val_mae: 0.4448\n",
            "Epoch 416/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1664 - mae: 0.4594\n",
            "Epoch 416: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1672 - mae: 0.4612 - val_loss: 0.1559 - val_mae: 0.4424\n",
            "Epoch 417/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1689 - mae: 0.4620\n",
            "Epoch 417: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1673 - mae: 0.4603 - val_loss: 0.1542 - val_mae: 0.4407\n",
            "Epoch 418/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1680 - mae: 0.4622\n",
            "Epoch 418: loss did not improve from 0.16695\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1677 - mae: 0.4616 - val_loss: 0.1552 - val_mae: 0.4417\n",
            "Epoch 419/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1671 - mae: 0.4599\n",
            "Epoch 419: loss improved from 0.16695 to 0.16654, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1665 - mae: 0.4594 - val_loss: 0.1555 - val_mae: 0.4428\n",
            "Epoch 420/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1669 - mae: 0.4594\n",
            "Epoch 420: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1675 - mae: 0.4601 - val_loss: 0.1548 - val_mae: 0.4406\n",
            "Epoch 421/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1676 - mae: 0.4615\n",
            "Epoch 421: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1676 - mae: 0.4615 - val_loss: 0.1544 - val_mae: 0.4409\n",
            "Epoch 422/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1665 - mae: 0.4603\n",
            "Epoch 422: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1675 - mae: 0.4608 - val_loss: 0.1535 - val_mae: 0.4395\n",
            "Epoch 423/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1666 - mae: 0.4600\n",
            "Epoch 423: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1674 - mae: 0.4612 - val_loss: 0.1541 - val_mae: 0.4397\n",
            "Epoch 424/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1671 - mae: 0.4592\n",
            "Epoch 424: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1678 - mae: 0.4608 - val_loss: 0.1552 - val_mae: 0.4421\n",
            "Epoch 425/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1675 - mae: 0.4608\n",
            "Epoch 425: loss did not improve from 0.16654\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1675 - mae: 0.4609 - val_loss: 0.1551 - val_mae: 0.4411\n",
            "Epoch 426/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1669 - mae: 0.4622\n",
            "Epoch 426: loss improved from 0.16654 to 0.16637, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1664 - mae: 0.4606 - val_loss: 0.1556 - val_mae: 0.4426\n",
            "Epoch 427/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1690 - mae: 0.4625\n",
            "Epoch 427: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1666 - mae: 0.4596 - val_loss: 0.1543 - val_mae: 0.4408\n",
            "Epoch 428/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1672 - mae: 0.4605\n",
            "Epoch 428: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1672 - mae: 0.4605 - val_loss: 0.1560 - val_mae: 0.4429\n",
            "Epoch 429/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1679 - mae: 0.4614\n",
            "Epoch 429: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1679 - mae: 0.4614 - val_loss: 0.1554 - val_mae: 0.4417\n",
            "Epoch 430/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1681 - mae: 0.4615\n",
            "Epoch 430: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1671 - mae: 0.4602 - val_loss: 0.1553 - val_mae: 0.4423\n",
            "Epoch 431/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1657 - mae: 0.4588\n",
            "Epoch 431: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1671 - mae: 0.4608 - val_loss: 0.1540 - val_mae: 0.4396\n",
            "Epoch 432/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1652 - mae: 0.4579\n",
            "Epoch 432: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1676 - mae: 0.4604 - val_loss: 0.1552 - val_mae: 0.4420\n",
            "Epoch 433/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1679 - mae: 0.4623\n",
            "Epoch 433: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1676 - mae: 0.4621 - val_loss: 0.1540 - val_mae: 0.4396\n",
            "Epoch 434/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1664 - mae: 0.4599\n",
            "Epoch 434: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1673 - mae: 0.4612 - val_loss: 0.1535 - val_mae: 0.4398\n",
            "Epoch 435/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1676 - mae: 0.4608\n",
            "Epoch 435: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1675 - mae: 0.4602 - val_loss: 0.1547 - val_mae: 0.4409\n",
            "Epoch 436/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1682 - mae: 0.4620\n",
            "Epoch 436: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1677 - mae: 0.4613 - val_loss: 0.1556 - val_mae: 0.4419\n",
            "Epoch 437/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1678 - mae: 0.4612\n",
            "Epoch 437: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1675 - mae: 0.4608 - val_loss: 0.1560 - val_mae: 0.4430\n",
            "Epoch 438/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1669 - mae: 0.4593\n",
            "Epoch 438: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1674 - mae: 0.4605 - val_loss: 0.1545 - val_mae: 0.4412\n",
            "Epoch 439/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1667 - mae: 0.4592\n",
            "Epoch 439: loss did not improve from 0.16637\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1675 - mae: 0.4606 - val_loss: 0.1541 - val_mae: 0.4407\n",
            "Epoch 440/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1660 - mae: 0.4591\n",
            "Epoch 440: loss improved from 0.16637 to 0.16621, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1662 - mae: 0.4596 - val_loss: 0.1552 - val_mae: 0.4410\n",
            "Epoch 441/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1673 - mae: 0.4614\n",
            "Epoch 441: loss did not improve from 0.16621\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1673 - mae: 0.4614 - val_loss: 0.1547 - val_mae: 0.4422\n",
            "Epoch 442/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1675 - mae: 0.4606\n",
            "Epoch 442: loss did not improve from 0.16621\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1675 - mae: 0.4606 - val_loss: 0.1551 - val_mae: 0.4414\n",
            "Epoch 443/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1673 - mae: 0.4603\n",
            "Epoch 443: loss did not improve from 0.16621\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1673 - mae: 0.4603 - val_loss: 0.1557 - val_mae: 0.4428\n",
            "Epoch 444/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1675 - mae: 0.4605\n",
            "Epoch 444: loss did not improve from 0.16621\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1670 - mae: 0.4596 - val_loss: 0.1562 - val_mae: 0.4442\n",
            "Epoch 445/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1664 - mae: 0.4599\n",
            "Epoch 445: loss improved from 0.16621 to 0.16596, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1660 - mae: 0.4592 - val_loss: 0.1548 - val_mae: 0.4419\n",
            "Epoch 446/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1675 - mae: 0.4611\n",
            "Epoch 446: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1671 - mae: 0.4610 - val_loss: 0.1557 - val_mae: 0.4423\n",
            "Epoch 447/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1670 - mae: 0.4602\n",
            "Epoch 447: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1673 - mae: 0.4612 - val_loss: 0.1544 - val_mae: 0.4404\n",
            "Epoch 448/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1658 - mae: 0.4582\n",
            "Epoch 448: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1668 - mae: 0.4594 - val_loss: 0.1553 - val_mae: 0.4426\n",
            "Epoch 449/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1674 - mae: 0.4616\n",
            "Epoch 449: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1671 - mae: 0.4614 - val_loss: 0.1564 - val_mae: 0.4436\n",
            "Epoch 450/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1678 - mae: 0.4601\n",
            "Epoch 450: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.1670 - mae: 0.4594 - val_loss: 0.1539 - val_mae: 0.4407\n",
            "Epoch 451/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1666 - mae: 0.4595\n",
            "Epoch 451: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1668 - mae: 0.4597 - val_loss: 0.1553 - val_mae: 0.4419\n",
            "Epoch 452/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1673 - mae: 0.4611\n",
            "Epoch 452: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1673 - mae: 0.4611 - val_loss: 0.1545 - val_mae: 0.4411\n",
            "Epoch 453/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1675 - mae: 0.4597\n",
            "Epoch 453: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1674 - mae: 0.4606 - val_loss: 0.1548 - val_mae: 0.4416\n",
            "Epoch 454/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1678 - mae: 0.4611\n",
            "Epoch 454: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1668 - mae: 0.4597 - val_loss: 0.1557 - val_mae: 0.4425\n",
            "Epoch 455/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1685 - mae: 0.4635\n",
            "Epoch 455: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1670 - mae: 0.4605 - val_loss: 0.1551 - val_mae: 0.4419\n",
            "Epoch 456/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1673 - mae: 0.4604\n",
            "Epoch 456: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1673 - mae: 0.4604 - val_loss: 0.1542 - val_mae: 0.4402\n",
            "Epoch 457/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1637 - mae: 0.4553\n",
            "Epoch 457: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1661 - mae: 0.4582 - val_loss: 0.1569 - val_mae: 0.4446\n",
            "Epoch 458/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1665 - mae: 0.4593\n",
            "Epoch 458: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1673 - mae: 0.4606 - val_loss: 0.1576 - val_mae: 0.4459\n",
            "Epoch 459/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1651 - mae: 0.4592\n",
            "Epoch 459: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1667 - mae: 0.4612 - val_loss: 0.1534 - val_mae: 0.4393\n",
            "Epoch 460/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1675 - mae: 0.4609\n",
            "Epoch 460: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1671 - mae: 0.4601 - val_loss: 0.1553 - val_mae: 0.4417\n",
            "Epoch 461/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1655 - mae: 0.4587\n",
            "Epoch 461: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1672 - mae: 0.4611 - val_loss: 0.1535 - val_mae: 0.4395\n",
            "Epoch 462/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1670 - mae: 0.4598\n",
            "Epoch 462: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1670 - mae: 0.4598 - val_loss: 0.1552 - val_mae: 0.4423\n",
            "Epoch 463/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1687 - mae: 0.4623\n",
            "Epoch 463: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1672 - mae: 0.4604 - val_loss: 0.1547 - val_mae: 0.4415\n",
            "Epoch 464/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1661 - mae: 0.4584\n",
            "Epoch 464: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1669 - mae: 0.4600 - val_loss: 0.1555 - val_mae: 0.4424\n",
            "Epoch 465/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1664 - mae: 0.4594\n",
            "Epoch 465: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1664 - mae: 0.4594 - val_loss: 0.1554 - val_mae: 0.4431\n",
            "Epoch 466/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1650 - mae: 0.4582\n",
            "Epoch 466: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1670 - mae: 0.4611 - val_loss: 0.1564 - val_mae: 0.4441\n",
            "Epoch 467/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1680 - mae: 0.4619\n",
            "Epoch 467: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1665 - mae: 0.4593 - val_loss: 0.1560 - val_mae: 0.4429\n",
            "Epoch 468/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1678 - mae: 0.4622\n",
            "Epoch 468: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1668 - mae: 0.4598 - val_loss: 0.1546 - val_mae: 0.4405\n",
            "Epoch 469/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1684 - mae: 0.4607\n",
            "Epoch 469: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1669 - mae: 0.4596 - val_loss: 0.1547 - val_mae: 0.4412\n",
            "Epoch 470/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1667 - mae: 0.4603\n",
            "Epoch 470: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1669 - mae: 0.4605 - val_loss: 0.1547 - val_mae: 0.4408\n",
            "Epoch 471/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1669 - mae: 0.4599\n",
            "Epoch 471: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1669 - mae: 0.4599 - val_loss: 0.1548 - val_mae: 0.4408\n",
            "Epoch 472/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1660 - mae: 0.4589\n",
            "Epoch 472: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1670 - mae: 0.4599 - val_loss: 0.1546 - val_mae: 0.4414\n",
            "Epoch 473/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1685 - mae: 0.4623\n",
            "Epoch 473: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1671 - mae: 0.4605 - val_loss: 0.1552 - val_mae: 0.4417\n",
            "Epoch 474/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1673 - mae: 0.4605\n",
            "Epoch 474: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1668 - mae: 0.4599 - val_loss: 0.1571 - val_mae: 0.4445\n",
            "Epoch 475/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1677 - mae: 0.4609\n",
            "Epoch 475: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.1672 - mae: 0.4603 - val_loss: 0.1547 - val_mae: 0.4415\n",
            "Epoch 476/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1664 - mae: 0.4593\n",
            "Epoch 476: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1666 - mae: 0.4597 - val_loss: 0.1558 - val_mae: 0.4432\n",
            "Epoch 477/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1665 - mae: 0.4593\n",
            "Epoch 477: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1665 - mae: 0.4593 - val_loss: 0.1561 - val_mae: 0.4438\n",
            "Epoch 478/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1659 - mae: 0.4586\n",
            "Epoch 478: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1673 - mae: 0.4605 - val_loss: 0.1558 - val_mae: 0.4428\n",
            "Epoch 479/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1662 - mae: 0.4589\n",
            "Epoch 479: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1669 - mae: 0.4601 - val_loss: 0.1547 - val_mae: 0.4413\n",
            "Epoch 480/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1667 - mae: 0.4591\n",
            "Epoch 480: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1668 - mae: 0.4595 - val_loss: 0.1547 - val_mae: 0.4412\n",
            "Epoch 481/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1651 - mae: 0.4574\n",
            "Epoch 481: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1666 - mae: 0.4599 - val_loss: 0.1550 - val_mae: 0.4412\n",
            "Epoch 482/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1678 - mae: 0.4612\n",
            "Epoch 482: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1669 - mae: 0.4597 - val_loss: 0.1533 - val_mae: 0.4388\n",
            "Epoch 483/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1652 - mae: 0.4575\n",
            "Epoch 483: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1661 - mae: 0.4587 - val_loss: 0.1556 - val_mae: 0.4426\n",
            "Epoch 484/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1656 - mae: 0.4584\n",
            "Epoch 484: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1666 - mae: 0.4601 - val_loss: 0.1544 - val_mae: 0.4401\n",
            "Epoch 485/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1675 - mae: 0.4610\n",
            "Epoch 485: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1670 - mae: 0.4604 - val_loss: 0.1548 - val_mae: 0.4420\n",
            "Epoch 486/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1661 - mae: 0.4599\n",
            "Epoch 486: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1667 - mae: 0.4600 - val_loss: 0.1552 - val_mae: 0.4418\n",
            "Epoch 487/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1667 - mae: 0.4609\n",
            "Epoch 487: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1671 - mae: 0.4606 - val_loss: 0.1554 - val_mae: 0.4415\n",
            "Epoch 488/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1665 - mae: 0.4593\n",
            "Epoch 488: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1667 - mae: 0.4596 - val_loss: 0.1557 - val_mae: 0.4434\n",
            "Epoch 489/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1668 - mae: 0.4601\n",
            "Epoch 489: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1667 - mae: 0.4598 - val_loss: 0.1555 - val_mae: 0.4427\n",
            "Epoch 490/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1658 - mae: 0.4583\n",
            "Epoch 490: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1670 - mae: 0.4596 - val_loss: 0.1550 - val_mae: 0.4410\n",
            "Epoch 491/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1694 - mae: 0.4643\n",
            "Epoch 491: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1677 - mae: 0.4620 - val_loss: 0.1556 - val_mae: 0.4435\n",
            "Epoch 492/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1676 - mae: 0.4611\n",
            "Epoch 492: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1674 - mae: 0.4611 - val_loss: 0.1554 - val_mae: 0.4424\n",
            "Epoch 493/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1673 - mae: 0.4594\n",
            "Epoch 493: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1672 - mae: 0.4594 - val_loss: 0.1548 - val_mae: 0.4421\n",
            "Epoch 494/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1668 - mae: 0.4605\n",
            "Epoch 494: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1668 - mae: 0.4605 - val_loss: 0.1558 - val_mae: 0.4424\n",
            "Epoch 495/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1662 - mae: 0.4594\n",
            "Epoch 495: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1660 - mae: 0.4590 - val_loss: 0.1546 - val_mae: 0.4415\n",
            "Epoch 496/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1674 - mae: 0.4606\n",
            "Epoch 496: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.1671 - mae: 0.4606 - val_loss: 0.1545 - val_mae: 0.4417\n",
            "Epoch 497/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1657 - mae: 0.4591\n",
            "Epoch 497: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1660 - mae: 0.4593 - val_loss: 0.1555 - val_mae: 0.4424\n",
            "Epoch 498/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1669 - mae: 0.4604\n",
            "Epoch 498: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1669 - mae: 0.4604 - val_loss: 0.1544 - val_mae: 0.4412\n",
            "Epoch 499/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1667 - mae: 0.4598\n",
            "Epoch 499: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1667 - mae: 0.4604 - val_loss: 0.1560 - val_mae: 0.4435\n",
            "Epoch 500/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1673 - mae: 0.4616\n",
            "Epoch 500: loss did not improve from 0.16596\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1664 - mae: 0.4603 - val_loss: 0.1550 - val_mae: 0.4419\n",
            "Step time : 0.023\n",
            "Total time : 795.436\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_entrainement.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YurVEewK76YB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "4965b69d-d594-494f-fc15-47f6af128fc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction de la période\")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nous avons pu resoudre le problème de l'overfiting en diminuant les erreurs sur les données de validations**"
      ],
      "metadata": {
        "id": "C63RDjOslf_U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE0ARCgco5HJ"
      },
      "source": [
        "# 5. Prédictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouStOYe4F_jF"
      },
      "source": [
        "Puisque le format d'entrée permet de prendre des séquences infinies, nous pouvons entrer des séquences de n'importe quelle taille :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amfVL_Tcbaak"
      },
      "outputs": [],
      "source": [
        "taille_fenetre = 20\n",
        "Serie_Normalisee = np.array(Serie_Normalisee)\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ3ibdZmlAyF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "d3e2849a-8d9b-4fff-9d7b-eae65f4f944a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTM simple couche')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTM simple couche (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ogIRkqaI54d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041bce97-d147-4d15-daf4-92e6f519202a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8076931561974006\n",
            "5.356233981002425\n"
          ]
        }
      ],
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TTGWrMoUuyij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb-rPANs8VhJ"
      },
      "source": [
        "# 6. Ajout de la régularisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pychE-9H8VPy"
      },
      "outputs": [],
      "source": [
        "# Remise à zéro des états du modèle\n",
        "tf.keras.backend.clear_session()\n",
        "model.load_weights(\"model_initial.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hezoZRXRTca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f6b3f2-8d06-449b-e4e9-df0871d5cf7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 20)]         0           []                               \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 20, 1)        0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 40)           6720        ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 40)           1640        ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 80)           0           ['dense[0][0]',                  \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            81          ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,441\n",
            "Trainable params: 8,441\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre))\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees)\n",
        "s_encodeur = tf.keras.layers.LSTM(40,kernel_regularizer=tf.keras.regularizers.l2(1e-5))(e_adapt)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(40,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8jU_qkC8t_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e5d19a-b59a-4823-99a2-2681a361906d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     66/Unknown - 3s 11ms/step - loss: 0.4368 - mae: 0.8172\n",
            "Epoch 1: loss improved from inf to 0.43533, saving model to poids.hdf5\n",
            "68/68 [==============================] - 3s 15ms/step - loss: 0.4353 - mae: 0.8153 - lr: 1.0000e-08\n",
            "Epoch 2/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4379 - mae: 0.8189\n",
            "Epoch 2: loss improved from 0.43533 to 0.43417, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4342 - mae: 0.8142 - lr: 1.2589e-08\n",
            "Epoch 3/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4358 - mae: 0.8160\n",
            "Epoch 3: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4358 - mae: 0.8160 - lr: 1.5849e-08\n",
            "Epoch 4/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.4425 - mae: 0.8246\n",
            "Epoch 4: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4357 - mae: 0.8159 - lr: 1.9953e-08\n",
            "Epoch 5/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.4368 - mae: 0.8167\n",
            "Epoch 5: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4353 - mae: 0.8153 - lr: 2.5119e-08\n",
            "Epoch 6/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.4350 - mae: 0.8151\n",
            "Epoch 6: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4350 - mae: 0.8151 - lr: 3.1623e-08\n",
            "Epoch 7/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4369 - mae: 0.8177\n",
            "Epoch 7: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4361 - mae: 0.8165 - lr: 3.9811e-08\n",
            "Epoch 8/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4348 - mae: 0.8161\n",
            "Epoch 8: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4343 - mae: 0.8144 - lr: 5.0119e-08\n",
            "Epoch 9/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4361 - mae: 0.8158\n",
            "Epoch 9: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4356 - mae: 0.8156 - lr: 6.3096e-08\n",
            "Epoch 10/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4336 - mae: 0.8127\n",
            "Epoch 10: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4353 - mae: 0.8153 - lr: 7.9433e-08\n",
            "Epoch 11/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.4343 - mae: 0.8147\n",
            "Epoch 11: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4352 - mae: 0.8154 - lr: 1.0000e-07\n",
            "Epoch 12/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4290 - mae: 0.8086\n",
            "Epoch 12: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4350 - mae: 0.8151 - lr: 1.2589e-07\n",
            "Epoch 13/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.4328 - mae: 0.8136\n",
            "Epoch 13: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4353 - mae: 0.8152 - lr: 1.5849e-07\n",
            "Epoch 14/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4371 - mae: 0.8184\n",
            "Epoch 14: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4357 - mae: 0.8159 - lr: 1.9953e-07\n",
            "Epoch 15/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4354 - mae: 0.8149\n",
            "Epoch 15: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4347 - mae: 0.8145 - lr: 2.5119e-07\n",
            "Epoch 16/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.4333 - mae: 0.8131\n",
            "Epoch 16: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4345 - mae: 0.8146 - lr: 3.1623e-07\n",
            "Epoch 17/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4372 - mae: 0.8174\n",
            "Epoch 17: loss did not improve from 0.43417\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4344 - mae: 0.8142 - lr: 3.9811e-07\n",
            "Epoch 18/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.4355 - mae: 0.8161\n",
            "Epoch 18: loss improved from 0.43417 to 0.43414, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4341 - mae: 0.8142 - lr: 5.0119e-07\n",
            "Epoch 19/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4375 - mae: 0.8187\n",
            "Epoch 19: loss improved from 0.43414 to 0.43365, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4336 - mae: 0.8133 - lr: 6.3096e-07\n",
            "Epoch 20/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.4342 - mae: 0.8140\n",
            "Epoch 20: loss improved from 0.43365 to 0.43226, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4323 - mae: 0.8120 - lr: 7.9433e-07\n",
            "Epoch 21/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.4317 - mae: 0.8112\n",
            "Epoch 21: loss did not improve from 0.43226\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.4330 - mae: 0.8126 - lr: 1.0000e-06\n",
            "Epoch 22/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4333 - mae: 0.8134\n",
            "Epoch 22: loss improved from 0.43226 to 0.43114, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4311 - mae: 0.8102 - lr: 1.2589e-06\n",
            "Epoch 23/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.4337 - mae: 0.8127\n",
            "Epoch 23: loss improved from 0.43114 to 0.43013, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4301 - mae: 0.8091 - lr: 1.5849e-06\n",
            "Epoch 24/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.4320 - mae: 0.8119\n",
            "Epoch 24: loss improved from 0.43013 to 0.42907, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4291 - mae: 0.8076 - lr: 1.9953e-06\n",
            "Epoch 25/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4311 - mae: 0.8104\n",
            "Epoch 25: loss improved from 0.42907 to 0.42747, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4275 - mae: 0.8058 - lr: 2.5119e-06\n",
            "Epoch 26/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.4256 - mae: 0.8039\n",
            "Epoch 26: loss improved from 0.42747 to 0.42516, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4252 - mae: 0.8030 - lr: 3.1623e-06\n",
            "Epoch 27/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.4150 - mae: 0.7908\n",
            "Epoch 27: loss improved from 0.42516 to 0.42230, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4223 - mae: 0.7994 - lr: 3.9811e-06\n",
            "Epoch 28/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4220 - mae: 0.7987\n",
            "Epoch 28: loss improved from 0.42230 to 0.41958, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4196 - mae: 0.7961 - lr: 5.0119e-06\n",
            "Epoch 29/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.4160 - mae: 0.7918\n",
            "Epoch 29: loss improved from 0.41958 to 0.41517, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4152 - mae: 0.7906 - lr: 6.3096e-06\n",
            "Epoch 30/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4116 - mae: 0.7864\n",
            "Epoch 30: loss improved from 0.41517 to 0.41105, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4110 - mae: 0.7857 - lr: 7.9433e-06\n",
            "Epoch 31/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.4057 - mae: 0.7804\n",
            "Epoch 31: loss improved from 0.41105 to 0.40507, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.4051 - mae: 0.7782 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3989 - mae: 0.7713\n",
            "Epoch 32: loss improved from 0.40507 to 0.39800, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3980 - mae: 0.7695 - lr: 1.2589e-05\n",
            "Epoch 33/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.3913 - mae: 0.7620\n",
            "Epoch 33: loss improved from 0.39800 to 0.38870, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3887 - mae: 0.7579 - lr: 1.5849e-05\n",
            "Epoch 34/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3780 - mae: 0.7438\n",
            "Epoch 34: loss improved from 0.38870 to 0.37725, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3773 - mae: 0.7433 - lr: 1.9953e-05\n",
            "Epoch 35/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3667 - mae: 0.7299\n",
            "Epoch 35: loss improved from 0.37725 to 0.36516, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3652 - mae: 0.7278 - lr: 2.5119e-05\n",
            "Epoch 36/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3491 - mae: 0.7073\n",
            "Epoch 36: loss improved from 0.36516 to 0.34887, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3489 - mae: 0.7070 - lr: 3.1623e-05\n",
            "Epoch 37/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3344 - mae: 0.6875\n",
            "Epoch 37: loss improved from 0.34887 to 0.33244, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3324 - mae: 0.6854 - lr: 3.9811e-05\n",
            "Epoch 38/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3139 - mae: 0.6610\n",
            "Epoch 38: loss improved from 0.33244 to 0.31280, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3128 - mae: 0.6595 - lr: 5.0119e-05\n",
            "Epoch 39/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.2923 - mae: 0.6315\n",
            "Epoch 39: loss improved from 0.31280 to 0.29189, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.2919 - mae: 0.6313 - lr: 6.3096e-05\n",
            "Epoch 40/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2686 - mae: 0.6015\n",
            "Epoch 40: loss improved from 0.29189 to 0.27085, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.2708 - mae: 0.6028 - lr: 7.9433e-05\n",
            "Epoch 41/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2515 - mae: 0.5763\n",
            "Epoch 41: loss improved from 0.27085 to 0.25025, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2503 - mae: 0.5746 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2372 - mae: 0.5567\n",
            "Epoch 42: loss improved from 0.25025 to 0.23315, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2332 - mae: 0.5513 - lr: 1.2589e-04\n",
            "Epoch 43/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2195 - mae: 0.5334\n",
            "Epoch 43: loss improved from 0.23315 to 0.22006, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2201 - mae: 0.5342 - lr: 1.5849e-04\n",
            "Epoch 44/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2130 - mae: 0.5250\n",
            "Epoch 44: loss improved from 0.22006 to 0.21050, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2105 - mae: 0.5222 - lr: 1.9953e-04\n",
            "Epoch 45/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2082 - mae: 0.5195\n",
            "Epoch 45: loss improved from 0.21050 to 0.20677, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2068 - mae: 0.5180 - lr: 2.5119e-04\n",
            "Epoch 46/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2047 - mae: 0.5156\n",
            "Epoch 46: loss improved from 0.20677 to 0.20515, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2051 - mae: 0.5168 - lr: 3.1623e-04\n",
            "Epoch 47/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2046 - mae: 0.5166\n",
            "Epoch 47: loss improved from 0.20515 to 0.20458, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2046 - mae: 0.5166 - lr: 3.9811e-04\n",
            "Epoch 48/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2007 - mae: 0.5106\n",
            "Epoch 48: loss improved from 0.20458 to 0.20399, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2040 - mae: 0.5157 - lr: 5.0119e-04\n",
            "Epoch 49/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.2044 - mae: 0.5159\n",
            "Epoch 49: loss improved from 0.20399 to 0.20350, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2035 - mae: 0.5148 - lr: 6.3096e-04\n",
            "Epoch 50/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2042 - mae: 0.5154\n",
            "Epoch 50: loss did not improve from 0.20350\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2037 - mae: 0.5149 - lr: 7.9433e-04\n",
            "Epoch 51/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2026 - mae: 0.5135\n",
            "Epoch 51: loss improved from 0.20350 to 0.20322, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2032 - mae: 0.5143 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2011 - mae: 0.5113\n",
            "Epoch 52: loss improved from 0.20322 to 0.20289, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2029 - mae: 0.5139 - lr: 0.0013\n",
            "Epoch 53/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2047 - mae: 0.5159\n",
            "Epoch 53: loss improved from 0.20289 to 0.20265, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2027 - mae: 0.5135 - lr: 0.0016\n",
            "Epoch 54/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2015 - mae: 0.5112\n",
            "Epoch 54: loss improved from 0.20265 to 0.20147, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2015 - mae: 0.5112 - lr: 0.0020\n",
            "Epoch 55/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1999 - mae: 0.5107\n",
            "Epoch 55: loss improved from 0.20147 to 0.20059, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2006 - mae: 0.5108 - lr: 0.0025\n",
            "Epoch 56/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1983 - mae: 0.5075\n",
            "Epoch 56: loss improved from 0.20059 to 0.20029, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2003 - mae: 0.5094 - lr: 0.0032\n",
            "Epoch 57/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2013 - mae: 0.5112\n",
            "Epoch 57: loss improved from 0.20029 to 0.19951, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.1995 - mae: 0.5085 - lr: 0.0040\n",
            "Epoch 58/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1965 - mae: 0.5030\n",
            "Epoch 58: loss improved from 0.19951 to 0.19800, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1980 - mae: 0.5062 - lr: 0.0050\n",
            "Epoch 59/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1974 - mae: 0.5047\n",
            "Epoch 59: loss improved from 0.19800 to 0.19704, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1970 - mae: 0.5041 - lr: 0.0063\n",
            "Epoch 60/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1967 - mae: 0.5043\n",
            "Epoch 60: loss improved from 0.19704 to 0.19618, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1962 - mae: 0.5034 - lr: 0.0079\n",
            "Epoch 61/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1948 - mae: 0.5024\n",
            "Epoch 61: loss improved from 0.19618 to 0.19482, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1948 - mae: 0.5024 - lr: 0.0100\n",
            "Epoch 62/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1959 - mae: 0.5034\n",
            "Epoch 62: loss improved from 0.19482 to 0.19418, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1942 - mae: 0.5019 - lr: 0.0126\n",
            "Epoch 63/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1931 - mae: 0.5001\n",
            "Epoch 63: loss improved from 0.19418 to 0.19186, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1919 - mae: 0.4985 - lr: 0.0158\n",
            "Epoch 64/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1893 - mae: 0.4946\n",
            "Epoch 64: loss improved from 0.19186 to 0.18903, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1890 - mae: 0.4942 - lr: 0.0200\n",
            "Epoch 65/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1886 - mae: 0.4916\n",
            "Epoch 65: loss improved from 0.18903 to 0.18859, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1886 - mae: 0.4916 - lr: 0.0251\n",
            "Epoch 66/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1841 - mae: 0.4852\n",
            "Epoch 66: loss improved from 0.18859 to 0.18471, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1847 - mae: 0.4871 - lr: 0.0316\n",
            "Epoch 67/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1845 - mae: 0.4871\n",
            "Epoch 67: loss improved from 0.18471 to 0.18447, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1845 - mae: 0.4871 - lr: 0.0398\n",
            "Epoch 68/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1828 - mae: 0.4831\n",
            "Epoch 68: loss improved from 0.18447 to 0.18209, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1821 - mae: 0.4820 - lr: 0.0501\n",
            "Epoch 69/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1910 - mae: 0.4971\n",
            "Epoch 69: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1910 - mae: 0.4971 - lr: 0.0631\n",
            "Epoch 70/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1926 - mae: 0.4987\n",
            "Epoch 70: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1926 - mae: 0.4987 - lr: 0.0794\n",
            "Epoch 71/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1989 - mae: 0.5085\n",
            "Epoch 71: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1977 - mae: 0.5082 - lr: 0.1000\n",
            "Epoch 72/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1823 - mae: 0.4868\n",
            "Epoch 72: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1824 - mae: 0.4872 - lr: 0.1259\n",
            "Epoch 73/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1918 - mae: 0.4980\n",
            "Epoch 73: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1918 - mae: 0.4980 - lr: 0.1585\n",
            "Epoch 74/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1852 - mae: 0.4883\n",
            "Epoch 74: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1852 - mae: 0.4883 - lr: 0.1995\n",
            "Epoch 75/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1838 - mae: 0.4870\n",
            "Epoch 75: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1838 - mae: 0.4868 - lr: 0.2512\n",
            "Epoch 76/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1952 - mae: 0.5046\n",
            "Epoch 76: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1957 - mae: 0.5046 - lr: 0.3162\n",
            "Epoch 77/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2228 - mae: 0.5414\n",
            "Epoch 77: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.2229 - mae: 0.5417 - lr: 0.3981\n",
            "Epoch 78/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2002 - mae: 0.5115\n",
            "Epoch 78: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2002 - mae: 0.5115 - lr: 0.5012\n",
            "Epoch 79/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2290 - mae: 0.5562\n",
            "Epoch 79: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2249 - mae: 0.5506 - lr: 0.6310\n",
            "Epoch 80/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2488 - mae: 0.5830\n",
            "Epoch 80: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2488 - mae: 0.5830 - lr: 0.7943\n",
            "Epoch 81/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.2047 - mae: 0.5127\n",
            "Epoch 81: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.2052 - mae: 0.5134 - lr: 1.0000\n",
            "Epoch 82/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2551 - mae: 0.5872\n",
            "Epoch 82: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.2528 - mae: 0.5846 - lr: 1.2589\n",
            "Epoch 83/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3347 - mae: 0.6798\n",
            "Epoch 83: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.3347 - mae: 0.6798 - lr: 1.5849\n",
            "Epoch 84/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 39.9199 - mae: 40.4031\n",
            "Epoch 84: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 40.1121 - mae: 40.5955 - lr: 1.9953\n",
            "Epoch 85/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 58.8410 - mae: 59.3351\n",
            "Epoch 85: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 58.8410 - mae: 59.3351 - lr: 2.5119\n",
            "Epoch 86/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 74.2006 - mae: 74.6970\n",
            "Epoch 86: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 74.2006 - mae: 74.6970 - lr: 3.1623\n",
            "Epoch 87/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 92.1896 - mae: 92.6863\n",
            "Epoch 87: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 93.5219 - mae: 94.0187 - lr: 3.9811\n",
            "Epoch 88/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 117.5348 - mae: 118.0318\n",
            "Epoch 88: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 117.5402 - mae: 118.0372 - lr: 5.0119\n",
            "Epoch 89/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 148.5300 - mae: 149.0276\n",
            "Epoch 89: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 2s 19ms/step - loss: 148.5281 - mae: 149.0256 - lr: 6.3096\n",
            "Epoch 90/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 188.2355 - mae: 188.7335\n",
            "Epoch 90: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 187.1071 - mae: 187.6051 - lr: 7.9433\n",
            "Epoch 91/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 235.6656 - mae: 236.1640\n",
            "Epoch 91: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 2s 18ms/step - loss: 235.6656 - mae: 236.1640 - lr: 10.0000\n",
            "Epoch 92/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 296.8055 - mae: 297.3044\n",
            "Epoch 92: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 296.8034 - mae: 297.3023 - lr: 12.5893\n",
            "Epoch 93/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 374.7214 - mae: 375.2206\n",
            "Epoch 93: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 373.7394 - mae: 374.2386 - lr: 15.8489\n",
            "Epoch 94/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 470.7026 - mae: 471.2022\n",
            "Epoch 94: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 470.7026 - mae: 471.2022 - lr: 19.9526\n",
            "Epoch 95/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 592.6713 - mae: 593.1709\n",
            "Epoch 95: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 592.6750 - mae: 593.1747 - lr: 25.1189\n",
            "Epoch 96/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 745.2708 - mae: 745.7692\n",
            "Epoch 96: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 744.2723 - mae: 744.7703 - lr: 31.6228\n",
            "Epoch 97/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 918.8187 - mae: 919.2530\n",
            "Epoch 97: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 918.8193 - mae: 919.2543 - lr: 39.8107\n",
            "Epoch 98/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 1156.7814 - mae: 1157.2590\n",
            "Epoch 98: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 1156.7814 - mae: 1157.2590 - lr: 50.1187\n",
            "Epoch 99/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 1456.5903 - mae: 1457.0856\n",
            "Epoch 99: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 1456.3801 - mae: 1456.8755 - lr: 63.0957\n",
            "Epoch 100/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 1833.5482 - mae: 1834.0476\n",
            "Epoch 100: loss did not improve from 0.18209\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 1833.6372 - mae: 1834.1365 - lr: 79.4328\n"
          ]
        }
      ],
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xriMOrio8yRA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "4b718209-551b-4415-bb62-c46a756070b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 0.5])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj2yfIFz81LC"
      },
      "outputs": [],
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khE1V79z87ZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945ef92e-4706-439c-e4d4-f27ff1b05c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "     63/Unknown - 3s 11ms/step - loss: 0.1812 - mae: 0.4809\n",
            "Epoch 1: loss improved from inf to 0.17945, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 4s 29ms/step - loss: 0.1794 - mae: 0.4802 - val_loss: 0.1582 - val_mae: 0.4482\n",
            "Epoch 2/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1811 - mae: 0.4827\n",
            "Epoch 2: loss did not improve from 0.17945\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1811 - mae: 0.4827 - val_loss: 0.1607 - val_mae: 0.4514\n",
            "Epoch 3/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1774 - mae: 0.4792\n",
            "Epoch 3: loss improved from 0.17945 to 0.17746, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1775 - mae: 0.4793 - val_loss: 0.1600 - val_mae: 0.4484\n",
            "Epoch 4/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1773 - mae: 0.4774\n",
            "Epoch 4: loss did not improve from 0.17746\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1778 - mae: 0.4778 - val_loss: 0.1603 - val_mae: 0.4494\n",
            "Epoch 5/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4741\n",
            "Epoch 5: loss improved from 0.17746 to 0.17728, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1773 - mae: 0.4761 - val_loss: 0.1584 - val_mae: 0.4474\n",
            "Epoch 6/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1771 - mae: 0.4781\n",
            "Epoch 6: loss improved from 0.17728 to 0.17708, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1771 - mae: 0.4781 - val_loss: 0.1579 - val_mae: 0.4479\n",
            "Epoch 7/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1781 - mae: 0.4774\n",
            "Epoch 7: loss did not improve from 0.17708\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1776 - mae: 0.4771 - val_loss: 0.1617 - val_mae: 0.4533\n",
            "Epoch 8/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1780 - mae: 0.4781\n",
            "Epoch 8: loss did not improve from 0.17708\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1780 - mae: 0.4784 - val_loss: 0.1571 - val_mae: 0.4447\n",
            "Epoch 9/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1782 - mae: 0.4782\n",
            "Epoch 9: loss improved from 0.17708 to 0.17655, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1766 - mae: 0.4760 - val_loss: 0.1582 - val_mae: 0.4467\n",
            "Epoch 10/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1785 - mae: 0.4799\n",
            "Epoch 10: loss did not improve from 0.17655\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1774 - mae: 0.4778 - val_loss: 0.1592 - val_mae: 0.4473\n",
            "Epoch 11/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1775 - mae: 0.4764\n",
            "Epoch 11: loss did not improve from 0.17655\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1769 - mae: 0.4757 - val_loss: 0.1574 - val_mae: 0.4473\n",
            "Epoch 12/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1768 - mae: 0.4764\n",
            "Epoch 12: loss did not improve from 0.17655\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1766 - mae: 0.4760 - val_loss: 0.1585 - val_mae: 0.4488\n",
            "Epoch 13/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1765 - mae: 0.4760\n",
            "Epoch 13: loss improved from 0.17655 to 0.17651, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1765 - mae: 0.4760 - val_loss: 0.1564 - val_mae: 0.4441\n",
            "Epoch 14/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4770\n",
            "Epoch 14: loss improved from 0.17651 to 0.17630, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1763 - mae: 0.4764 - val_loss: 0.1560 - val_mae: 0.4430\n",
            "Epoch 15/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4755\n",
            "Epoch 15: loss did not improve from 0.17630\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1769 - mae: 0.4756 - val_loss: 0.1579 - val_mae: 0.4455\n",
            "Epoch 16/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1770 - mae: 0.4764\n",
            "Epoch 16: loss did not improve from 0.17630\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1774 - mae: 0.4773 - val_loss: 0.1570 - val_mae: 0.4447\n",
            "Epoch 17/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1766 - mae: 0.4759\n",
            "Epoch 17: loss did not improve from 0.17630\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1766 - mae: 0.4759 - val_loss: 0.1565 - val_mae: 0.4443\n",
            "Epoch 18/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1754 - mae: 0.4744\n",
            "Epoch 18: loss did not improve from 0.17630\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1764 - mae: 0.4763 - val_loss: 0.1571 - val_mae: 0.4445\n",
            "Epoch 19/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4760\n",
            "Epoch 19: loss did not improve from 0.17630\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1767 - mae: 0.4762 - val_loss: 0.1580 - val_mae: 0.4455\n",
            "Epoch 20/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1772 - mae: 0.4769\n",
            "Epoch 20: loss did not improve from 0.17630\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1765 - mae: 0.4751 - val_loss: 0.1572 - val_mae: 0.4467\n",
            "Epoch 21/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1767 - mae: 0.4760\n",
            "Epoch 21: loss improved from 0.17630 to 0.17588, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1589 - val_mae: 0.4470\n",
            "Epoch 22/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4726\n",
            "Epoch 22: loss did not improve from 0.17588\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1766 - mae: 0.4757 - val_loss: 0.1576 - val_mae: 0.4463\n",
            "Epoch 23/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1774 - mae: 0.4773\n",
            "Epoch 23: loss improved from 0.17588 to 0.17572, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1757 - mae: 0.4743 - val_loss: 0.1581 - val_mae: 0.4469\n",
            "Epoch 24/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4746\n",
            "Epoch 24: loss did not improve from 0.17572\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1760 - mae: 0.4744 - val_loss: 0.1570 - val_mae: 0.4455\n",
            "Epoch 25/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4770\n",
            "Epoch 25: loss did not improve from 0.17572\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1767 - mae: 0.4772 - val_loss: 0.1563 - val_mae: 0.4446\n",
            "Epoch 26/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4762\n",
            "Epoch 26: loss did not improve from 0.17572\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1579 - val_mae: 0.4457\n",
            "Epoch 27/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1776 - mae: 0.4775\n",
            "Epoch 27: loss did not improve from 0.17572\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1761 - mae: 0.4756 - val_loss: 0.1561 - val_mae: 0.4429\n",
            "Epoch 28/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4744\n",
            "Epoch 28: loss did not improve from 0.17572\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1761 - mae: 0.4744 - val_loss: 0.1573 - val_mae: 0.4449\n",
            "Epoch 29/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1774 - mae: 0.4759\n",
            "Epoch 29: loss did not improve from 0.17572\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1769 - mae: 0.4758 - val_loss: 0.1571 - val_mae: 0.4458\n",
            "Epoch 30/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4740\n",
            "Epoch 30: loss improved from 0.17572 to 0.17567, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1757 - mae: 0.4754 - val_loss: 0.1565 - val_mae: 0.4440\n",
            "Epoch 31/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4701\n",
            "Epoch 31: loss improved from 0.17567 to 0.17528, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1753 - mae: 0.4733 - val_loss: 0.1559 - val_mae: 0.4439\n",
            "Epoch 32/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1751 - mae: 0.4740\n",
            "Epoch 32: loss improved from 0.17528 to 0.17506, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1751 - mae: 0.4740 - val_loss: 0.1552 - val_mae: 0.4421\n",
            "Epoch 33/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4714\n",
            "Epoch 33: loss did not improve from 0.17506\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1752 - mae: 0.4732 - val_loss: 0.1539 - val_mae: 0.4418\n",
            "Epoch 34/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4743\n",
            "Epoch 34: loss did not improve from 0.17506\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1759 - mae: 0.4743 - val_loss: 0.1556 - val_mae: 0.4438\n",
            "Epoch 35/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1745 - mae: 0.4732\n",
            "Epoch 35: loss improved from 0.17506 to 0.17447, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1745 - mae: 0.4732 - val_loss: 0.1565 - val_mae: 0.4439\n",
            "Epoch 36/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 36: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1573 - val_mae: 0.4470\n",
            "Epoch 37/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1726 - mae: 0.4705\n",
            "Epoch 37: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1753 - mae: 0.4736 - val_loss: 0.1552 - val_mae: 0.4430\n",
            "Epoch 38/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4770\n",
            "Epoch 38: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1757 - mae: 0.4750 - val_loss: 0.1570 - val_mae: 0.4451\n",
            "Epoch 39/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 39: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1563 - val_mae: 0.4432\n",
            "Epoch 40/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4759\n",
            "Epoch 40: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1564 - val_mae: 0.4440\n",
            "Epoch 41/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4766\n",
            "Epoch 41: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1762 - mae: 0.4747 - val_loss: 0.1561 - val_mae: 0.4434\n",
            "Epoch 42/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4754\n",
            "Epoch 42: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1752 - mae: 0.4742 - val_loss: 0.1572 - val_mae: 0.4459\n",
            "Epoch 43/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4737\n",
            "Epoch 43: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1557 - val_mae: 0.4431\n",
            "Epoch 44/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4734\n",
            "Epoch 44: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1562 - val_mae: 0.4440\n",
            "Epoch 45/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 45: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1571 - val_mae: 0.4451\n",
            "Epoch 46/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1758 - mae: 0.4745\n",
            "Epoch 46: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1755 - mae: 0.4739 - val_loss: 0.1568 - val_mae: 0.4444\n",
            "Epoch 47/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4729\n",
            "Epoch 47: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1754 - mae: 0.4736 - val_loss: 0.1571 - val_mae: 0.4465\n",
            "Epoch 48/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1760 - mae: 0.4745\n",
            "Epoch 48: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1749 - mae: 0.4733 - val_loss: 0.1555 - val_mae: 0.4445\n",
            "Epoch 49/500\n",
            "62/68 [==========================>...] - ETA: 0s - loss: 0.1743 - mae: 0.4737\n",
            "Epoch 49: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1752 - mae: 0.4741 - val_loss: 0.1567 - val_mae: 0.4463\n",
            "Epoch 50/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 50: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1745 - mae: 0.4729 - val_loss: 0.1566 - val_mae: 0.4441\n",
            "Epoch 51/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4744\n",
            "Epoch 51: loss did not improve from 0.17447\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1756 - mae: 0.4740 - val_loss: 0.1562 - val_mae: 0.4433\n",
            "Epoch 52/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4737\n",
            "Epoch 52: loss improved from 0.17447 to 0.17438, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1744 - mae: 0.4730 - val_loss: 0.1565 - val_mae: 0.4437\n",
            "Epoch 53/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4737\n",
            "Epoch 53: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1750 - mae: 0.4735 - val_loss: 0.1566 - val_mae: 0.4437\n",
            "Epoch 54/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 54: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1751 - mae: 0.4739 - val_loss: 0.1566 - val_mae: 0.4444\n",
            "Epoch 55/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4731\n",
            "Epoch 55: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1749 - mae: 0.4730 - val_loss: 0.1563 - val_mae: 0.4445\n",
            "Epoch 56/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1742 - mae: 0.4712\n",
            "Epoch 56: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 2s 33ms/step - loss: 0.1746 - mae: 0.4729 - val_loss: 0.1566 - val_mae: 0.4444\n",
            "Epoch 57/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1753 - mae: 0.4736\n",
            "Epoch 57: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1753 - mae: 0.4736 - val_loss: 0.1523 - val_mae: 0.4388\n",
            "Epoch 58/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1750 - mae: 0.4736\n",
            "Epoch 58: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1750 - mae: 0.4736 - val_loss: 0.1563 - val_mae: 0.4450\n",
            "Epoch 59/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4741\n",
            "Epoch 59: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1751 - mae: 0.4742 - val_loss: 0.1562 - val_mae: 0.4447\n",
            "Epoch 60/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4737\n",
            "Epoch 60: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1754 - mae: 0.4737 - val_loss: 0.1572 - val_mae: 0.4445\n",
            "Epoch 61/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4739\n",
            "Epoch 61: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1753 - mae: 0.4743 - val_loss: 0.1552 - val_mae: 0.4422\n",
            "Epoch 62/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1750 - mae: 0.4733\n",
            "Epoch 62: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1750 - mae: 0.4733 - val_loss: 0.1553 - val_mae: 0.4425\n",
            "Epoch 63/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4727\n",
            "Epoch 63: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1754 - mae: 0.4739 - val_loss: 0.1562 - val_mae: 0.4438\n",
            "Epoch 64/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1747 - mae: 0.4723\n",
            "Epoch 64: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1747 - mae: 0.4723 - val_loss: 0.1564 - val_mae: 0.4441\n",
            "Epoch 65/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1751 - mae: 0.4739\n",
            "Epoch 65: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1751 - mae: 0.4739 - val_loss: 0.1551 - val_mae: 0.4419\n",
            "Epoch 66/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1770 - mae: 0.4749\n",
            "Epoch 66: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1750 - mae: 0.4731 - val_loss: 0.1569 - val_mae: 0.4463\n",
            "Epoch 67/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4740\n",
            "Epoch 67: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1747 - mae: 0.4734 - val_loss: 0.1546 - val_mae: 0.4416\n",
            "Epoch 68/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1750 - mae: 0.4735\n",
            "Epoch 68: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1750 - mae: 0.4735 - val_loss: 0.1560 - val_mae: 0.4436\n",
            "Epoch 69/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4715\n",
            "Epoch 69: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1745 - mae: 0.4726 - val_loss: 0.1560 - val_mae: 0.4431\n",
            "Epoch 70/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1752 - mae: 0.4736\n",
            "Epoch 70: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1752 - mae: 0.4736 - val_loss: 0.1570 - val_mae: 0.4454\n",
            "Epoch 71/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4737\n",
            "Epoch 71: loss did not improve from 0.17438\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1747 - mae: 0.4731 - val_loss: 0.1547 - val_mae: 0.4412\n",
            "Epoch 72/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4724\n",
            "Epoch 72: loss improved from 0.17438 to 0.17433, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1743 - mae: 0.4719 - val_loss: 0.1560 - val_mae: 0.4439\n",
            "Epoch 73/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4725\n",
            "Epoch 73: loss did not improve from 0.17433\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1746 - mae: 0.4731 - val_loss: 0.1560 - val_mae: 0.4437\n",
            "Epoch 74/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1776 - mae: 0.4782\n",
            "Epoch 74: loss did not improve from 0.17433\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1746 - mae: 0.4729 - val_loss: 0.1554 - val_mae: 0.4428\n",
            "Epoch 75/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4728\n",
            "Epoch 75: loss improved from 0.17433 to 0.17432, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1743 - mae: 0.4728 - val_loss: 0.1558 - val_mae: 0.4440\n",
            "Epoch 76/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4737\n",
            "Epoch 76: loss did not improve from 0.17432\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1750 - mae: 0.4731 - val_loss: 0.1572 - val_mae: 0.4459\n",
            "Epoch 77/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1740 - mae: 0.4722\n",
            "Epoch 77: loss did not improve from 0.17432\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1746 - mae: 0.4734 - val_loss: 0.1562 - val_mae: 0.4436\n",
            "Epoch 78/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1771 - mae: 0.4768\n",
            "Epoch 78: loss did not improve from 0.17432\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1746 - mae: 0.4727 - val_loss: 0.1549 - val_mae: 0.4425\n",
            "Epoch 79/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1751 - mae: 0.4731\n",
            "Epoch 79: loss did not improve from 0.17432\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1751 - mae: 0.4731 - val_loss: 0.1567 - val_mae: 0.4456\n",
            "Epoch 80/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1745 - mae: 0.4727\n",
            "Epoch 80: loss did not improve from 0.17432\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1745 - mae: 0.4727 - val_loss: 0.1566 - val_mae: 0.4449\n",
            "Epoch 81/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1778 - mae: 0.4786\n",
            "Epoch 81: loss did not improve from 0.17432\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1748 - mae: 0.4732 - val_loss: 0.1564 - val_mae: 0.4443\n",
            "Epoch 82/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1760 - mae: 0.4742\n",
            "Epoch 82: loss did not improve from 0.17432\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1747 - mae: 0.4730 - val_loss: 0.1567 - val_mae: 0.4449\n",
            "Epoch 83/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1745 - mae: 0.4720\n",
            "Epoch 83: loss improved from 0.17432 to 0.17385, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1739 - mae: 0.4716 - val_loss: 0.1566 - val_mae: 0.4437\n",
            "Epoch 84/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4730\n",
            "Epoch 84: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1747 - mae: 0.4732 - val_loss: 0.1550 - val_mae: 0.4417\n",
            "Epoch 85/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4717\n",
            "Epoch 85: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1744 - mae: 0.4719 - val_loss: 0.1560 - val_mae: 0.4444\n",
            "Epoch 86/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1751 - mae: 0.4731\n",
            "Epoch 86: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1745 - mae: 0.4723 - val_loss: 0.1569 - val_mae: 0.4457\n",
            "Epoch 87/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4754\n",
            "Epoch 87: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1747 - mae: 0.4733 - val_loss: 0.1553 - val_mae: 0.4426\n",
            "Epoch 88/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 88: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1743 - mae: 0.4720 - val_loss: 0.1552 - val_mae: 0.4426\n",
            "Epoch 89/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4704\n",
            "Epoch 89: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1749 - mae: 0.4731 - val_loss: 0.1560 - val_mae: 0.4428\n",
            "Epoch 90/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4745\n",
            "Epoch 90: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1750 - mae: 0.4738 - val_loss: 0.1568 - val_mae: 0.4450\n",
            "Epoch 91/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4757\n",
            "Epoch 91: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1745 - mae: 0.4723 - val_loss: 0.1557 - val_mae: 0.4429\n",
            "Epoch 92/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1742 - mae: 0.4717\n",
            "Epoch 92: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1742 - mae: 0.4717 - val_loss: 0.1564 - val_mae: 0.4442\n",
            "Epoch 93/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1757 - mae: 0.4743\n",
            "Epoch 93: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1748 - mae: 0.4731 - val_loss: 0.1570 - val_mae: 0.4457\n",
            "Epoch 94/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1748 - mae: 0.4733\n",
            "Epoch 94: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1747 - mae: 0.4736 - val_loss: 0.1562 - val_mae: 0.4443\n",
            "Epoch 95/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1755 - mae: 0.4734\n",
            "Epoch 95: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1743 - mae: 0.4721 - val_loss: 0.1570 - val_mae: 0.4460\n",
            "Epoch 96/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4732\n",
            "Epoch 96: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1745 - mae: 0.4726 - val_loss: 0.1541 - val_mae: 0.4410\n",
            "Epoch 97/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1760 - mae: 0.4739\n",
            "Epoch 97: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1748 - mae: 0.4726 - val_loss: 0.1553 - val_mae: 0.4428\n",
            "Epoch 98/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4723\n",
            "Epoch 98: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1740 - mae: 0.4719 - val_loss: 0.1556 - val_mae: 0.4426\n",
            "Epoch 99/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1748 - mae: 0.4732\n",
            "Epoch 99: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1748 - mae: 0.4732 - val_loss: 0.1554 - val_mae: 0.4429\n",
            "Epoch 100/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1753 - mae: 0.4744\n",
            "Epoch 100: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1744 - mae: 0.4723 - val_loss: 0.1553 - val_mae: 0.4426\n",
            "Epoch 101/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4746\n",
            "Epoch 101: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4726 - val_loss: 0.1546 - val_mae: 0.4419\n",
            "Epoch 102/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1751 - mae: 0.4738\n",
            "Epoch 102: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1742 - mae: 0.4721 - val_loss: 0.1556 - val_mae: 0.4434\n",
            "Epoch 103/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1749 - mae: 0.4737\n",
            "Epoch 103: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1749 - mae: 0.4737 - val_loss: 0.1545 - val_mae: 0.4413\n",
            "Epoch 104/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1754 - mae: 0.4731\n",
            "Epoch 104: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1748 - mae: 0.4726 - val_loss: 0.1555 - val_mae: 0.4439\n",
            "Epoch 105/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4720\n",
            "Epoch 105: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1743 - mae: 0.4720 - val_loss: 0.1569 - val_mae: 0.4454\n",
            "Epoch 106/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4707\n",
            "Epoch 106: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1747 - mae: 0.4730 - val_loss: 0.1550 - val_mae: 0.4418\n",
            "Epoch 107/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1771 - mae: 0.4755\n",
            "Epoch 107: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1744 - mae: 0.4725 - val_loss: 0.1559 - val_mae: 0.4436\n",
            "Epoch 108/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1744 - mae: 0.4720\n",
            "Epoch 108: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1744 - mae: 0.4720 - val_loss: 0.1561 - val_mae: 0.4434\n",
            "Epoch 109/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1741 - mae: 0.4723\n",
            "Epoch 109: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1740 - mae: 0.4720 - val_loss: 0.1554 - val_mae: 0.4425\n",
            "Epoch 110/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1741 - mae: 0.4717\n",
            "Epoch 110: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4717 - val_loss: 0.1559 - val_mae: 0.4431\n",
            "Epoch 111/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4732\n",
            "Epoch 111: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1745 - mae: 0.4726 - val_loss: 0.1557 - val_mae: 0.4434\n",
            "Epoch 112/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4722\n",
            "Epoch 112: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1748 - mae: 0.4731 - val_loss: 0.1573 - val_mae: 0.4470\n",
            "Epoch 113/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4729\n",
            "Epoch 113: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1744 - mae: 0.4723 - val_loss: 0.1562 - val_mae: 0.4445\n",
            "Epoch 114/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4733\n",
            "Epoch 114: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1743 - mae: 0.4724 - val_loss: 0.1569 - val_mae: 0.4452\n",
            "Epoch 115/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4727\n",
            "Epoch 115: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1747 - mae: 0.4726 - val_loss: 0.1548 - val_mae: 0.4415\n",
            "Epoch 116/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1740 - mae: 0.4714\n",
            "Epoch 116: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1746 - mae: 0.4722 - val_loss: 0.1551 - val_mae: 0.4424\n",
            "Epoch 117/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1702 - mae: 0.4654\n",
            "Epoch 117: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1748 - mae: 0.4732 - val_loss: 0.1555 - val_mae: 0.4437\n",
            "Epoch 118/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4715\n",
            "Epoch 118: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1747 - mae: 0.4733 - val_loss: 0.1564 - val_mae: 0.4441\n",
            "Epoch 119/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1769 - mae: 0.4760\n",
            "Epoch 119: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1749 - mae: 0.4728 - val_loss: 0.1560 - val_mae: 0.4437\n",
            "Epoch 120/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1761 - mae: 0.4759\n",
            "Epoch 120: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1747 - mae: 0.4730 - val_loss: 0.1552 - val_mae: 0.4424\n",
            "Epoch 121/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1761 - mae: 0.4745\n",
            "Epoch 121: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1741 - mae: 0.4718 - val_loss: 0.1559 - val_mae: 0.4441\n",
            "Epoch 122/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4727\n",
            "Epoch 122: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1747 - mae: 0.4737 - val_loss: 0.1564 - val_mae: 0.4435\n",
            "Epoch 123/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1751 - mae: 0.4724\n",
            "Epoch 123: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1746 - mae: 0.4718 - val_loss: 0.1566 - val_mae: 0.4444\n",
            "Epoch 124/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1751 - mae: 0.4735\n",
            "Epoch 124: loss did not improve from 0.17385\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1745 - mae: 0.4725 - val_loss: 0.1544 - val_mae: 0.4418\n",
            "Epoch 125/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4745\n",
            "Epoch 125: loss improved from 0.17385 to 0.17384, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4722 - val_loss: 0.1553 - val_mae: 0.4425\n",
            "Epoch 126/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1746 - mae: 0.4723\n",
            "Epoch 126: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1746 - mae: 0.4723 - val_loss: 0.1569 - val_mae: 0.4451\n",
            "Epoch 127/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4704\n",
            "Epoch 127: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1744 - mae: 0.4724 - val_loss: 0.1557 - val_mae: 0.4431\n",
            "Epoch 128/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4722\n",
            "Epoch 128: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1745 - mae: 0.4728 - val_loss: 0.1561 - val_mae: 0.4450\n",
            "Epoch 129/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4724\n",
            "Epoch 129: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1742 - mae: 0.4724 - val_loss: 0.1561 - val_mae: 0.4436\n",
            "Epoch 130/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1746 - mae: 0.4722\n",
            "Epoch 130: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1746 - mae: 0.4722 - val_loss: 0.1560 - val_mae: 0.4442\n",
            "Epoch 131/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4737\n",
            "Epoch 131: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1744 - mae: 0.4725 - val_loss: 0.1560 - val_mae: 0.4438\n",
            "Epoch 132/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1736 - mae: 0.4716\n",
            "Epoch 132: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1746 - mae: 0.4728 - val_loss: 0.1560 - val_mae: 0.4435\n",
            "Epoch 133/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1753 - mae: 0.4731\n",
            "Epoch 133: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4719 - val_loss: 0.1542 - val_mae: 0.4417\n",
            "Epoch 134/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4694\n",
            "Epoch 134: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4720 - val_loss: 0.1557 - val_mae: 0.4430\n",
            "Epoch 135/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1744 - mae: 0.4725\n",
            "Epoch 135: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1744 - mae: 0.4725 - val_loss: 0.1546 - val_mae: 0.4416\n",
            "Epoch 136/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4724\n",
            "Epoch 136: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4720 - val_loss: 0.1550 - val_mae: 0.4421\n",
            "Epoch 137/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1745 - mae: 0.4717\n",
            "Epoch 137: loss did not improve from 0.17384\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1744 - mae: 0.4719 - val_loss: 0.1556 - val_mae: 0.4430\n",
            "Epoch 138/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4710\n",
            "Epoch 138: loss improved from 0.17384 to 0.17377, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1738 - mae: 0.4716 - val_loss: 0.1560 - val_mae: 0.4438\n",
            "Epoch 139/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4722\n",
            "Epoch 139: loss improved from 0.17377 to 0.17374, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4717 - val_loss: 0.1556 - val_mae: 0.4427\n",
            "Epoch 140/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4729\n",
            "Epoch 140: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1746 - mae: 0.4727 - val_loss: 0.1563 - val_mae: 0.4437\n",
            "Epoch 141/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1753 - mae: 0.4740\n",
            "Epoch 141: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4723 - val_loss: 0.1559 - val_mae: 0.4436\n",
            "Epoch 142/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4708\n",
            "Epoch 142: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4716 - val_loss: 0.1556 - val_mae: 0.4429\n",
            "Epoch 143/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1738 - mae: 0.4716\n",
            "Epoch 143: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4716 - val_loss: 0.1561 - val_mae: 0.4437\n",
            "Epoch 144/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1739 - mae: 0.4710\n",
            "Epoch 144: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1739 - mae: 0.4710 - val_loss: 0.1551 - val_mae: 0.4425\n",
            "Epoch 145/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4730\n",
            "Epoch 145: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1739 - mae: 0.4715 - val_loss: 0.1565 - val_mae: 0.4446\n",
            "Epoch 146/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4713\n",
            "Epoch 146: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1742 - mae: 0.4722 - val_loss: 0.1553 - val_mae: 0.4426\n",
            "Epoch 147/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4751\n",
            "Epoch 147: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1745 - mae: 0.4725 - val_loss: 0.1559 - val_mae: 0.4434\n",
            "Epoch 148/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4723\n",
            "Epoch 148: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4718 - val_loss: 0.1561 - val_mae: 0.4439\n",
            "Epoch 149/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1728 - mae: 0.4700\n",
            "Epoch 149: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1746 - mae: 0.4724 - val_loss: 0.1558 - val_mae: 0.4439\n",
            "Epoch 150/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1745 - mae: 0.4723\n",
            "Epoch 150: loss did not improve from 0.17374\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1745 - mae: 0.4723 - val_loss: 0.1569 - val_mae: 0.4452\n",
            "Epoch 151/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1724 - mae: 0.4697\n",
            "Epoch 151: loss improved from 0.17374 to 0.17373, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4714 - val_loss: 0.1553 - val_mae: 0.4424\n",
            "Epoch 152/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4715\n",
            "Epoch 152: loss did not improve from 0.17373\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4709 - val_loss: 0.1562 - val_mae: 0.4437\n",
            "Epoch 153/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4728\n",
            "Epoch 153: loss did not improve from 0.17373\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4718 - val_loss: 0.1542 - val_mae: 0.4406\n",
            "Epoch 154/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1740 - mae: 0.4719\n",
            "Epoch 154: loss did not improve from 0.17373\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4719 - val_loss: 0.1560 - val_mae: 0.4436\n",
            "Epoch 155/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4721\n",
            "Epoch 155: loss did not improve from 0.17373\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4721 - val_loss: 0.1549 - val_mae: 0.4422\n",
            "Epoch 156/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1772 - mae: 0.4773\n",
            "Epoch 156: loss did not improve from 0.17373\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1745 - mae: 0.4728 - val_loss: 0.1561 - val_mae: 0.4431\n",
            "Epoch 157/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4718\n",
            "Epoch 157: loss did not improve from 0.17373\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1743 - mae: 0.4718 - val_loss: 0.1554 - val_mae: 0.4427\n",
            "Epoch 158/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4705\n",
            "Epoch 158: loss improved from 0.17373 to 0.17338, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4705 - val_loss: 0.1533 - val_mae: 0.4401\n",
            "Epoch 159/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4722\n",
            "Epoch 159: loss did not improve from 0.17338\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4722 - val_loss: 0.1561 - val_mae: 0.4437\n",
            "Epoch 160/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4721\n",
            "Epoch 160: loss did not improve from 0.17338\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4721 - val_loss: 0.1553 - val_mae: 0.4424\n",
            "Epoch 161/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1751 - mae: 0.4735\n",
            "Epoch 161: loss did not improve from 0.17338\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4711 - val_loss: 0.1557 - val_mae: 0.4436\n",
            "Epoch 162/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1754 - mae: 0.4739\n",
            "Epoch 162: loss did not improve from 0.17338\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4724 - val_loss: 0.1550 - val_mae: 0.4417\n",
            "Epoch 163/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4734\n",
            "Epoch 163: loss did not improve from 0.17338\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1742 - mae: 0.4718 - val_loss: 0.1549 - val_mae: 0.4417\n",
            "Epoch 164/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4715\n",
            "Epoch 164: loss did not improve from 0.17338\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1739 - mae: 0.4718 - val_loss: 0.1543 - val_mae: 0.4412\n",
            "Epoch 165/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1711 - mae: 0.4678\n",
            "Epoch 165: loss improved from 0.17338 to 0.17308, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1731 - mae: 0.4705 - val_loss: 0.1563 - val_mae: 0.4434\n",
            "Epoch 166/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1745 - mae: 0.4720\n",
            "Epoch 166: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1745 - mae: 0.4720 - val_loss: 0.1553 - val_mae: 0.4433\n",
            "Epoch 167/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4723\n",
            "Epoch 167: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1740 - mae: 0.4714 - val_loss: 0.1552 - val_mae: 0.4430\n",
            "Epoch 168/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4713\n",
            "Epoch 168: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4719 - val_loss: 0.1567 - val_mae: 0.4450\n",
            "Epoch 169/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1758 - mae: 0.4741\n",
            "Epoch 169: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4722 - val_loss: 0.1548 - val_mae: 0.4412\n",
            "Epoch 170/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1742 - mae: 0.4717\n",
            "Epoch 170: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1742 - mae: 0.4717 - val_loss: 0.1546 - val_mae: 0.4417\n",
            "Epoch 171/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1722 - mae: 0.4691\n",
            "Epoch 171: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4718 - val_loss: 0.1566 - val_mae: 0.4447\n",
            "Epoch 172/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4723\n",
            "Epoch 172: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4713 - val_loss: 0.1561 - val_mae: 0.4436\n",
            "Epoch 173/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 173: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1743 - mae: 0.4723 - val_loss: 0.1561 - val_mae: 0.4438\n",
            "Epoch 174/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4730\n",
            "Epoch 174: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1744 - mae: 0.4723 - val_loss: 0.1562 - val_mae: 0.4443\n",
            "Epoch 175/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4733\n",
            "Epoch 175: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4718 - val_loss: 0.1546 - val_mae: 0.4420\n",
            "Epoch 176/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1724 - mae: 0.4682\n",
            "Epoch 176: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1742 - mae: 0.4716 - val_loss: 0.1551 - val_mae: 0.4422\n",
            "Epoch 177/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1753 - mae: 0.4741\n",
            "Epoch 177: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4724 - val_loss: 0.1554 - val_mae: 0.4425\n",
            "Epoch 178/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4728\n",
            "Epoch 178: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1744 - mae: 0.4722 - val_loss: 0.1549 - val_mae: 0.4424\n",
            "Epoch 179/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4718\n",
            "Epoch 179: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1738 - mae: 0.4716 - val_loss: 0.1546 - val_mae: 0.4415\n",
            "Epoch 180/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4744\n",
            "Epoch 180: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1744 - mae: 0.4720 - val_loss: 0.1565 - val_mae: 0.4452\n",
            "Epoch 181/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1755 - mae: 0.4740\n",
            "Epoch 181: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1740 - mae: 0.4717 - val_loss: 0.1557 - val_mae: 0.4427\n",
            "Epoch 182/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4730\n",
            "Epoch 182: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1744 - mae: 0.4723 - val_loss: 0.1559 - val_mae: 0.4433\n",
            "Epoch 183/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1721 - mae: 0.4676\n",
            "Epoch 183: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1742 - mae: 0.4716 - val_loss: 0.1553 - val_mae: 0.4437\n",
            "Epoch 184/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4708\n",
            "Epoch 184: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4723 - val_loss: 0.1550 - val_mae: 0.4426\n",
            "Epoch 185/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4733\n",
            "Epoch 185: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4714 - val_loss: 0.1562 - val_mae: 0.4439\n",
            "Epoch 186/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4734\n",
            "Epoch 186: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4720 - val_loss: 0.1558 - val_mae: 0.4431\n",
            "Epoch 187/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1744 - mae: 0.4723\n",
            "Epoch 187: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1744 - mae: 0.4723 - val_loss: 0.1565 - val_mae: 0.4443\n",
            "Epoch 188/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4729\n",
            "Epoch 188: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4714 - val_loss: 0.1564 - val_mae: 0.4441\n",
            "Epoch 189/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1739 - mae: 0.4715\n",
            "Epoch 189: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4715 - val_loss: 0.1545 - val_mae: 0.4419\n",
            "Epoch 190/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1738 - mae: 0.4714\n",
            "Epoch 190: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4714 - val_loss: 0.1559 - val_mae: 0.4432\n",
            "Epoch 191/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1738 - mae: 0.4717\n",
            "Epoch 191: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4715 - val_loss: 0.1558 - val_mae: 0.4431\n",
            "Epoch 192/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4712\n",
            "Epoch 192: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4712 - val_loss: 0.1558 - val_mae: 0.4427\n",
            "Epoch 193/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1738 - mae: 0.4715\n",
            "Epoch 193: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4720 - val_loss: 0.1552 - val_mae: 0.4421\n",
            "Epoch 194/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4720\n",
            "Epoch 194: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1736 - mae: 0.4711 - val_loss: 0.1549 - val_mae: 0.4418\n",
            "Epoch 195/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4695\n",
            "Epoch 195: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1742 - mae: 0.4715 - val_loss: 0.1560 - val_mae: 0.4429\n",
            "Epoch 196/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1722 - mae: 0.4683\n",
            "Epoch 196: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4711 - val_loss: 0.1570 - val_mae: 0.4458\n",
            "Epoch 197/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1754 - mae: 0.4745\n",
            "Epoch 197: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4715 - val_loss: 0.1566 - val_mae: 0.4446\n",
            "Epoch 198/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 198: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1739 - mae: 0.4716 - val_loss: 0.1556 - val_mae: 0.4430\n",
            "Epoch 199/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1742 - mae: 0.4718\n",
            "Epoch 199: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1742 - mae: 0.4718 - val_loss: 0.1565 - val_mae: 0.4452\n",
            "Epoch 200/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1722 - mae: 0.4686\n",
            "Epoch 200: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4718 - val_loss: 0.1558 - val_mae: 0.4433\n",
            "Epoch 201/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4715\n",
            "Epoch 201: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4717 - val_loss: 0.1555 - val_mae: 0.4426\n",
            "Epoch 202/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1754 - mae: 0.4737\n",
            "Epoch 202: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1741 - mae: 0.4717 - val_loss: 0.1552 - val_mae: 0.4422\n",
            "Epoch 203/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1742 - mae: 0.4714\n",
            "Epoch 203: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4708 - val_loss: 0.1556 - val_mae: 0.4427\n",
            "Epoch 204/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1738 - mae: 0.4720\n",
            "Epoch 204: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4724 - val_loss: 0.1553 - val_mae: 0.4416\n",
            "Epoch 205/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1742 - mae: 0.4712\n",
            "Epoch 205: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1742 - mae: 0.4712 - val_loss: 0.1552 - val_mae: 0.4433\n",
            "Epoch 206/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4729\n",
            "Epoch 206: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4722 - val_loss: 0.1549 - val_mae: 0.4421\n",
            "Epoch 207/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1720 - mae: 0.4680\n",
            "Epoch 207: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4713 - val_loss: 0.1569 - val_mae: 0.4450\n",
            "Epoch 208/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4723\n",
            "Epoch 208: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4723 - val_loss: 0.1550 - val_mae: 0.4421\n",
            "Epoch 209/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1746 - mae: 0.4714\n",
            "Epoch 209: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4716 - val_loss: 0.1555 - val_mae: 0.4428\n",
            "Epoch 210/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1739 - mae: 0.4712\n",
            "Epoch 210: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4712 - val_loss: 0.1549 - val_mae: 0.4422\n",
            "Epoch 211/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4728\n",
            "Epoch 211: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4711 - val_loss: 0.1564 - val_mae: 0.4439\n",
            "Epoch 212/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4705\n",
            "Epoch 212: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1739 - mae: 0.4713 - val_loss: 0.1559 - val_mae: 0.4434\n",
            "Epoch 213/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4722\n",
            "Epoch 213: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1736 - mae: 0.4716 - val_loss: 0.1552 - val_mae: 0.4421\n",
            "Epoch 214/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4727\n",
            "Epoch 214: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4712 - val_loss: 0.1565 - val_mae: 0.4449\n",
            "Epoch 215/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4727\n",
            "Epoch 215: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4719 - val_loss: 0.1559 - val_mae: 0.4434\n",
            "Epoch 216/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4725\n",
            "Epoch 216: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1740 - mae: 0.4714 - val_loss: 0.1557 - val_mae: 0.4435\n",
            "Epoch 217/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4705\n",
            "Epoch 217: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4714 - val_loss: 0.1558 - val_mae: 0.4433\n",
            "Epoch 218/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1719 - mae: 0.4688\n",
            "Epoch 218: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4718 - val_loss: 0.1561 - val_mae: 0.4442\n",
            "Epoch 219/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4725\n",
            "Epoch 219: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1742 - mae: 0.4720 - val_loss: 0.1561 - val_mae: 0.4440\n",
            "Epoch 220/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4720\n",
            "Epoch 220: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4717 - val_loss: 0.1559 - val_mae: 0.4432\n",
            "Epoch 221/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1741 - mae: 0.4713\n",
            "Epoch 221: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1741 - mae: 0.4713 - val_loss: 0.1555 - val_mae: 0.4428\n",
            "Epoch 222/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1743 - mae: 0.4720\n",
            "Epoch 222: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4722 - val_loss: 0.1556 - val_mae: 0.4423\n",
            "Epoch 223/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1750 - mae: 0.4727\n",
            "Epoch 223: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4709 - val_loss: 0.1564 - val_mae: 0.4450\n",
            "Epoch 224/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4704\n",
            "Epoch 224: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4718 - val_loss: 0.1537 - val_mae: 0.4410\n",
            "Epoch 225/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4725\n",
            "Epoch 225: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1743 - mae: 0.4727 - val_loss: 0.1556 - val_mae: 0.4434\n",
            "Epoch 226/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4708\n",
            "Epoch 226: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1741 - mae: 0.4714 - val_loss: 0.1553 - val_mae: 0.4418\n",
            "Epoch 227/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1722 - mae: 0.4685\n",
            "Epoch 227: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4708 - val_loss: 0.1561 - val_mae: 0.4441\n",
            "Epoch 228/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4718\n",
            "Epoch 228: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4717 - val_loss: 0.1558 - val_mae: 0.4436\n",
            "Epoch 229/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4716\n",
            "Epoch 229: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4716 - val_loss: 0.1549 - val_mae: 0.4425\n",
            "Epoch 230/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4720\n",
            "Epoch 230: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4719 - val_loss: 0.1563 - val_mae: 0.4437\n",
            "Epoch 231/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1733 - mae: 0.4715\n",
            "Epoch 231: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1735 - mae: 0.4711 - val_loss: 0.1545 - val_mae: 0.4415\n",
            "Epoch 232/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4727\n",
            "Epoch 232: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4716 - val_loss: 0.1554 - val_mae: 0.4420\n",
            "Epoch 233/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1764 - mae: 0.4751\n",
            "Epoch 233: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4710 - val_loss: 0.1545 - val_mae: 0.4414\n",
            "Epoch 234/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.4695\n",
            "Epoch 234: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4714 - val_loss: 0.1553 - val_mae: 0.4426\n",
            "Epoch 235/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4712\n",
            "Epoch 235: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4715 - val_loss: 0.1561 - val_mae: 0.4436\n",
            "Epoch 236/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1747 - mae: 0.4727\n",
            "Epoch 236: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.1737 - mae: 0.4711 - val_loss: 0.1566 - val_mae: 0.4445\n",
            "Epoch 237/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4732\n",
            "Epoch 237: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1738 - mae: 0.4710 - val_loss: 0.1563 - val_mae: 0.4439\n",
            "Epoch 238/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1730 - mae: 0.4703\n",
            "Epoch 238: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1741 - mae: 0.4716 - val_loss: 0.1559 - val_mae: 0.4435\n",
            "Epoch 239/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1739 - mae: 0.4714\n",
            "Epoch 239: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4714 - val_loss: 0.1555 - val_mae: 0.4433\n",
            "Epoch 240/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4729\n",
            "Epoch 240: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4716 - val_loss: 0.1554 - val_mae: 0.4431\n",
            "Epoch 241/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1739 - mae: 0.4716\n",
            "Epoch 241: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4716 - val_loss: 0.1529 - val_mae: 0.4399\n",
            "Epoch 242/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4703\n",
            "Epoch 242: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4713 - val_loss: 0.1542 - val_mae: 0.4409\n",
            "Epoch 243/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1739 - mae: 0.4716\n",
            "Epoch 243: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1739 - mae: 0.4716 - val_loss: 0.1566 - val_mae: 0.4443\n",
            "Epoch 244/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1738 - mae: 0.4712\n",
            "Epoch 244: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4712 - val_loss: 0.1562 - val_mae: 0.4437\n",
            "Epoch 245/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1737 - mae: 0.4709\n",
            "Epoch 245: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4717 - val_loss: 0.1529 - val_mae: 0.4388\n",
            "Epoch 246/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4719\n",
            "Epoch 246: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4715 - val_loss: 0.1550 - val_mae: 0.4424\n",
            "Epoch 247/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4701\n",
            "Epoch 247: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4709 - val_loss: 0.1561 - val_mae: 0.4435\n",
            "Epoch 248/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1741 - mae: 0.4720\n",
            "Epoch 248: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4720 - val_loss: 0.1556 - val_mae: 0.4433\n",
            "Epoch 249/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1740 - mae: 0.4715\n",
            "Epoch 249: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4715 - val_loss: 0.1555 - val_mae: 0.4422\n",
            "Epoch 250/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4701\n",
            "Epoch 250: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4707 - val_loss: 0.1547 - val_mae: 0.4410\n",
            "Epoch 251/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1724 - mae: 0.4700\n",
            "Epoch 251: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1736 - mae: 0.4714 - val_loss: 0.1548 - val_mae: 0.4419\n",
            "Epoch 252/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4721\n",
            "Epoch 252: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1741 - mae: 0.4715 - val_loss: 0.1558 - val_mae: 0.4431\n",
            "Epoch 253/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1717 - mae: 0.4687\n",
            "Epoch 253: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1733 - mae: 0.4710 - val_loss: 0.1558 - val_mae: 0.4435\n",
            "Epoch 254/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4713\n",
            "Epoch 254: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1739 - mae: 0.4713 - val_loss: 0.1570 - val_mae: 0.4455\n",
            "Epoch 255/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1738 - mae: 0.4705\n",
            "Epoch 255: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4712 - val_loss: 0.1563 - val_mae: 0.4444\n",
            "Epoch 256/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1734 - mae: 0.4706\n",
            "Epoch 256: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1741 - mae: 0.4721 - val_loss: 0.1564 - val_mae: 0.4444\n",
            "Epoch 257/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4710\n",
            "Epoch 257: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1733 - mae: 0.4704 - val_loss: 0.1548 - val_mae: 0.4413\n",
            "Epoch 258/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1737 - mae: 0.4710\n",
            "Epoch 258: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1535 - val_mae: 0.4399\n",
            "Epoch 259/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1730 - mae: 0.4704\n",
            "Epoch 259: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4708 - val_loss: 0.1561 - val_mae: 0.4439\n",
            "Epoch 260/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4712\n",
            "Epoch 260: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4710 - val_loss: 0.1558 - val_mae: 0.4433\n",
            "Epoch 261/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4723\n",
            "Epoch 261: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4714 - val_loss: 0.1558 - val_mae: 0.4432\n",
            "Epoch 262/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4710\n",
            "Epoch 262: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4712 - val_loss: 0.1555 - val_mae: 0.4428\n",
            "Epoch 263/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1740 - mae: 0.4716\n",
            "Epoch 263: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4716 - val_loss: 0.1548 - val_mae: 0.4422\n",
            "Epoch 264/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4685\n",
            "Epoch 264: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1733 - mae: 0.4707 - val_loss: 0.1548 - val_mae: 0.4415\n",
            "Epoch 265/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4702\n",
            "Epoch 265: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4711 - val_loss: 0.1548 - val_mae: 0.4419\n",
            "Epoch 266/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1757 - mae: 0.4742\n",
            "Epoch 266: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4718 - val_loss: 0.1542 - val_mae: 0.4409\n",
            "Epoch 267/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1738 - mae: 0.4703\n",
            "Epoch 267: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4714 - val_loss: 0.1540 - val_mae: 0.4402\n",
            "Epoch 268/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4718\n",
            "Epoch 268: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1740 - mae: 0.4716 - val_loss: 0.1550 - val_mae: 0.4425\n",
            "Epoch 269/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1755 - mae: 0.4738\n",
            "Epoch 269: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4718 - val_loss: 0.1560 - val_mae: 0.4436\n",
            "Epoch 270/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4726\n",
            "Epoch 270: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4714 - val_loss: 0.1555 - val_mae: 0.4432\n",
            "Epoch 271/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4708\n",
            "Epoch 271: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4715 - val_loss: 0.1557 - val_mae: 0.4438\n",
            "Epoch 272/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 272: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4716 - val_loss: 0.1564 - val_mae: 0.4440\n",
            "Epoch 273/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4715\n",
            "Epoch 273: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4706 - val_loss: 0.1544 - val_mae: 0.4413\n",
            "Epoch 274/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1761 - mae: 0.4742\n",
            "Epoch 274: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4715 - val_loss: 0.1553 - val_mae: 0.4427\n",
            "Epoch 275/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.4708\n",
            "Epoch 275: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4714 - val_loss: 0.1559 - val_mae: 0.4436\n",
            "Epoch 276/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4710\n",
            "Epoch 276: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4710 - val_loss: 0.1553 - val_mae: 0.4419\n",
            "Epoch 277/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4711\n",
            "Epoch 277: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1739 - mae: 0.4710 - val_loss: 0.1550 - val_mae: 0.4425\n",
            "Epoch 278/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4704\n",
            "Epoch 278: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4712 - val_loss: 0.1550 - val_mae: 0.4422\n",
            "Epoch 279/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4710\n",
            "Epoch 279: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4709 - val_loss: 0.1558 - val_mae: 0.4435\n",
            "Epoch 280/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1755 - mae: 0.4735\n",
            "Epoch 280: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4718 - val_loss: 0.1555 - val_mae: 0.4431\n",
            "Epoch 281/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1750 - mae: 0.4728\n",
            "Epoch 281: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4706 - val_loss: 0.1551 - val_mae: 0.4428\n",
            "Epoch 282/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.4712\n",
            "Epoch 282: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1735 - mae: 0.4711 - val_loss: 0.1563 - val_mae: 0.4445\n",
            "Epoch 283/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4719\n",
            "Epoch 283: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4708 - val_loss: 0.1562 - val_mae: 0.4433\n",
            "Epoch 284/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4684\n",
            "Epoch 284: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4716 - val_loss: 0.1545 - val_mae: 0.4418\n",
            "Epoch 285/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4697\n",
            "Epoch 285: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4710 - val_loss: 0.1566 - val_mae: 0.4449\n",
            "Epoch 286/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1734 - mae: 0.4701\n",
            "Epoch 286: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1741 - mae: 0.4718 - val_loss: 0.1562 - val_mae: 0.4436\n",
            "Epoch 287/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1732 - mae: 0.4703\n",
            "Epoch 287: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1732 - mae: 0.4703 - val_loss: 0.1566 - val_mae: 0.4442\n",
            "Epoch 288/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4722\n",
            "Epoch 288: loss did not improve from 0.17308\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4714 - val_loss: 0.1561 - val_mae: 0.4435\n",
            "Epoch 289/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1725 - mae: 0.4687\n",
            "Epoch 289: loss improved from 0.17308 to 0.17278, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1728 - mae: 0.4697 - val_loss: 0.1558 - val_mae: 0.4430\n",
            "Epoch 290/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1758 - mae: 0.4741\n",
            "Epoch 290: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4708 - val_loss: 0.1545 - val_mae: 0.4413\n",
            "Epoch 291/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1738 - mae: 0.4714\n",
            "Epoch 291: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4714 - val_loss: 0.1558 - val_mae: 0.4430\n",
            "Epoch 292/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4711\n",
            "Epoch 292: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4713 - val_loss: 0.1559 - val_mae: 0.4431\n",
            "Epoch 293/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4705\n",
            "Epoch 293: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1740 - mae: 0.4715 - val_loss: 0.1559 - val_mae: 0.4433\n",
            "Epoch 294/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1715 - mae: 0.4691\n",
            "Epoch 294: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1732 - mae: 0.4707 - val_loss: 0.1561 - val_mae: 0.4435\n",
            "Epoch 295/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4707\n",
            "Epoch 295: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4714 - val_loss: 0.1562 - val_mae: 0.4438\n",
            "Epoch 296/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4697\n",
            "Epoch 296: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4705 - val_loss: 0.1564 - val_mae: 0.4447\n",
            "Epoch 297/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1727 - mae: 0.4695\n",
            "Epoch 297: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4709 - val_loss: 0.1555 - val_mae: 0.4431\n",
            "Epoch 298/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1748 - mae: 0.4732\n",
            "Epoch 298: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4703 - val_loss: 0.1544 - val_mae: 0.4412\n",
            "Epoch 299/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1722 - mae: 0.4688\n",
            "Epoch 299: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1731 - mae: 0.4703 - val_loss: 0.1533 - val_mae: 0.4394\n",
            "Epoch 300/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4702\n",
            "Epoch 300: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4713 - val_loss: 0.1559 - val_mae: 0.4438\n",
            "Epoch 301/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4711\n",
            "Epoch 301: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1734 - mae: 0.4703 - val_loss: 0.1551 - val_mae: 0.4427\n",
            "Epoch 302/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1737 - mae: 0.4712\n",
            "Epoch 302: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4712 - val_loss: 0.1551 - val_mae: 0.4422\n",
            "Epoch 303/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.4705\n",
            "Epoch 303: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4708 - val_loss: 0.1558 - val_mae: 0.4435\n",
            "Epoch 304/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4710\n",
            "Epoch 304: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4714 - val_loss: 0.1559 - val_mae: 0.4437\n",
            "Epoch 305/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4709\n",
            "Epoch 305: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4714 - val_loss: 0.1562 - val_mae: 0.4439\n",
            "Epoch 306/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 306: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1741 - mae: 0.4714 - val_loss: 0.1554 - val_mae: 0.4429\n",
            "Epoch 307/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4695\n",
            "Epoch 307: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4707 - val_loss: 0.1557 - val_mae: 0.4433\n",
            "Epoch 308/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4727\n",
            "Epoch 308: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4716 - val_loss: 0.1564 - val_mae: 0.4445\n",
            "Epoch 309/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4723\n",
            "Epoch 309: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4716 - val_loss: 0.1561 - val_mae: 0.4437\n",
            "Epoch 310/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4740\n",
            "Epoch 310: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1742 - mae: 0.4714 - val_loss: 0.1564 - val_mae: 0.4442\n",
            "Epoch 311/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1717 - mae: 0.4680\n",
            "Epoch 311: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1731 - mae: 0.4700 - val_loss: 0.1541 - val_mae: 0.4420\n",
            "Epoch 312/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.4706\n",
            "Epoch 312: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4714 - val_loss: 0.1554 - val_mae: 0.4425\n",
            "Epoch 313/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4722\n",
            "Epoch 313: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4714 - val_loss: 0.1556 - val_mae: 0.4433\n",
            "Epoch 314/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4700\n",
            "Epoch 314: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4701 - val_loss: 0.1550 - val_mae: 0.4421\n",
            "Epoch 315/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4703\n",
            "Epoch 315: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1740 - mae: 0.4721 - val_loss: 0.1551 - val_mae: 0.4428\n",
            "Epoch 316/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4728\n",
            "Epoch 316: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4714 - val_loss: 0.1558 - val_mae: 0.4438\n",
            "Epoch 317/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4698\n",
            "Epoch 317: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1736 - mae: 0.4706 - val_loss: 0.1551 - val_mae: 0.4421\n",
            "Epoch 318/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4729\n",
            "Epoch 318: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1546 - val_mae: 0.4410\n",
            "Epoch 319/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4714\n",
            "Epoch 319: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4715 - val_loss: 0.1561 - val_mae: 0.4436\n",
            "Epoch 320/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4720\n",
            "Epoch 320: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4709 - val_loss: 0.1567 - val_mae: 0.4441\n",
            "Epoch 321/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1719 - mae: 0.4664\n",
            "Epoch 321: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4700 - val_loss: 0.1562 - val_mae: 0.4444\n",
            "Epoch 322/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4710\n",
            "Epoch 322: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4710 - val_loss: 0.1551 - val_mae: 0.4421\n",
            "Epoch 323/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4702\n",
            "Epoch 323: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4702 - val_loss: 0.1554 - val_mae: 0.4427\n",
            "Epoch 324/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1708 - mae: 0.4674\n",
            "Epoch 324: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4715 - val_loss: 0.1564 - val_mae: 0.4443\n",
            "Epoch 325/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4702\n",
            "Epoch 325: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4708 - val_loss: 0.1565 - val_mae: 0.4448\n",
            "Epoch 326/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4720\n",
            "Epoch 326: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1732 - mae: 0.4705 - val_loss: 0.1558 - val_mae: 0.4438\n",
            "Epoch 327/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4699\n",
            "Epoch 327: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4712 - val_loss: 0.1553 - val_mae: 0.4423\n",
            "Epoch 328/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4692\n",
            "Epoch 328: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1733 - mae: 0.4701 - val_loss: 0.1562 - val_mae: 0.4442\n",
            "Epoch 329/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4702\n",
            "Epoch 329: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4713 - val_loss: 0.1564 - val_mae: 0.4446\n",
            "Epoch 330/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1740 - mae: 0.4715\n",
            "Epoch 330: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1740 - mae: 0.4715 - val_loss: 0.1552 - val_mae: 0.4423\n",
            "Epoch 331/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1721 - mae: 0.4681\n",
            "Epoch 331: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1733 - mae: 0.4703 - val_loss: 0.1560 - val_mae: 0.4441\n",
            "Epoch 332/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4711\n",
            "Epoch 332: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1568 - val_mae: 0.4450\n",
            "Epoch 333/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4727\n",
            "Epoch 333: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1732 - mae: 0.4700 - val_loss: 0.1558 - val_mae: 0.4435\n",
            "Epoch 334/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1738 - mae: 0.4715\n",
            "Epoch 334: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4706 - val_loss: 0.1542 - val_mae: 0.4418\n",
            "Epoch 335/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4717\n",
            "Epoch 335: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1736 - mae: 0.4708 - val_loss: 0.1559 - val_mae: 0.4437\n",
            "Epoch 336/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1701 - mae: 0.4656\n",
            "Epoch 336: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4706 - val_loss: 0.1565 - val_mae: 0.4445\n",
            "Epoch 337/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1757 - mae: 0.4735\n",
            "Epoch 337: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4713 - val_loss: 0.1545 - val_mae: 0.4417\n",
            "Epoch 338/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4718\n",
            "Epoch 338: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4711 - val_loss: 0.1561 - val_mae: 0.4437\n",
            "Epoch 339/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4703\n",
            "Epoch 339: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4712 - val_loss: 0.1560 - val_mae: 0.4436\n",
            "Epoch 340/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4702\n",
            "Epoch 340: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1739 - mae: 0.4715 - val_loss: 0.1548 - val_mae: 0.4428\n",
            "Epoch 341/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4705\n",
            "Epoch 341: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1735 - mae: 0.4707 - val_loss: 0.1551 - val_mae: 0.4423\n",
            "Epoch 342/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4692\n",
            "Epoch 342: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4711 - val_loss: 0.1557 - val_mae: 0.4427\n",
            "Epoch 343/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4713\n",
            "Epoch 343: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4705 - val_loss: 0.1562 - val_mae: 0.4446\n",
            "Epoch 344/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1739 - mae: 0.4713\n",
            "Epoch 344: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1739 - mae: 0.4713 - val_loss: 0.1546 - val_mae: 0.4415\n",
            "Epoch 345/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4710\n",
            "Epoch 345: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4710 - val_loss: 0.1553 - val_mae: 0.4428\n",
            "Epoch 346/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4727\n",
            "Epoch 346: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4712 - val_loss: 0.1549 - val_mae: 0.4420\n",
            "Epoch 347/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4714\n",
            "Epoch 347: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4709 - val_loss: 0.1566 - val_mae: 0.4446\n",
            "Epoch 348/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1773 - mae: 0.4769\n",
            "Epoch 348: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1739 - mae: 0.4715 - val_loss: 0.1550 - val_mae: 0.4415\n",
            "Epoch 349/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4715\n",
            "Epoch 349: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4707 - val_loss: 0.1553 - val_mae: 0.4428\n",
            "Epoch 350/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.4708\n",
            "Epoch 350: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1738 - mae: 0.4715 - val_loss: 0.1560 - val_mae: 0.4436\n",
            "Epoch 351/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4714\n",
            "Epoch 351: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4709 - val_loss: 0.1553 - val_mae: 0.4420\n",
            "Epoch 352/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1746 - mae: 0.4729\n",
            "Epoch 352: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1730 - mae: 0.4702 - val_loss: 0.1560 - val_mae: 0.4433\n",
            "Epoch 353/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1723 - mae: 0.4689\n",
            "Epoch 353: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1735 - mae: 0.4705 - val_loss: 0.1563 - val_mae: 0.4439\n",
            "Epoch 354/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4727\n",
            "Epoch 354: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1739 - mae: 0.4716 - val_loss: 0.1546 - val_mae: 0.4427\n",
            "Epoch 355/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1742 - mae: 0.4714\n",
            "Epoch 355: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4707 - val_loss: 0.1553 - val_mae: 0.4432\n",
            "Epoch 356/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4707\n",
            "Epoch 356: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4709 - val_loss: 0.1550 - val_mae: 0.4426\n",
            "Epoch 357/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1737 - mae: 0.4712\n",
            "Epoch 357: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4712 - val_loss: 0.1557 - val_mae: 0.4431\n",
            "Epoch 358/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4708\n",
            "Epoch 358: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4711 - val_loss: 0.1546 - val_mae: 0.4418\n",
            "Epoch 359/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4708\n",
            "Epoch 359: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4708 - val_loss: 0.1559 - val_mae: 0.4434\n",
            "Epoch 360/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1737 - mae: 0.4709\n",
            "Epoch 360: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1737 - mae: 0.4709 - val_loss: 0.1554 - val_mae: 0.4421\n",
            "Epoch 361/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4710\n",
            "Epoch 361: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1547 - val_mae: 0.4415\n",
            "Epoch 362/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4710\n",
            "Epoch 362: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4706 - val_loss: 0.1553 - val_mae: 0.4420\n",
            "Epoch 363/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4712\n",
            "Epoch 363: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1560 - val_mae: 0.4437\n",
            "Epoch 364/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4725\n",
            "Epoch 364: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4712 - val_loss: 0.1560 - val_mae: 0.4442\n",
            "Epoch 365/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4702\n",
            "Epoch 365: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1736 - mae: 0.4709 - val_loss: 0.1558 - val_mae: 0.4435\n",
            "Epoch 366/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1730 - mae: 0.4701\n",
            "Epoch 366: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1730 - mae: 0.4701 - val_loss: 0.1554 - val_mae: 0.4428\n",
            "Epoch 367/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4700\n",
            "Epoch 367: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4709 - val_loss: 0.1555 - val_mae: 0.4428\n",
            "Epoch 368/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4719\n",
            "Epoch 368: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4708 - val_loss: 0.1554 - val_mae: 0.4422\n",
            "Epoch 369/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4714\n",
            "Epoch 369: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1731 - mae: 0.4704 - val_loss: 0.1559 - val_mae: 0.4433\n",
            "Epoch 370/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1713 - mae: 0.4675\n",
            "Epoch 370: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1736 - mae: 0.4709 - val_loss: 0.1564 - val_mae: 0.4440\n",
            "Epoch 371/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4702\n",
            "Epoch 371: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4702 - val_loss: 0.1555 - val_mae: 0.4427\n",
            "Epoch 372/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1700 - mae: 0.4657\n",
            "Epoch 372: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1731 - mae: 0.4705 - val_loss: 0.1539 - val_mae: 0.4405\n",
            "Epoch 373/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4720\n",
            "Epoch 373: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4707 - val_loss: 0.1548 - val_mae: 0.4415\n",
            "Epoch 374/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1706 - mae: 0.4671\n",
            "Epoch 374: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4709 - val_loss: 0.1554 - val_mae: 0.4425\n",
            "Epoch 375/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4695\n",
            "Epoch 375: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1736 - mae: 0.4709 - val_loss: 0.1548 - val_mae: 0.4422\n",
            "Epoch 376/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1742 - mae: 0.4719\n",
            "Epoch 376: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4705 - val_loss: 0.1538 - val_mae: 0.4402\n",
            "Epoch 377/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4700\n",
            "Epoch 377: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4708 - val_loss: 0.1556 - val_mae: 0.4428\n",
            "Epoch 378/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4706\n",
            "Epoch 378: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1734 - mae: 0.4706 - val_loss: 0.1543 - val_mae: 0.4415\n",
            "Epoch 379/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1733 - mae: 0.4705\n",
            "Epoch 379: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1733 - mae: 0.4705 - val_loss: 0.1561 - val_mae: 0.4439\n",
            "Epoch 380/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4702\n",
            "Epoch 380: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1738 - mae: 0.4708 - val_loss: 0.1544 - val_mae: 0.4408\n",
            "Epoch 381/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4713\n",
            "Epoch 381: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1738 - mae: 0.4714 - val_loss: 0.1557 - val_mae: 0.4423\n",
            "Epoch 382/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1766 - mae: 0.4748\n",
            "Epoch 382: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4708 - val_loss: 0.1542 - val_mae: 0.4408\n",
            "Epoch 383/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1739 - mae: 0.4717\n",
            "Epoch 383: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1738 - mae: 0.4715 - val_loss: 0.1558 - val_mae: 0.4436\n",
            "Epoch 384/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4723\n",
            "Epoch 384: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4707 - val_loss: 0.1552 - val_mae: 0.4424\n",
            "Epoch 385/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1682 - mae: 0.4646\n",
            "Epoch 385: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1732 - mae: 0.4705 - val_loss: 0.1558 - val_mae: 0.4431\n",
            "Epoch 386/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4714\n",
            "Epoch 386: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4704 - val_loss: 0.1539 - val_mae: 0.4401\n",
            "Epoch 387/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1728 - mae: 0.4698\n",
            "Epoch 387: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1735 - mae: 0.4704 - val_loss: 0.1557 - val_mae: 0.4429\n",
            "Epoch 388/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4687\n",
            "Epoch 388: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1734 - mae: 0.4703 - val_loss: 0.1551 - val_mae: 0.4425\n",
            "Epoch 389/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1754 - mae: 0.4739\n",
            "Epoch 389: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1736 - mae: 0.4711 - val_loss: 0.1547 - val_mae: 0.4413\n",
            "Epoch 390/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4711\n",
            "Epoch 390: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1736 - mae: 0.4707 - val_loss: 0.1553 - val_mae: 0.4422\n",
            "Epoch 391/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4715\n",
            "Epoch 391: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4711 - val_loss: 0.1548 - val_mae: 0.4423\n",
            "Epoch 392/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1740 - mae: 0.4708\n",
            "Epoch 392: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4714 - val_loss: 0.1555 - val_mae: 0.4427\n",
            "Epoch 393/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1739 - mae: 0.4714\n",
            "Epoch 393: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1739 - mae: 0.4714 - val_loss: 0.1555 - val_mae: 0.4425\n",
            "Epoch 394/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4733\n",
            "Epoch 394: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1733 - mae: 0.4704 - val_loss: 0.1536 - val_mae: 0.4401\n",
            "Epoch 395/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4709\n",
            "Epoch 395: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1729 - mae: 0.4695 - val_loss: 0.1547 - val_mae: 0.4414\n",
            "Epoch 396/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4707\n",
            "Epoch 396: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1732 - mae: 0.4704 - val_loss: 0.1553 - val_mae: 0.4421\n",
            "Epoch 397/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4712\n",
            "Epoch 397: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4705 - val_loss: 0.1549 - val_mae: 0.4422\n",
            "Epoch 398/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4713\n",
            "Epoch 398: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4709 - val_loss: 0.1553 - val_mae: 0.4424\n",
            "Epoch 399/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1730 - mae: 0.4695\n",
            "Epoch 399: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1738 - mae: 0.4711 - val_loss: 0.1554 - val_mae: 0.4423\n",
            "Epoch 400/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1736 - mae: 0.4704\n",
            "Epoch 400: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4704 - val_loss: 0.1569 - val_mae: 0.4452\n",
            "Epoch 401/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1738 - mae: 0.4714\n",
            "Epoch 401: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4714 - val_loss: 0.1552 - val_mae: 0.4431\n",
            "Epoch 402/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4714\n",
            "Epoch 402: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4709 - val_loss: 0.1556 - val_mae: 0.4428\n",
            "Epoch 403/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1755 - mae: 0.4733\n",
            "Epoch 403: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1734 - mae: 0.4707 - val_loss: 0.1546 - val_mae: 0.4411\n",
            "Epoch 404/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4688\n",
            "Epoch 404: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4707 - val_loss: 0.1552 - val_mae: 0.4424\n",
            "Epoch 405/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.4715\n",
            "Epoch 405: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1737 - mae: 0.4712 - val_loss: 0.1555 - val_mae: 0.4429\n",
            "Epoch 406/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4724\n",
            "Epoch 406: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4708 - val_loss: 0.1560 - val_mae: 0.4432\n",
            "Epoch 407/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1726 - mae: 0.4693\n",
            "Epoch 407: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4709 - val_loss: 0.1557 - val_mae: 0.4433\n",
            "Epoch 408/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1739 - mae: 0.4696\n",
            "Epoch 408: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1733 - mae: 0.4703 - val_loss: 0.1550 - val_mae: 0.4421\n",
            "Epoch 409/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4731\n",
            "Epoch 409: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4711 - val_loss: 0.1546 - val_mae: 0.4415\n",
            "Epoch 410/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1735 - mae: 0.4705\n",
            "Epoch 410: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4705 - val_loss: 0.1567 - val_mae: 0.4448\n",
            "Epoch 411/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4691\n",
            "Epoch 411: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1733 - mae: 0.4702 - val_loss: 0.1546 - val_mae: 0.4412\n",
            "Epoch 412/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4722\n",
            "Epoch 412: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1735 - mae: 0.4703 - val_loss: 0.1558 - val_mae: 0.4434\n",
            "Epoch 413/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4709\n",
            "Epoch 413: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1735 - mae: 0.4707 - val_loss: 0.1546 - val_mae: 0.4411\n",
            "Epoch 414/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4727\n",
            "Epoch 414: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4711 - val_loss: 0.1548 - val_mae: 0.4422\n",
            "Epoch 415/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4718\n",
            "Epoch 415: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4706 - val_loss: 0.1545 - val_mae: 0.4412\n",
            "Epoch 416/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1709 - mae: 0.4665\n",
            "Epoch 416: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1735 - mae: 0.4704 - val_loss: 0.1546 - val_mae: 0.4419\n",
            "Epoch 417/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4710\n",
            "Epoch 417: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1732 - mae: 0.4702 - val_loss: 0.1550 - val_mae: 0.4424\n",
            "Epoch 418/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1728 - mae: 0.4707\n",
            "Epoch 418: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4710 - val_loss: 0.1549 - val_mae: 0.4419\n",
            "Epoch 419/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1754 - mae: 0.4729\n",
            "Epoch 419: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1732 - mae: 0.4702 - val_loss: 0.1548 - val_mae: 0.4422\n",
            "Epoch 420/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4704\n",
            "Epoch 420: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4705 - val_loss: 0.1558 - val_mae: 0.4429\n",
            "Epoch 421/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4702\n",
            "Epoch 421: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4706 - val_loss: 0.1551 - val_mae: 0.4421\n",
            "Epoch 422/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4705\n",
            "Epoch 422: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1730 - mae: 0.4701 - val_loss: 0.1552 - val_mae: 0.4428\n",
            "Epoch 423/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4686\n",
            "Epoch 423: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4704 - val_loss: 0.1550 - val_mae: 0.4420\n",
            "Epoch 424/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4694\n",
            "Epoch 424: loss did not improve from 0.17278\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4709 - val_loss: 0.1530 - val_mae: 0.4392\n",
            "Epoch 425/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1715 - mae: 0.4679\n",
            "Epoch 425: loss improved from 0.17278 to 0.17249, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1725 - mae: 0.4697 - val_loss: 0.1563 - val_mae: 0.4440\n",
            "Epoch 426/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4727\n",
            "Epoch 426: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1733 - mae: 0.4703 - val_loss: 0.1555 - val_mae: 0.4420\n",
            "Epoch 427/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4731\n",
            "Epoch 427: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4706 - val_loss: 0.1558 - val_mae: 0.4430\n",
            "Epoch 428/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4703\n",
            "Epoch 428: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1730 - mae: 0.4700 - val_loss: 0.1553 - val_mae: 0.4419\n",
            "Epoch 429/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4706\n",
            "Epoch 429: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1736 - mae: 0.4709 - val_loss: 0.1548 - val_mae: 0.4414\n",
            "Epoch 430/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4719\n",
            "Epoch 430: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4708 - val_loss: 0.1554 - val_mae: 0.4421\n",
            "Epoch 431/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4708\n",
            "Epoch 431: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1734 - mae: 0.4705 - val_loss: 0.1559 - val_mae: 0.4436\n",
            "Epoch 432/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4701\n",
            "Epoch 432: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4711 - val_loss: 0.1554 - val_mae: 0.4423\n",
            "Epoch 433/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1730 - mae: 0.4699\n",
            "Epoch 433: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4707 - val_loss: 0.1539 - val_mae: 0.4399\n",
            "Epoch 434/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.4709\n",
            "Epoch 434: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4707 - val_loss: 0.1556 - val_mae: 0.4429\n",
            "Epoch 435/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4704\n",
            "Epoch 435: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1737 - mae: 0.4712 - val_loss: 0.1539 - val_mae: 0.4399\n",
            "Epoch 436/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4705\n",
            "Epoch 436: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4707 - val_loss: 0.1548 - val_mae: 0.4421\n",
            "Epoch 437/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4708\n",
            "Epoch 437: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1545 - val_mae: 0.4416\n",
            "Epoch 438/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1737 - mae: 0.4709\n",
            "Epoch 438: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4709 - val_loss: 0.1557 - val_mae: 0.4431\n",
            "Epoch 439/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4715\n",
            "Epoch 439: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4710 - val_loss: 0.1556 - val_mae: 0.4433\n",
            "Epoch 440/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1732 - mae: 0.4699\n",
            "Epoch 440: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1732 - mae: 0.4699 - val_loss: 0.1563 - val_mae: 0.4443\n",
            "Epoch 441/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1728 - mae: 0.4697\n",
            "Epoch 441: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1728 - mae: 0.4697 - val_loss: 0.1554 - val_mae: 0.4422\n",
            "Epoch 442/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4693\n",
            "Epoch 442: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4704 - val_loss: 0.1553 - val_mae: 0.4421\n",
            "Epoch 443/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4699\n",
            "Epoch 443: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1731 - mae: 0.4699 - val_loss: 0.1552 - val_mae: 0.4421\n",
            "Epoch 444/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4724\n",
            "Epoch 444: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1739 - mae: 0.4718 - val_loss: 0.1534 - val_mae: 0.4392\n",
            "Epoch 445/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4701\n",
            "Epoch 445: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4707 - val_loss: 0.1549 - val_mae: 0.4413\n",
            "Epoch 446/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4735\n",
            "Epoch 446: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1550 - val_mae: 0.4419\n",
            "Epoch 447/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4697\n",
            "Epoch 447: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4705 - val_loss: 0.1548 - val_mae: 0.4419\n",
            "Epoch 448/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4711\n",
            "Epoch 448: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1730 - mae: 0.4701 - val_loss: 0.1563 - val_mae: 0.4439\n",
            "Epoch 449/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1736 - mae: 0.4711\n",
            "Epoch 449: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4711 - val_loss: 0.1554 - val_mae: 0.4424\n",
            "Epoch 450/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4697\n",
            "Epoch 450: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1733 - mae: 0.4702 - val_loss: 0.1557 - val_mae: 0.4429\n",
            "Epoch 451/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4717\n",
            "Epoch 451: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1733 - mae: 0.4702 - val_loss: 0.1562 - val_mae: 0.4440\n",
            "Epoch 452/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4737\n",
            "Epoch 452: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4705 - val_loss: 0.1553 - val_mae: 0.4422\n",
            "Epoch 453/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1767 - mae: 0.4759\n",
            "Epoch 453: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4707 - val_loss: 0.1559 - val_mae: 0.4438\n",
            "Epoch 454/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1747 - mae: 0.4722\n",
            "Epoch 454: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4707 - val_loss: 0.1552 - val_mae: 0.4422\n",
            "Epoch 455/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4692\n",
            "Epoch 455: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4705 - val_loss: 0.1542 - val_mae: 0.4406\n",
            "Epoch 456/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1735 - mae: 0.4708\n",
            "Epoch 456: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4708 - val_loss: 0.1560 - val_mae: 0.4435\n",
            "Epoch 457/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1755 - mae: 0.4731\n",
            "Epoch 457: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4703 - val_loss: 0.1565 - val_mae: 0.4442\n",
            "Epoch 458/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4705\n",
            "Epoch 458: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4705 - val_loss: 0.1551 - val_mae: 0.4422\n",
            "Epoch 459/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4705\n",
            "Epoch 459: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4706 - val_loss: 0.1561 - val_mae: 0.4434\n",
            "Epoch 460/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1719 - mae: 0.4680\n",
            "Epoch 460: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4710 - val_loss: 0.1564 - val_mae: 0.4440\n",
            "Epoch 461/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1726 - mae: 0.4698\n",
            "Epoch 461: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4705 - val_loss: 0.1563 - val_mae: 0.4446\n",
            "Epoch 462/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1735 - mae: 0.4707\n",
            "Epoch 462: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1735 - mae: 0.4707 - val_loss: 0.1546 - val_mae: 0.4416\n",
            "Epoch 463/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1757 - mae: 0.4735\n",
            "Epoch 463: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1738 - mae: 0.4707 - val_loss: 0.1557 - val_mae: 0.4434\n",
            "Epoch 464/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4700\n",
            "Epoch 464: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1737 - mae: 0.4708 - val_loss: 0.1561 - val_mae: 0.4439\n",
            "Epoch 465/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4744\n",
            "Epoch 465: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1736 - mae: 0.4712 - val_loss: 0.1549 - val_mae: 0.4428\n",
            "Epoch 466/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4709\n",
            "Epoch 466: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1734 - mae: 0.4703 - val_loss: 0.1553 - val_mae: 0.4430\n",
            "Epoch 467/500\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.1748 - mae: 0.4732\n",
            "Epoch 467: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1738 - mae: 0.4710 - val_loss: 0.1541 - val_mae: 0.4402\n",
            "Epoch 468/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4711\n",
            "Epoch 468: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4708 - val_loss: 0.1561 - val_mae: 0.4437\n",
            "Epoch 469/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4711\n",
            "Epoch 469: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1736 - mae: 0.4708 - val_loss: 0.1558 - val_mae: 0.4437\n",
            "Epoch 470/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1733 - mae: 0.4705\n",
            "Epoch 470: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1733 - mae: 0.4705 - val_loss: 0.1543 - val_mae: 0.4412\n",
            "Epoch 471/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4710\n",
            "Epoch 471: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4707 - val_loss: 0.1562 - val_mae: 0.4445\n",
            "Epoch 472/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4692\n",
            "Epoch 472: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1731 - mae: 0.4701 - val_loss: 0.1551 - val_mae: 0.4423\n",
            "Epoch 473/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4707\n",
            "Epoch 473: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1735 - mae: 0.4707 - val_loss: 0.1555 - val_mae: 0.4429\n",
            "Epoch 474/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1735 - mae: 0.4704\n",
            "Epoch 474: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1735 - mae: 0.4704 - val_loss: 0.1528 - val_mae: 0.4398\n",
            "Epoch 475/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4705\n",
            "Epoch 475: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1557 - val_mae: 0.4430\n",
            "Epoch 476/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4715\n",
            "Epoch 476: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4708 - val_loss: 0.1561 - val_mae: 0.4435\n",
            "Epoch 477/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4717\n",
            "Epoch 477: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1550 - val_mae: 0.4418\n",
            "Epoch 478/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4703\n",
            "Epoch 478: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1732 - mae: 0.4704 - val_loss: 0.1554 - val_mae: 0.4427\n",
            "Epoch 479/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4742\n",
            "Epoch 479: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4708 - val_loss: 0.1559 - val_mae: 0.4435\n",
            "Epoch 480/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4711\n",
            "Epoch 480: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4704 - val_loss: 0.1561 - val_mae: 0.4443\n",
            "Epoch 481/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4682\n",
            "Epoch 481: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4709 - val_loss: 0.1556 - val_mae: 0.4431\n",
            "Epoch 482/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4715\n",
            "Epoch 482: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4713 - val_loss: 0.1562 - val_mae: 0.4437\n",
            "Epoch 483/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1738 - mae: 0.4711\n",
            "Epoch 483: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1738 - mae: 0.4711 - val_loss: 0.1553 - val_mae: 0.4424\n",
            "Epoch 484/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4704\n",
            "Epoch 484: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1734 - mae: 0.4704 - val_loss: 0.1559 - val_mae: 0.4445\n",
            "Epoch 485/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4709\n",
            "Epoch 485: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1556 - val_mae: 0.4433\n",
            "Epoch 486/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4713\n",
            "Epoch 486: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1732 - mae: 0.4701 - val_loss: 0.1558 - val_mae: 0.4434\n",
            "Epoch 487/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4715\n",
            "Epoch 487: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1733 - mae: 0.4705 - val_loss: 0.1551 - val_mae: 0.4418\n",
            "Epoch 488/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4696\n",
            "Epoch 488: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1733 - mae: 0.4702 - val_loss: 0.1556 - val_mae: 0.4431\n",
            "Epoch 489/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.4734\n",
            "Epoch 489: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1730 - mae: 0.4698 - val_loss: 0.1556 - val_mae: 0.4435\n",
            "Epoch 490/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4695\n",
            "Epoch 490: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4705 - val_loss: 0.1564 - val_mae: 0.4440\n",
            "Epoch 491/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4706\n",
            "Epoch 491: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1734 - mae: 0.4706 - val_loss: 0.1556 - val_mae: 0.4429\n",
            "Epoch 492/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1733 - mae: 0.4704\n",
            "Epoch 492: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1733 - mae: 0.4704 - val_loss: 0.1551 - val_mae: 0.4422\n",
            "Epoch 493/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4724\n",
            "Epoch 493: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4704 - val_loss: 0.1565 - val_mae: 0.4449\n",
            "Epoch 494/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4700\n",
            "Epoch 494: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4702 - val_loss: 0.1560 - val_mae: 0.4437\n",
            "Epoch 495/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1735 - mae: 0.4708\n",
            "Epoch 495: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1735 - mae: 0.4708 - val_loss: 0.1544 - val_mae: 0.4411\n",
            "Epoch 496/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.4724\n",
            "Epoch 496: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4705 - val_loss: 0.1561 - val_mae: 0.4444\n",
            "Epoch 497/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4722\n",
            "Epoch 497: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1734 - mae: 0.4707 - val_loss: 0.1544 - val_mae: 0.4417\n",
            "Epoch 498/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1726 - mae: 0.4696\n",
            "Epoch 498: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.1736 - mae: 0.4708 - val_loss: 0.1542 - val_mae: 0.4412\n",
            "Epoch 499/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4708\n",
            "Epoch 499: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1562 - val_mae: 0.4438\n",
            "Epoch 500/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4712\n",
            "Epoch 500: loss did not improve from 0.17249\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1730 - mae: 0.4698 - val_loss: 0.1563 - val_mae: 0.4437\n",
            "Step time : 0.019\n",
            "Total time : 658.898\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_entrainement.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuDKOH5I-d3x"
      },
      "outputs": [],
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "Serie_Normalisee = np.array(Serie_Normalisee)\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG94YFpF-oRh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "70a13314-b997-4fea-ad6e-7c46845b60d5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTM simple couche')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTM simple couche (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Zhf87jJ-vGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484a558f-2051-4099-edc5-c5c568b86902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8014878668460166\n",
            "5.322318351820038\n"
          ]
        }
      ],
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xkt08sBynJhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Création du modèle LSTM Bi-directionnel"
      ],
      "metadata": {
        "id": "pDHeyI6GmtY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Construction du modèle**"
      ],
      "metadata": {
        "id": "T8x7jL7Om7d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remise à zéro des états du modèle\n",
        "tf.keras.backend.clear_session()\n",
        "model.load_weights(\"model_initial.hdf5\")"
      ],
      "metadata": {
        "id": "ynVSVTNG5mPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre))\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees)\n",
        "s_encodeur = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40))(e_adapt)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(80,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPUojrEX5mEp",
        "outputId": "635d2d02-6108-49d5-afe9-b1a2a8571ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 20)]         0           []                               \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 20, 1)        0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 80)           13440       ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 80)           6480        ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 160)          0           ['dense[0][0]',                  \n",
            "                                                                  'bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            161         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20,081\n",
            "Trainable params: 20,081\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ],
      "metadata": {
        "id": "OJApdywRnPkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-3,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqZbbPVZ5l8Z",
        "outputId": "5279ee34-425a-4b72-a199-f30055ff19dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     64/Unknown - 4s 14ms/step - loss: 0.3475 - mae: 0.7042\n",
            "Epoch 1: loss improved from inf to 0.34797, saving model to poids.hdf5\n",
            "68/68 [==============================] - 4s 15ms/step - loss: 0.3480 - mae: 0.7049 - lr: 1.0000e-08\n",
            "Epoch 2/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3490 - mae: 0.7062\n",
            "Epoch 2: loss did not improve from 0.34797\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.3490 - mae: 0.7064 - lr: 1.2589e-08\n",
            "Epoch 3/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3491 - mae: 0.7062\n",
            "Epoch 3: loss did not improve from 0.34797\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.3491 - mae: 0.7062 - lr: 1.5849e-08\n",
            "Epoch 4/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3514 - mae: 0.7094\n",
            "Epoch 4: loss improved from 0.34797 to 0.34779, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.3478 - mae: 0.7048 - lr: 1.9953e-08\n",
            "Epoch 5/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3459 - mae: 0.7034\n",
            "Epoch 5: loss did not improve from 0.34779\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3490 - mae: 0.7061 - lr: 2.5119e-08\n",
            "Epoch 6/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3484 - mae: 0.7061\n",
            "Epoch 6: loss did not improve from 0.34779\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3489 - mae: 0.7061 - lr: 3.1623e-08\n",
            "Epoch 7/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3481 - mae: 0.7052\n",
            "Epoch 7: loss did not improve from 0.34779\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3484 - mae: 0.7057 - lr: 3.9811e-08\n",
            "Epoch 8/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3516 - mae: 0.7090\n",
            "Epoch 8: loss did not improve from 0.34779\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3486 - mae: 0.7057 - lr: 5.0119e-08\n",
            "Epoch 9/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3471 - mae: 0.7040\n",
            "Epoch 9: loss did not improve from 0.34779\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3491 - mae: 0.7064 - lr: 6.3096e-08\n",
            "Epoch 10/100\n",
            "63/68 [==========================>...] - ETA: 0s - loss: 0.3527 - mae: 0.7107\n",
            "Epoch 10: loss did not improve from 0.34779\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3478 - mae: 0.7047 - lr: 7.9433e-08\n",
            "Epoch 11/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3480 - mae: 0.7049\n",
            "Epoch 11: loss did not improve from 0.34779\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3483 - mae: 0.7053 - lr: 1.0000e-07\n",
            "Epoch 12/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3516 - mae: 0.7104\n",
            "Epoch 12: loss did not improve from 0.34779\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.3488 - mae: 0.7063 - lr: 1.2589e-07\n",
            "Epoch 13/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3472 - mae: 0.7047\n",
            "Epoch 13: loss improved from 0.34779 to 0.34772, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3477 - mae: 0.7048 - lr: 1.5849e-07\n",
            "Epoch 14/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3473 - mae: 0.7040\n",
            "Epoch 14: loss did not improve from 0.34772\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3479 - mae: 0.7049 - lr: 1.9953e-07\n",
            "Epoch 15/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3452 - mae: 0.7021\n",
            "Epoch 15: loss did not improve from 0.34772\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3482 - mae: 0.7054 - lr: 2.5119e-07\n",
            "Epoch 16/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3501 - mae: 0.7080\n",
            "Epoch 16: loss did not improve from 0.34772\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3488 - mae: 0.7062 - lr: 3.1623e-07\n",
            "Epoch 17/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3492 - mae: 0.7071\n",
            "Epoch 17: loss did not improve from 0.34772\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3485 - mae: 0.7054 - lr: 3.9811e-07\n",
            "Epoch 18/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3499 - mae: 0.7072\n",
            "Epoch 18: loss improved from 0.34772 to 0.34766, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3477 - mae: 0.7045 - lr: 5.0119e-07\n",
            "Epoch 19/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3479 - mae: 0.7050\n",
            "Epoch 19: loss did not improve from 0.34766\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.3479 - mae: 0.7050 - lr: 6.3096e-07\n",
            "Epoch 20/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.3485 - mae: 0.7055\n",
            "Epoch 20: loss improved from 0.34766 to 0.34695, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3469 - mae: 0.7035 - lr: 7.9433e-07\n",
            "Epoch 21/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.3439 - mae: 0.6999\n",
            "Epoch 21: loss did not improve from 0.34695\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3470 - mae: 0.7037 - lr: 1.0000e-06\n",
            "Epoch 22/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3480 - mae: 0.7052\n",
            "Epoch 22: loss improved from 0.34695 to 0.34686, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3469 - mae: 0.7035 - lr: 1.2589e-06\n",
            "Epoch 23/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3458 - mae: 0.7020\n",
            "Epoch 23: loss improved from 0.34686 to 0.34590, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3459 - mae: 0.7022 - lr: 1.5849e-06\n",
            "Epoch 24/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3476 - mae: 0.7045\n",
            "Epoch 24: loss improved from 0.34590 to 0.34525, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3452 - mae: 0.7017 - lr: 1.9953e-06\n",
            "Epoch 25/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3436 - mae: 0.6996\n",
            "Epoch 25: loss improved from 0.34525 to 0.34438, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3444 - mae: 0.7002 - lr: 2.5119e-06\n",
            "Epoch 26/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3435 - mae: 0.6990\n",
            "Epoch 26: loss improved from 0.34438 to 0.34347, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.3435 - mae: 0.6990 - lr: 3.1623e-06\n",
            "Epoch 27/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3422 - mae: 0.6984\n",
            "Epoch 27: loss improved from 0.34347 to 0.34128, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3413 - mae: 0.6962 - lr: 3.9811e-06\n",
            "Epoch 28/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3399 - mae: 0.6946\n",
            "Epoch 28: loss improved from 0.34128 to 0.33989, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.3399 - mae: 0.6946 - lr: 5.0119e-06\n",
            "Epoch 29/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3356 - mae: 0.6896\n",
            "Epoch 29: loss improved from 0.33989 to 0.33743, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3374 - mae: 0.6915 - lr: 6.3096e-06\n",
            "Epoch 30/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3352 - mae: 0.6882\n",
            "Epoch 30: loss improved from 0.33743 to 0.33516, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 16ms/step - loss: 0.3352 - mae: 0.6882 - lr: 7.9433e-06\n",
            "Epoch 31/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3327 - mae: 0.6846\n",
            "Epoch 31: loss improved from 0.33516 to 0.33168, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 0.3317 - mae: 0.6840 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3265 - mae: 0.6771\n",
            "Epoch 32: loss improved from 0.33168 to 0.32651, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3265 - mae: 0.6771 - lr: 1.2589e-05\n",
            "Epoch 33/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.3225 - mae: 0.6715\n",
            "Epoch 33: loss improved from 0.32651 to 0.32207, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3221 - mae: 0.6712 - lr: 1.5849e-05\n",
            "Epoch 34/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.3169 - mae: 0.6658\n",
            "Epoch 34: loss improved from 0.32207 to 0.31580, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.3158 - mae: 0.6631 - lr: 1.9953e-05\n",
            "Epoch 35/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.3087 - mae: 0.6542\n",
            "Epoch 35: loss improved from 0.31580 to 0.30875, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3087 - mae: 0.6542 - lr: 2.5119e-05\n",
            "Epoch 36/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2994 - mae: 0.6419\n",
            "Epoch 36: loss improved from 0.30875 to 0.30080, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.3008 - mae: 0.6437 - lr: 3.1623e-05\n",
            "Epoch 37/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.2902 - mae: 0.6297\n",
            "Epoch 37: loss improved from 0.30080 to 0.29113, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2911 - mae: 0.6310 - lr: 3.9811e-05\n",
            "Epoch 38/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.2824 - mae: 0.6190\n",
            "Epoch 38: loss improved from 0.29113 to 0.28067, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2807 - mae: 0.6170 - lr: 5.0119e-05\n",
            "Epoch 39/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2696 - mae: 0.6024\n",
            "Epoch 39: loss improved from 0.28067 to 0.26960, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2696 - mae: 0.6024 - lr: 6.3096e-05\n",
            "Epoch 40/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2588 - mae: 0.5871\n",
            "Epoch 40: loss improved from 0.26960 to 0.25784, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2578 - mae: 0.5864 - lr: 7.9433e-05\n",
            "Epoch 41/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2465 - mae: 0.5706\n",
            "Epoch 41: loss improved from 0.25784 to 0.24649, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2465 - mae: 0.5706 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.2377 - mae: 0.5593\n",
            "Epoch 42: loss improved from 0.24649 to 0.23676, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2368 - mae: 0.5586 - lr: 1.2589e-04\n",
            "Epoch 43/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.2268 - mae: 0.5460\n",
            "Epoch 43: loss improved from 0.23676 to 0.22823, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2282 - mae: 0.5482 - lr: 1.5849e-04\n",
            "Epoch 44/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.2230 - mae: 0.5425\n",
            "Epoch 44: loss improved from 0.22823 to 0.22231, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2223 - mae: 0.5414 - lr: 1.9953e-04\n",
            "Epoch 45/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2200 - mae: 0.5395\n",
            "Epoch 45: loss improved from 0.22231 to 0.21915, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2192 - mae: 0.5388 - lr: 2.5119e-04\n",
            "Epoch 46/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2159 - mae: 0.5357\n",
            "Epoch 46: loss improved from 0.21915 to 0.21588, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2159 - mae: 0.5348 - lr: 3.1623e-04\n",
            "Epoch 47/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2135 - mae: 0.5320\n",
            "Epoch 47: loss improved from 0.21588 to 0.21430, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2143 - mae: 0.5331 - lr: 3.9811e-04\n",
            "Epoch 48/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2122 - mae: 0.5296\n",
            "Epoch 48: loss improved from 0.21430 to 0.21161, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2116 - mae: 0.5292 - lr: 5.0119e-04\n",
            "Epoch 49/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2085 - mae: 0.5258\n",
            "Epoch 49: loss improved from 0.21161 to 0.21036, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2104 - mae: 0.5274 - lr: 6.3096e-04\n",
            "Epoch 50/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2076 - mae: 0.5231\n",
            "Epoch 50: loss improved from 0.21036 to 0.20765, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2076 - mae: 0.5231 - lr: 7.9433e-04\n",
            "Epoch 51/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2051 - mae: 0.5192\n",
            "Epoch 51: loss improved from 0.20765 to 0.20588, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2059 - mae: 0.5204 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.2039 - mae: 0.5172\n",
            "Epoch 52: loss improved from 0.20588 to 0.20396, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2040 - mae: 0.5173 - lr: 0.0013\n",
            "Epoch 53/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.2023 - mae: 0.5148\n",
            "Epoch 53: loss improved from 0.20396 to 0.20226, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2023 - mae: 0.5148 - lr: 0.0016\n",
            "Epoch 54/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2017 - mae: 0.5139\n",
            "Epoch 54: loss improved from 0.20226 to 0.20166, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2017 - mae: 0.5134 - lr: 0.0020\n",
            "Epoch 55/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2018 - mae: 0.5129\n",
            "Epoch 55: loss improved from 0.20166 to 0.20032, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2003 - mae: 0.5103 - lr: 0.0025\n",
            "Epoch 56/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1992 - mae: 0.5085\n",
            "Epoch 56: loss improved from 0.20032 to 0.19982, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1998 - mae: 0.5096 - lr: 0.0032\n",
            "Epoch 57/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1997 - mae: 0.5095\n",
            "Epoch 57: loss improved from 0.19982 to 0.19886, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1989 - mae: 0.5086 - lr: 0.0040\n",
            "Epoch 58/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1957 - mae: 0.5031\n",
            "Epoch 58: loss improved from 0.19886 to 0.19763, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1976 - mae: 0.5056 - lr: 0.0050\n",
            "Epoch 59/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1973 - mae: 0.5054\n",
            "Epoch 59: loss did not improve from 0.19763\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1977 - mae: 0.5065 - lr: 0.0063\n",
            "Epoch 60/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1982 - mae: 0.5059\n",
            "Epoch 60: loss improved from 0.19763 to 0.19618, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1962 - mae: 0.5040 - lr: 0.0079\n",
            "Epoch 61/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1954 - mae: 0.5039\n",
            "Epoch 61: loss improved from 0.19618 to 0.19522, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 0.1952 - mae: 0.5030 - lr: 0.0100\n",
            "Epoch 62/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1946 - mae: 0.5037\n",
            "Epoch 62: loss improved from 0.19522 to 0.19468, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1947 - mae: 0.5027 - lr: 0.0126\n",
            "Epoch 63/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1962 - mae: 0.5059\n",
            "Epoch 63: loss improved from 0.19468 to 0.19421, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1942 - mae: 0.5027 - lr: 0.0158\n",
            "Epoch 64/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1923 - mae: 0.4993\n",
            "Epoch 64: loss improved from 0.19421 to 0.19206, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1921 - mae: 0.4991 - lr: 0.0200\n",
            "Epoch 65/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1924 - mae: 0.5002\n",
            "Epoch 65: loss improved from 0.19206 to 0.19184, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1918 - mae: 0.4987 - lr: 0.0251\n",
            "Epoch 66/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1944 - mae: 0.5042\n",
            "Epoch 66: loss did not improve from 0.19184\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1924 - mae: 0.5013 - lr: 0.0316\n",
            "Epoch 67/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1916 - mae: 0.5008\n",
            "Epoch 67: loss improved from 0.19184 to 0.19095, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1910 - mae: 0.4993 - lr: 0.0398\n",
            "Epoch 68/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1852 - mae: 0.4898\n",
            "Epoch 68: loss improved from 0.19095 to 0.18524, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1852 - mae: 0.4898 - lr: 0.0501\n",
            "Epoch 69/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1832 - mae: 0.4865\n",
            "Epoch 69: loss improved from 0.18524 to 0.18317, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1832 - mae: 0.4865 - lr: 0.0631\n",
            "Epoch 70/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1814 - mae: 0.4830\n",
            "Epoch 70: loss improved from 0.18317 to 0.18145, saving model to poids.hdf5\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1814 - mae: 0.4830 - lr: 0.0794\n",
            "Epoch 71/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1891 - mae: 0.4968\n",
            "Epoch 71: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1883 - mae: 0.4952 - lr: 0.1000\n",
            "Epoch 72/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1885 - mae: 0.4924\n",
            "Epoch 72: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1881 - mae: 0.4911 - lr: 0.1259\n",
            "Epoch 73/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1873 - mae: 0.4917\n",
            "Epoch 73: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1887 - mae: 0.4939 - lr: 0.1585\n",
            "Epoch 74/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1845 - mae: 0.4871\n",
            "Epoch 74: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1841 - mae: 0.4859 - lr: 0.1995\n",
            "Epoch 75/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1974 - mae: 0.5107\n",
            "Epoch 75: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1974 - mae: 0.5103 - lr: 0.2512\n",
            "Epoch 76/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1968 - mae: 0.5059\n",
            "Epoch 76: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1979 - mae: 0.5080 - lr: 0.3162\n",
            "Epoch 77/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1895 - mae: 0.4943\n",
            "Epoch 77: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.1895 - mae: 0.4943 - lr: 0.3981\n",
            "Epoch 78/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1984 - mae: 0.5111\n",
            "Epoch 78: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.1984 - mae: 0.5111 - lr: 0.5012\n",
            "Epoch 79/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.2157 - mae: 0.5352\n",
            "Epoch 79: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2167 - mae: 0.5370 - lr: 0.6310\n",
            "Epoch 80/100\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.2144 - mae: 0.5287\n",
            "Epoch 80: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 0.2116 - mae: 0.5251 - lr: 0.7943\n",
            "Epoch 81/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.2064 - mae: 0.5202\n",
            "Epoch 81: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.2067 - mae: 0.5203 - lr: 1.0000\n",
            "Epoch 82/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 1.9745 - mae: 2.3641\n",
            "Epoch 82: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 1.9947 - mae: 2.3857 - lr: 1.2589\n",
            "Epoch 83/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 12.6453 - mae: 13.1420\n",
            "Epoch 83: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 12.6453 - mae: 13.1420 - lr: 1.5849\n",
            "Epoch 84/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 61.3391 - mae: 61.8389\n",
            "Epoch 84: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 64.0230 - mae: 64.5228 - lr: 1.9953\n",
            "Epoch 85/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 247.8840 - mae: 248.3827\n",
            "Epoch 85: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 247.8840 - mae: 248.3827 - lr: 2.5119\n",
            "Epoch 86/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 82.0572 - mae: 82.5564\n",
            "Epoch 86: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 81.3790 - mae: 81.8782 - lr: 3.1623\n",
            "Epoch 87/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 229.7361 - mae: 230.2332\n",
            "Epoch 87: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 226.4663 - mae: 226.9635 - lr: 3.9811\n",
            "Epoch 88/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 604.9949 - mae: 605.4949\n",
            "Epoch 88: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 604.9949 - mae: 605.4949 - lr: 5.0119\n",
            "Epoch 89/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 586.3995 - mae: 586.8995\n",
            "Epoch 89: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 601.8086 - mae: 602.3086 - lr: 6.3096\n",
            "Epoch 90/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 707.6140 - mae: 708.1121\n",
            "Epoch 90: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 726.1536 - mae: 726.6517 - lr: 7.9433\n",
            "Epoch 91/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 580.0215 - mae: 580.5215\n",
            "Epoch 91: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 579.0319 - mae: 579.5319 - lr: 10.0000\n",
            "Epoch 92/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 534.7942 - mae: 535.2935\n",
            "Epoch 92: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 551.6685 - mae: 552.1678 - lr: 12.5893\n",
            "Epoch 93/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 1022.7319 - mae: 1023.2319\n",
            "Epoch 93: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 1009.3505 - mae: 1009.8505 - lr: 15.8489\n",
            "Epoch 94/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 1520.5653 - mae: 1521.0653\n",
            "Epoch 94: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 1526.9600 - mae: 1527.4600 - lr: 19.9526\n",
            "Epoch 95/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 1684.7578 - mae: 1685.2578\n",
            "Epoch 95: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 1680.6949 - mae: 1681.1949 - lr: 25.1189\n",
            "Epoch 96/100\n",
            "68/68 [==============================] - ETA: 0s - loss: 2932.2263 - mae: 2932.7263\n",
            "Epoch 96: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 2932.2263 - mae: 2932.7263 - lr: 31.6228\n",
            "Epoch 97/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 4975.1631 - mae: 4975.6631\n",
            "Epoch 97: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 5024.1074 - mae: 5024.6074 - lr: 39.8107\n",
            "Epoch 98/100\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 4702.0889 - mae: 4702.5884\n",
            "Epoch 98: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 4751.2393 - mae: 4751.7388 - lr: 50.1187\n",
            "Epoch 99/100\n",
            "67/68 [============================>.] - ETA: 0s - loss: 5188.1157 - mae: 5188.6157\n",
            "Epoch 99: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 5197.3159 - mae: 5197.8159 - lr: 63.0957\n",
            "Epoch 100/100\n",
            "66/68 [============================>.] - ETA: 0s - loss: 3364.6941 - mae: 3365.1941\n",
            "Epoch 100: loss did not improve from 0.18145\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 3357.3208 - mae: 3357.8208 - lr: 79.4328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([taux[0], taux[99], 0, 0.5])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "YM874KCW5ly_",
        "outputId": "f67267dd-71a8-4693-8df9-12663c6f47da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, \"Evolution de l'erreur en fonction du taux d'apprentissage\")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Entrainement du modèle**"
      ],
      "metadata": {
        "id": "HUkx5Zpmni1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "metadata": {
        "id": "aV7uDZmNndzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.015,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_entrainement.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMPhcHAhndu-",
        "outputId": "dca2e66c-a504-4f01-c422-e2035ca28980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "     66/Unknown - 4s 13ms/step - loss: 0.1780 - mae: 0.4778\n",
            "Epoch 1: loss improved from inf to 0.18005, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 6s 36ms/step - loss: 0.1800 - mae: 0.4800 - val_loss: 0.1602 - val_mae: 0.4475\n",
            "Epoch 2/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1759 - mae: 0.4757\n",
            "Epoch 2: loss improved from 0.18005 to 0.17737, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1774 - mae: 0.4775 - val_loss: 0.1603 - val_mae: 0.4476\n",
            "Epoch 3/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1767 - mae: 0.4771\n",
            "Epoch 3: loss improved from 0.17737 to 0.17672, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1767 - mae: 0.4771 - val_loss: 0.1603 - val_mae: 0.4491\n",
            "Epoch 4/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4783\n",
            "Epoch 4: loss did not improve from 0.17672\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1778 - mae: 0.4798 - val_loss: 0.1635 - val_mae: 0.4548\n",
            "Epoch 5/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1777 - mae: 0.4787\n",
            "Epoch 5: loss did not improve from 0.17672\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1777 - mae: 0.4787 - val_loss: 0.1585 - val_mae: 0.4466\n",
            "Epoch 6/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1768 - mae: 0.4760\n",
            "Epoch 6: loss did not improve from 0.17672\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1768 - mae: 0.4760 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 7/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1775 - mae: 0.4777\n",
            "Epoch 7: loss improved from 0.17672 to 0.17668, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1767 - mae: 0.4759 - val_loss: 0.1575 - val_mae: 0.4435\n",
            "Epoch 8/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4762\n",
            "Epoch 8: loss did not improve from 0.17668\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1772 - mae: 0.4772 - val_loss: 0.1583 - val_mae: 0.4449\n",
            "Epoch 9/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1776 - mae: 0.4775\n",
            "Epoch 9: loss did not improve from 0.17668\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1772 - mae: 0.4771 - val_loss: 0.1593 - val_mae: 0.4483\n",
            "Epoch 10/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4766\n",
            "Epoch 10: loss improved from 0.17668 to 0.17587, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1611 - val_mae: 0.4489\n",
            "Epoch 11/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1767 - mae: 0.4760\n",
            "Epoch 11: loss did not improve from 0.17587\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1767 - mae: 0.4760 - val_loss: 0.1582 - val_mae: 0.4457\n",
            "Epoch 12/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4758\n",
            "Epoch 12: loss did not improve from 0.17587\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1762 - mae: 0.4758 - val_loss: 0.1550 - val_mae: 0.4411\n",
            "Epoch 13/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1775 - mae: 0.4785\n",
            "Epoch 13: loss did not improve from 0.17587\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1762 - mae: 0.4766 - val_loss: 0.1585 - val_mae: 0.4477\n",
            "Epoch 14/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1763 - mae: 0.4755\n",
            "Epoch 14: loss did not improve from 0.17587\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1764 - mae: 0.4761 - val_loss: 0.1569 - val_mae: 0.4439\n",
            "Epoch 15/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4758\n",
            "Epoch 15: loss improved from 0.17587 to 0.17577, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1758 - mae: 0.4758 - val_loss: 0.1561 - val_mae: 0.4424\n",
            "Epoch 16/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4771\n",
            "Epoch 16: loss improved from 0.17577 to 0.17533, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1753 - mae: 0.4751 - val_loss: 0.1586 - val_mae: 0.4457\n",
            "Epoch 17/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4724\n",
            "Epoch 17: loss did not improve from 0.17533\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1757 - mae: 0.4749 - val_loss: 0.1572 - val_mae: 0.4439\n",
            "Epoch 18/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4715\n",
            "Epoch 18: loss improved from 0.17533 to 0.17516, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1752 - mae: 0.4743 - val_loss: 0.1577 - val_mae: 0.4448\n",
            "Epoch 19/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4775\n",
            "Epoch 19: loss did not improve from 0.17516\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1757 - mae: 0.4760 - val_loss: 0.1585 - val_mae: 0.4463\n",
            "Epoch 20/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4732\n",
            "Epoch 20: loss improved from 0.17516 to 0.17509, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1751 - mae: 0.4731 - val_loss: 0.1562 - val_mae: 0.4433\n",
            "Epoch 21/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4751\n",
            "Epoch 21: loss did not improve from 0.17509\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1752 - mae: 0.4745 - val_loss: 0.1572 - val_mae: 0.4449\n",
            "Epoch 22/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1751 - mae: 0.4744\n",
            "Epoch 22: loss did not improve from 0.17509\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1751 - mae: 0.4744 - val_loss: 0.1587 - val_mae: 0.4455\n",
            "Epoch 23/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4750\n",
            "Epoch 23: loss did not improve from 0.17509\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1565 - val_mae: 0.4432\n",
            "Epoch 24/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1740 - mae: 0.4719\n",
            "Epoch 24: loss did not improve from 0.17509\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1751 - mae: 0.4743 - val_loss: 0.1578 - val_mae: 0.4451\n",
            "Epoch 25/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4732\n",
            "Epoch 25: loss did not improve from 0.17509\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1752 - mae: 0.4742 - val_loss: 0.1559 - val_mae: 0.4426\n",
            "Epoch 26/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4740\n",
            "Epoch 26: loss improved from 0.17509 to 0.17472, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1747 - mae: 0.4740 - val_loss: 0.1572 - val_mae: 0.4443\n",
            "Epoch 27/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1749 - mae: 0.4727\n",
            "Epoch 27: loss did not improve from 0.17472\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1749 - mae: 0.4727 - val_loss: 0.1577 - val_mae: 0.4461\n",
            "Epoch 28/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4736\n",
            "Epoch 28: loss did not improve from 0.17472\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1748 - mae: 0.4741 - val_loss: 0.1562 - val_mae: 0.4424\n",
            "Epoch 29/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4747\n",
            "Epoch 29: loss improved from 0.17472 to 0.17449, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1745 - mae: 0.4742 - val_loss: 0.1580 - val_mae: 0.4450\n",
            "Epoch 30/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4742\n",
            "Epoch 30: loss did not improve from 0.17449\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1756 - mae: 0.4742 - val_loss: 0.1589 - val_mae: 0.4464\n",
            "Epoch 31/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1749 - mae: 0.4736\n",
            "Epoch 31: loss did not improve from 0.17449\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1749 - mae: 0.4736 - val_loss: 0.1565 - val_mae: 0.4437\n",
            "Epoch 32/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4749\n",
            "Epoch 32: loss did not improve from 0.17449\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1749 - mae: 0.4743 - val_loss: 0.1585 - val_mae: 0.4465\n",
            "Epoch 33/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4743\n",
            "Epoch 33: loss did not improve from 0.17449\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1752 - mae: 0.4747 - val_loss: 0.1577 - val_mae: 0.4452\n",
            "Epoch 34/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4765\n",
            "Epoch 34: loss did not improve from 0.17449\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1749 - mae: 0.4736 - val_loss: 0.1573 - val_mae: 0.4458\n",
            "Epoch 35/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1715 - mae: 0.4705\n",
            "Epoch 35: loss did not improve from 0.17449\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1746 - mae: 0.4741 - val_loss: 0.1562 - val_mae: 0.4429\n",
            "Epoch 36/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4741\n",
            "Epoch 36: loss did not improve from 0.17449\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1753 - mae: 0.4744 - val_loss: 0.1573 - val_mae: 0.4441\n",
            "Epoch 37/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1747 - mae: 0.4738\n",
            "Epoch 37: loss did not improve from 0.17449\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1747 - mae: 0.4738 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 38/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 38: loss did not improve from 0.17449\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1752 - mae: 0.4739 - val_loss: 0.1573 - val_mae: 0.4448\n",
            "Epoch 39/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4751\n",
            "Epoch 39: loss improved from 0.17449 to 0.17412, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1741 - mae: 0.4737 - val_loss: 0.1579 - val_mae: 0.4456\n",
            "Epoch 40/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4734\n",
            "Epoch 40: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1748 - mae: 0.4734 - val_loss: 0.1565 - val_mae: 0.4434\n",
            "Epoch 41/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1730 - mae: 0.4715\n",
            "Epoch 41: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1746 - mae: 0.4736 - val_loss: 0.1565 - val_mae: 0.4431\n",
            "Epoch 42/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4725\n",
            "Epoch 42: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1745 - mae: 0.4738 - val_loss: 0.1566 - val_mae: 0.4435\n",
            "Epoch 43/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1758 - mae: 0.4744\n",
            "Epoch 43: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1742 - mae: 0.4730 - val_loss: 0.1566 - val_mae: 0.4445\n",
            "Epoch 44/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4716\n",
            "Epoch 44: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1744 - mae: 0.4732 - val_loss: 0.1554 - val_mae: 0.4436\n",
            "Epoch 45/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4738\n",
            "Epoch 45: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1746 - mae: 0.4744 - val_loss: 0.1576 - val_mae: 0.4445\n",
            "Epoch 46/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4748\n",
            "Epoch 46: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1746 - mae: 0.4736 - val_loss: 0.1571 - val_mae: 0.4438\n",
            "Epoch 47/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1768 - mae: 0.4767\n",
            "Epoch 47: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1746 - mae: 0.4729 - val_loss: 0.1571 - val_mae: 0.4431\n",
            "Epoch 48/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4730\n",
            "Epoch 48: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1745 - mae: 0.4735 - val_loss: 0.1575 - val_mae: 0.4441\n",
            "Epoch 49/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4747\n",
            "Epoch 49: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1744 - mae: 0.4723 - val_loss: 0.1563 - val_mae: 0.4430\n",
            "Epoch 50/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4734\n",
            "Epoch 50: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1743 - mae: 0.4734 - val_loss: 0.1569 - val_mae: 0.4441\n",
            "Epoch 51/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1741 - mae: 0.4722\n",
            "Epoch 51: loss did not improve from 0.17412\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1741 - mae: 0.4722 - val_loss: 0.1572 - val_mae: 0.4456\n",
            "Epoch 52/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4710\n",
            "Epoch 52: loss improved from 0.17412 to 0.17411, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1741 - mae: 0.4727 - val_loss: 0.1564 - val_mae: 0.4430\n",
            "Epoch 53/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4747\n",
            "Epoch 53: loss did not improve from 0.17411\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1743 - mae: 0.4736 - val_loss: 0.1568 - val_mae: 0.4435\n",
            "Epoch 54/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4725\n",
            "Epoch 54: loss improved from 0.17411 to 0.17410, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1741 - mae: 0.4723 - val_loss: 0.1567 - val_mae: 0.4440\n",
            "Epoch 55/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1746 - mae: 0.4740\n",
            "Epoch 55: loss did not improve from 0.17410\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1746 - mae: 0.4740 - val_loss: 0.1564 - val_mae: 0.4428\n",
            "Epoch 56/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1740 - mae: 0.4725\n",
            "Epoch 56: loss improved from 0.17410 to 0.17399, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1740 - mae: 0.4725 - val_loss: 0.1588 - val_mae: 0.4476\n",
            "Epoch 57/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4735\n",
            "Epoch 57: loss did not improve from 0.17399\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1743 - mae: 0.4730 - val_loss: 0.1578 - val_mae: 0.4451\n",
            "Epoch 58/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4705\n",
            "Epoch 58: loss did not improve from 0.17399\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1742 - mae: 0.4722 - val_loss: 0.1576 - val_mae: 0.4461\n",
            "Epoch 59/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4724\n",
            "Epoch 59: loss did not improve from 0.17399\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1741 - mae: 0.4730 - val_loss: 0.1572 - val_mae: 0.4450\n",
            "Epoch 60/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1736 - mae: 0.4724\n",
            "Epoch 60: loss improved from 0.17399 to 0.17363, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4724 - val_loss: 0.1583 - val_mae: 0.4471\n",
            "Epoch 61/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4716\n",
            "Epoch 61: loss did not improve from 0.17363\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1738 - mae: 0.4723 - val_loss: 0.1570 - val_mae: 0.4440\n",
            "Epoch 62/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4731\n",
            "Epoch 62: loss did not improve from 0.17363\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1743 - mae: 0.4731 - val_loss: 0.1567 - val_mae: 0.4444\n",
            "Epoch 63/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4731\n",
            "Epoch 63: loss improved from 0.17363 to 0.17350, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1735 - mae: 0.4722 - val_loss: 0.1568 - val_mae: 0.4445\n",
            "Epoch 64/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4723\n",
            "Epoch 64: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1743 - mae: 0.4723 - val_loss: 0.1561 - val_mae: 0.4419\n",
            "Epoch 65/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4745\n",
            "Epoch 65: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1739 - mae: 0.4724 - val_loss: 0.1582 - val_mae: 0.4470\n",
            "Epoch 66/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1743 - mae: 0.4736\n",
            "Epoch 66: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1743 - mae: 0.4736 - val_loss: 0.1570 - val_mae: 0.4443\n",
            "Epoch 67/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4750\n",
            "Epoch 67: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1740 - mae: 0.4732 - val_loss: 0.1563 - val_mae: 0.4431\n",
            "Epoch 68/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4755\n",
            "Epoch 68: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4721 - val_loss: 0.1573 - val_mae: 0.4448\n",
            "Epoch 69/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4738\n",
            "Epoch 69: loss improved from 0.17350 to 0.17350, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1735 - mae: 0.4722 - val_loss: 0.1581 - val_mae: 0.4463\n",
            "Epoch 70/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1738 - mae: 0.4719\n",
            "Epoch 70: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1738 - mae: 0.4719 - val_loss: 0.1558 - val_mae: 0.4422\n",
            "Epoch 71/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4748\n",
            "Epoch 71: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1741 - mae: 0.4729 - val_loss: 0.1560 - val_mae: 0.4423\n",
            "Epoch 72/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1740 - mae: 0.4724\n",
            "Epoch 72: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1740 - mae: 0.4724 - val_loss: 0.1567 - val_mae: 0.4448\n",
            "Epoch 73/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4732\n",
            "Epoch 73: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1737 - mae: 0.4726 - val_loss: 0.1530 - val_mae: 0.4387\n",
            "Epoch 74/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4737\n",
            "Epoch 74: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1741 - mae: 0.4721 - val_loss: 0.1570 - val_mae: 0.4435\n",
            "Epoch 75/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4733\n",
            "Epoch 75: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1738 - mae: 0.4723 - val_loss: 0.1574 - val_mae: 0.4438\n",
            "Epoch 76/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4727\n",
            "Epoch 76: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1743 - mae: 0.4727 - val_loss: 0.1571 - val_mae: 0.4441\n",
            "Epoch 77/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.4701\n",
            "Epoch 77: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1736 - mae: 0.4722 - val_loss: 0.1562 - val_mae: 0.4424\n",
            "Epoch 78/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4724\n",
            "Epoch 78: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4723 - val_loss: 0.1575 - val_mae: 0.4455\n",
            "Epoch 79/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4740\n",
            "Epoch 79: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1743 - mae: 0.4727 - val_loss: 0.1571 - val_mae: 0.4442\n",
            "Epoch 80/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4710\n",
            "Epoch 80: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1737 - mae: 0.4718 - val_loss: 0.1576 - val_mae: 0.4454\n",
            "Epoch 81/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4712\n",
            "Epoch 81: loss did not improve from 0.17350\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1741 - mae: 0.4726 - val_loss: 0.1574 - val_mae: 0.4456\n",
            "Epoch 82/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4705\n",
            "Epoch 82: loss improved from 0.17350 to 0.17256, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1726 - mae: 0.4708 - val_loss: 0.1564 - val_mae: 0.4432\n",
            "Epoch 83/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4721\n",
            "Epoch 83: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1741 - mae: 0.4725 - val_loss: 0.1558 - val_mae: 0.4437\n",
            "Epoch 84/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1731 - mae: 0.4716\n",
            "Epoch 84: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1731 - mae: 0.4716 - val_loss: 0.1563 - val_mae: 0.4429\n",
            "Epoch 85/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1740 - mae: 0.4726\n",
            "Epoch 85: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1740 - mae: 0.4726 - val_loss: 0.1569 - val_mae: 0.4430\n",
            "Epoch 86/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4711\n",
            "Epoch 86: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1740 - mae: 0.4717 - val_loss: 0.1562 - val_mae: 0.4428\n",
            "Epoch 87/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4714\n",
            "Epoch 87: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1734 - mae: 0.4717 - val_loss: 0.1565 - val_mae: 0.4441\n",
            "Epoch 88/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4758\n",
            "Epoch 88: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1737 - mae: 0.4725 - val_loss: 0.1567 - val_mae: 0.4444\n",
            "Epoch 89/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1736 - mae: 0.4724\n",
            "Epoch 89: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1736 - mae: 0.4724 - val_loss: 0.1563 - val_mae: 0.4430\n",
            "Epoch 90/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1736 - mae: 0.4718\n",
            "Epoch 90: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1736 - mae: 0.4718 - val_loss: 0.1566 - val_mae: 0.4441\n",
            "Epoch 91/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4730\n",
            "Epoch 91: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4721 - val_loss: 0.1558 - val_mae: 0.4424\n",
            "Epoch 92/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1736 - mae: 0.4715\n",
            "Epoch 92: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4715 - val_loss: 0.1561 - val_mae: 0.4436\n",
            "Epoch 93/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.4736\n",
            "Epoch 93: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1733 - mae: 0.4720 - val_loss: 0.1568 - val_mae: 0.4433\n",
            "Epoch 94/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4720\n",
            "Epoch 94: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1738 - mae: 0.4718 - val_loss: 0.1564 - val_mae: 0.4440\n",
            "Epoch 95/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4718\n",
            "Epoch 95: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1733 - mae: 0.4722 - val_loss: 0.1566 - val_mae: 0.4430\n",
            "Epoch 96/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4694\n",
            "Epoch 96: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1731 - mae: 0.4712 - val_loss: 0.1567 - val_mae: 0.4433\n",
            "Epoch 97/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4714\n",
            "Epoch 97: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4718 - val_loss: 0.1561 - val_mae: 0.4428\n",
            "Epoch 98/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4711\n",
            "Epoch 98: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1732 - mae: 0.4710 - val_loss: 0.1560 - val_mae: 0.4430\n",
            "Epoch 99/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4710\n",
            "Epoch 99: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1734 - mae: 0.4710 - val_loss: 0.1572 - val_mae: 0.4459\n",
            "Epoch 100/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4725\n",
            "Epoch 100: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1737 - mae: 0.4726 - val_loss: 0.1571 - val_mae: 0.4448\n",
            "Epoch 101/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4701\n",
            "Epoch 101: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1731 - mae: 0.4717 - val_loss: 0.1546 - val_mae: 0.4404\n",
            "Epoch 102/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4695\n",
            "Epoch 102: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1732 - mae: 0.4705 - val_loss: 0.1573 - val_mae: 0.4442\n",
            "Epoch 103/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4712\n",
            "Epoch 103: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1735 - mae: 0.4724 - val_loss: 0.1566 - val_mae: 0.4425\n",
            "Epoch 104/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4741\n",
            "Epoch 104: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1735 - mae: 0.4717 - val_loss: 0.1561 - val_mae: 0.4430\n",
            "Epoch 105/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4734\n",
            "Epoch 105: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1735 - mae: 0.4716 - val_loss: 0.1571 - val_mae: 0.4442\n",
            "Epoch 106/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4714\n",
            "Epoch 106: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1736 - mae: 0.4729 - val_loss: 0.1554 - val_mae: 0.4412\n",
            "Epoch 107/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1724 - mae: 0.4690\n",
            "Epoch 107: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1737 - mae: 0.4710 - val_loss: 0.1561 - val_mae: 0.4427\n",
            "Epoch 108/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1737 - mae: 0.4718\n",
            "Epoch 108: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1737 - mae: 0.4718 - val_loss: 0.1574 - val_mae: 0.4448\n",
            "Epoch 109/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1735 - mae: 0.4722\n",
            "Epoch 109: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1735 - mae: 0.4722 - val_loss: 0.1557 - val_mae: 0.4422\n",
            "Epoch 110/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4712\n",
            "Epoch 110: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1728 - mae: 0.4710 - val_loss: 0.1587 - val_mae: 0.4478\n",
            "Epoch 111/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4699\n",
            "Epoch 111: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1736 - mae: 0.4713 - val_loss: 0.1546 - val_mae: 0.4419\n",
            "Epoch 112/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1737 - mae: 0.4725\n",
            "Epoch 112: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1737 - mae: 0.4725 - val_loss: 0.1581 - val_mae: 0.4459\n",
            "Epoch 113/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4702\n",
            "Epoch 113: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1732 - mae: 0.4709 - val_loss: 0.1559 - val_mae: 0.4428\n",
            "Epoch 114/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1734 - mae: 0.4728\n",
            "Epoch 114: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1734 - mae: 0.4717 - val_loss: 0.1577 - val_mae: 0.4456\n",
            "Epoch 115/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1768 - mae: 0.4772\n",
            "Epoch 115: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1736 - mae: 0.4718 - val_loss: 0.1577 - val_mae: 0.4452\n",
            "Epoch 116/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.4731\n",
            "Epoch 116: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1734 - mae: 0.4714 - val_loss: 0.1568 - val_mae: 0.4438\n",
            "Epoch 117/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4707\n",
            "Epoch 117: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1733 - mae: 0.4714 - val_loss: 0.1572 - val_mae: 0.4445\n",
            "Epoch 118/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4701\n",
            "Epoch 118: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1738 - mae: 0.4722 - val_loss: 0.1568 - val_mae: 0.4432\n",
            "Epoch 119/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4713\n",
            "Epoch 119: loss did not improve from 0.17256\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1732 - mae: 0.4710 - val_loss: 0.1578 - val_mae: 0.4457\n",
            "Epoch 120/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1730 - mae: 0.4715\n",
            "Epoch 120: loss improved from 0.17256 to 0.17252, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1725 - mae: 0.4712 - val_loss: 0.1571 - val_mae: 0.4439\n",
            "Epoch 121/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1719 - mae: 0.4690\n",
            "Epoch 121: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1736 - mae: 0.4716 - val_loss: 0.1568 - val_mae: 0.4434\n",
            "Epoch 122/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4728\n",
            "Epoch 122: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1731 - mae: 0.4710 - val_loss: 0.1563 - val_mae: 0.4431\n",
            "Epoch 123/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1721 - mae: 0.4699\n",
            "Epoch 123: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1730 - mae: 0.4711 - val_loss: 0.1567 - val_mae: 0.4435\n",
            "Epoch 124/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4725\n",
            "Epoch 124: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1730 - mae: 0.4712 - val_loss: 0.1564 - val_mae: 0.4433\n",
            "Epoch 125/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4704\n",
            "Epoch 125: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1732 - mae: 0.4711 - val_loss: 0.1571 - val_mae: 0.4448\n",
            "Epoch 126/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1736 - mae: 0.4714\n",
            "Epoch 126: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1729 - mae: 0.4708 - val_loss: 0.1568 - val_mae: 0.4447\n",
            "Epoch 127/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4722\n",
            "Epoch 127: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1732 - mae: 0.4719 - val_loss: 0.1549 - val_mae: 0.4414\n",
            "Epoch 128/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4727\n",
            "Epoch 128: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1733 - mae: 0.4713 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 129/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1737 - mae: 0.4721\n",
            "Epoch 129: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1737 - mae: 0.4721 - val_loss: 0.1575 - val_mae: 0.4447\n",
            "Epoch 130/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1735 - mae: 0.4715\n",
            "Epoch 130: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1735 - mae: 0.4715 - val_loss: 0.1559 - val_mae: 0.4425\n",
            "Epoch 131/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1733 - mae: 0.4713\n",
            "Epoch 131: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1733 - mae: 0.4713 - val_loss: 0.1553 - val_mae: 0.4416\n",
            "Epoch 132/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1715 - mae: 0.4695\n",
            "Epoch 132: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4702 - val_loss: 0.1575 - val_mae: 0.4450\n",
            "Epoch 133/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4742\n",
            "Epoch 133: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1731 - mae: 0.4710 - val_loss: 0.1566 - val_mae: 0.4429\n",
            "Epoch 134/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4720\n",
            "Epoch 134: loss did not improve from 0.17252\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1733 - mae: 0.4714 - val_loss: 0.1576 - val_mae: 0.4460\n",
            "Epoch 135/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4727\n",
            "Epoch 135: loss improved from 0.17252 to 0.17214, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1721 - mae: 0.4703 - val_loss: 0.1573 - val_mae: 0.4438\n",
            "Epoch 136/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1733 - mae: 0.4711\n",
            "Epoch 136: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 0.1733 - mae: 0.4711 - val_loss: 0.1578 - val_mae: 0.4463\n",
            "Epoch 137/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4696\n",
            "Epoch 137: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1733 - mae: 0.4710 - val_loss: 0.1577 - val_mae: 0.4453\n",
            "Epoch 138/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1728 - mae: 0.4714\n",
            "Epoch 138: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1731 - mae: 0.4712 - val_loss: 0.1577 - val_mae: 0.4450\n",
            "Epoch 139/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1732 - mae: 0.4711\n",
            "Epoch 139: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1732 - mae: 0.4711 - val_loss: 0.1566 - val_mae: 0.4432\n",
            "Epoch 140/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4709\n",
            "Epoch 140: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1735 - mae: 0.4711 - val_loss: 0.1566 - val_mae: 0.4433\n",
            "Epoch 141/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4725\n",
            "Epoch 141: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1737 - mae: 0.4722 - val_loss: 0.1562 - val_mae: 0.4434\n",
            "Epoch 142/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4714\n",
            "Epoch 142: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1732 - mae: 0.4707 - val_loss: 0.1572 - val_mae: 0.4440\n",
            "Epoch 143/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.4735\n",
            "Epoch 143: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1733 - mae: 0.4717 - val_loss: 0.1564 - val_mae: 0.4433\n",
            "Epoch 144/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4698\n",
            "Epoch 144: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4700 - val_loss: 0.1565 - val_mae: 0.4437\n",
            "Epoch 145/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1721 - mae: 0.4694\n",
            "Epoch 145: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1732 - mae: 0.4711 - val_loss: 0.1563 - val_mae: 0.4436\n",
            "Epoch 146/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4704\n",
            "Epoch 146: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1732 - mae: 0.4711 - val_loss: 0.1564 - val_mae: 0.4437\n",
            "Epoch 147/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4716\n",
            "Epoch 147: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1734 - mae: 0.4717 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 148/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4705\n",
            "Epoch 148: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1733 - mae: 0.4712 - val_loss: 0.1574 - val_mae: 0.4451\n",
            "Epoch 149/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4724\n",
            "Epoch 149: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1732 - mae: 0.4712 - val_loss: 0.1538 - val_mae: 0.4398\n",
            "Epoch 150/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4726\n",
            "Epoch 150: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1733 - mae: 0.4716 - val_loss: 0.1571 - val_mae: 0.4442\n",
            "Epoch 151/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1729 - mae: 0.4705\n",
            "Epoch 151: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4705 - val_loss: 0.1576 - val_mae: 0.4444\n",
            "Epoch 152/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4719\n",
            "Epoch 152: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1730 - mae: 0.4710 - val_loss: 0.1577 - val_mae: 0.4449\n",
            "Epoch 153/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1732 - mae: 0.4717\n",
            "Epoch 153: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1732 - mae: 0.4717 - val_loss: 0.1568 - val_mae: 0.4444\n",
            "Epoch 154/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1739 - mae: 0.4733\n",
            "Epoch 154: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1731 - mae: 0.4715 - val_loss: 0.1573 - val_mae: 0.4439\n",
            "Epoch 155/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1730 - mae: 0.4704\n",
            "Epoch 155: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1730 - mae: 0.4704 - val_loss: 0.1563 - val_mae: 0.4430\n",
            "Epoch 156/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4704\n",
            "Epoch 156: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4708 - val_loss: 0.1557 - val_mae: 0.4424\n",
            "Epoch 157/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1729 - mae: 0.4706\n",
            "Epoch 157: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1729 - mae: 0.4706 - val_loss: 0.1566 - val_mae: 0.4431\n",
            "Epoch 158/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4706\n",
            "Epoch 158: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1731 - mae: 0.4708 - val_loss: 0.1552 - val_mae: 0.4409\n",
            "Epoch 159/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1735 - mae: 0.4717\n",
            "Epoch 159: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1735 - mae: 0.4717 - val_loss: 0.1569 - val_mae: 0.4435\n",
            "Epoch 160/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4707\n",
            "Epoch 160: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1734 - mae: 0.4707 - val_loss: 0.1562 - val_mae: 0.4423\n",
            "Epoch 161/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4711\n",
            "Epoch 161: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4706 - val_loss: 0.1567 - val_mae: 0.4434\n",
            "Epoch 162/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1734 - mae: 0.4719\n",
            "Epoch 162: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1734 - mae: 0.4719 - val_loss: 0.1569 - val_mae: 0.4440\n",
            "Epoch 163/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4732\n",
            "Epoch 163: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4706 - val_loss: 0.1569 - val_mae: 0.4444\n",
            "Epoch 164/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4695\n",
            "Epoch 164: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1729 - mae: 0.4702 - val_loss: 0.1577 - val_mae: 0.4449\n",
            "Epoch 165/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4695\n",
            "Epoch 165: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4704 - val_loss: 0.1558 - val_mae: 0.4432\n",
            "Epoch 166/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1724 - mae: 0.4698\n",
            "Epoch 166: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1731 - mae: 0.4710 - val_loss: 0.1565 - val_mae: 0.4433\n",
            "Epoch 167/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4726\n",
            "Epoch 167: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1731 - mae: 0.4714 - val_loss: 0.1569 - val_mae: 0.4438\n",
            "Epoch 168/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4708\n",
            "Epoch 168: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4706 - val_loss: 0.1571 - val_mae: 0.4435\n",
            "Epoch 169/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4701\n",
            "Epoch 169: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4707 - val_loss: 0.1556 - val_mae: 0.4416\n",
            "Epoch 170/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4715\n",
            "Epoch 170: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1730 - mae: 0.4703 - val_loss: 0.1571 - val_mae: 0.4448\n",
            "Epoch 171/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4703\n",
            "Epoch 171: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1730 - mae: 0.4704 - val_loss: 0.1580 - val_mae: 0.4460\n",
            "Epoch 172/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1740 - mae: 0.4730\n",
            "Epoch 172: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1732 - mae: 0.4714 - val_loss: 0.1558 - val_mae: 0.4422\n",
            "Epoch 173/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1724 - mae: 0.4704\n",
            "Epoch 173: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4705 - val_loss: 0.1559 - val_mae: 0.4422\n",
            "Epoch 174/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4706\n",
            "Epoch 174: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1730 - mae: 0.4704 - val_loss: 0.1575 - val_mae: 0.4447\n",
            "Epoch 175/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4711\n",
            "Epoch 175: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4707 - val_loss: 0.1572 - val_mae: 0.4449\n",
            "Epoch 176/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4722\n",
            "Epoch 176: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4708 - val_loss: 0.1563 - val_mae: 0.4427\n",
            "Epoch 177/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1735 - mae: 0.4718\n",
            "Epoch 177: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1731 - mae: 0.4710 - val_loss: 0.1573 - val_mae: 0.4443\n",
            "Epoch 178/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.4724\n",
            "Epoch 178: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1728 - mae: 0.4698 - val_loss: 0.1559 - val_mae: 0.4423\n",
            "Epoch 179/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1726 - mae: 0.4699\n",
            "Epoch 179: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4705 - val_loss: 0.1567 - val_mae: 0.4428\n",
            "Epoch 180/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4725\n",
            "Epoch 180: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1733 - mae: 0.4719 - val_loss: 0.1567 - val_mae: 0.4436\n",
            "Epoch 181/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4706\n",
            "Epoch 181: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1730 - mae: 0.4707 - val_loss: 0.1564 - val_mae: 0.4430\n",
            "Epoch 182/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4714\n",
            "Epoch 182: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 21ms/step - loss: 0.1726 - mae: 0.4704 - val_loss: 0.1566 - val_mae: 0.4435\n",
            "Epoch 183/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4716\n",
            "Epoch 183: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1729 - mae: 0.4704 - val_loss: 0.1573 - val_mae: 0.4441\n",
            "Epoch 184/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4704\n",
            "Epoch 184: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4701 - val_loss: 0.1570 - val_mae: 0.4443\n",
            "Epoch 185/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4702\n",
            "Epoch 185: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1730 - mae: 0.4708 - val_loss: 0.1559 - val_mae: 0.4426\n",
            "Epoch 186/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1727 - mae: 0.4705\n",
            "Epoch 186: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1727 - mae: 0.4705 - val_loss: 0.1565 - val_mae: 0.4431\n",
            "Epoch 187/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4721\n",
            "Epoch 187: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4701 - val_loss: 0.1571 - val_mae: 0.4443\n",
            "Epoch 188/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4696\n",
            "Epoch 188: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4705 - val_loss: 0.1563 - val_mae: 0.4427\n",
            "Epoch 189/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.4696\n",
            "Epoch 189: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4700 - val_loss: 0.1565 - val_mae: 0.4439\n",
            "Epoch 190/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4712\n",
            "Epoch 190: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1729 - mae: 0.4707 - val_loss: 0.1554 - val_mae: 0.4418\n",
            "Epoch 191/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4710\n",
            "Epoch 191: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1727 - mae: 0.4705 - val_loss: 0.1570 - val_mae: 0.4446\n",
            "Epoch 192/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1709 - mae: 0.4675\n",
            "Epoch 192: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4703 - val_loss: 0.1570 - val_mae: 0.4444\n",
            "Epoch 193/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1731 - mae: 0.4709\n",
            "Epoch 193: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1731 - mae: 0.4709 - val_loss: 0.1572 - val_mae: 0.4441\n",
            "Epoch 194/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4707\n",
            "Epoch 194: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1732 - mae: 0.4710 - val_loss: 0.1573 - val_mae: 0.4446\n",
            "Epoch 195/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4700\n",
            "Epoch 195: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4695 - val_loss: 0.1572 - val_mae: 0.4447\n",
            "Epoch 196/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1727 - mae: 0.4699\n",
            "Epoch 196: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1727 - mae: 0.4699 - val_loss: 0.1563 - val_mae: 0.4423\n",
            "Epoch 197/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1730 - mae: 0.4709\n",
            "Epoch 197: loss did not improve from 0.17214\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1730 - mae: 0.4709 - val_loss: 0.1563 - val_mae: 0.4429\n",
            "Epoch 198/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4698\n",
            "Epoch 198: loss improved from 0.17214 to 0.17211, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1721 - mae: 0.4697 - val_loss: 0.1557 - val_mae: 0.4422\n",
            "Epoch 199/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4722\n",
            "Epoch 199: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4701 - val_loss: 0.1558 - val_mae: 0.4423\n",
            "Epoch 200/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1729 - mae: 0.4704\n",
            "Epoch 200: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4704 - val_loss: 0.1582 - val_mae: 0.4465\n",
            "Epoch 201/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4705\n",
            "Epoch 201: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1725 - mae: 0.4700 - val_loss: 0.1565 - val_mae: 0.4429\n",
            "Epoch 202/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1714 - mae: 0.4687\n",
            "Epoch 202: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1728 - mae: 0.4707 - val_loss: 0.1573 - val_mae: 0.4445\n",
            "Epoch 203/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1719 - mae: 0.4676\n",
            "Epoch 203: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4694 - val_loss: 0.1574 - val_mae: 0.4450\n",
            "Epoch 204/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1729 - mae: 0.4706\n",
            "Epoch 204: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4706 - val_loss: 0.1565 - val_mae: 0.4433\n",
            "Epoch 205/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1717 - mae: 0.4692\n",
            "Epoch 205: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4701 - val_loss: 0.1572 - val_mae: 0.4444\n",
            "Epoch 206/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4715\n",
            "Epoch 206: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1726 - mae: 0.4701 - val_loss: 0.1577 - val_mae: 0.4454\n",
            "Epoch 207/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1729 - mae: 0.4706\n",
            "Epoch 207: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4706 - val_loss: 0.1558 - val_mae: 0.4420\n",
            "Epoch 208/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4711\n",
            "Epoch 208: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4701 - val_loss: 0.1563 - val_mae: 0.4433\n",
            "Epoch 209/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4740\n",
            "Epoch 209: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4706 - val_loss: 0.1556 - val_mae: 0.4424\n",
            "Epoch 210/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4704\n",
            "Epoch 210: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1728 - mae: 0.4702 - val_loss: 0.1580 - val_mae: 0.4457\n",
            "Epoch 211/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1726 - mae: 0.4704\n",
            "Epoch 211: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4704 - val_loss: 0.1554 - val_mae: 0.4412\n",
            "Epoch 212/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1719 - mae: 0.4686\n",
            "Epoch 212: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4700 - val_loss: 0.1567 - val_mae: 0.4437\n",
            "Epoch 213/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1722 - mae: 0.4704\n",
            "Epoch 213: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4698 - val_loss: 0.1572 - val_mae: 0.4452\n",
            "Epoch 214/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1721 - mae: 0.4700\n",
            "Epoch 214: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1726 - mae: 0.4708 - val_loss: 0.1567 - val_mae: 0.4435\n",
            "Epoch 215/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1730 - mae: 0.4705\n",
            "Epoch 215: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1728 - mae: 0.4698 - val_loss: 0.1572 - val_mae: 0.4443\n",
            "Epoch 216/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4720\n",
            "Epoch 216: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4712 - val_loss: 0.1570 - val_mae: 0.4436\n",
            "Epoch 217/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1725 - mae: 0.4705\n",
            "Epoch 217: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4695 - val_loss: 0.1560 - val_mae: 0.4433\n",
            "Epoch 218/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4710\n",
            "Epoch 218: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4705 - val_loss: 0.1577 - val_mae: 0.4457\n",
            "Epoch 219/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1698 - mae: 0.4668\n",
            "Epoch 219: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1725 - mae: 0.4702 - val_loss: 0.1550 - val_mae: 0.4412\n",
            "Epoch 220/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1722 - mae: 0.4685\n",
            "Epoch 220: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4698 - val_loss: 0.1568 - val_mae: 0.4431\n",
            "Epoch 221/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4694\n",
            "Epoch 221: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1721 - mae: 0.4696 - val_loss: 0.1576 - val_mae: 0.4452\n",
            "Epoch 222/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1718 - mae: 0.4692\n",
            "Epoch 222: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1728 - mae: 0.4701 - val_loss: 0.1569 - val_mae: 0.4439\n",
            "Epoch 223/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1698 - mae: 0.4667\n",
            "Epoch 223: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4706 - val_loss: 0.1563 - val_mae: 0.4434\n",
            "Epoch 224/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1721 - mae: 0.4688\n",
            "Epoch 224: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4705 - val_loss: 0.1575 - val_mae: 0.4448\n",
            "Epoch 225/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4728\n",
            "Epoch 225: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4704 - val_loss: 0.1562 - val_mae: 0.4430\n",
            "Epoch 226/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1716 - mae: 0.4689\n",
            "Epoch 226: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4703 - val_loss: 0.1566 - val_mae: 0.4442\n",
            "Epoch 227/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4709\n",
            "Epoch 227: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4697 - val_loss: 0.1571 - val_mae: 0.4447\n",
            "Epoch 228/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4698\n",
            "Epoch 228: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1727 - mae: 0.4700 - val_loss: 0.1575 - val_mae: 0.4446\n",
            "Epoch 229/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4732\n",
            "Epoch 229: loss did not improve from 0.17211\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1726 - mae: 0.4702 - val_loss: 0.1567 - val_mae: 0.4441\n",
            "Epoch 230/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1718 - mae: 0.4695\n",
            "Epoch 230: loss improved from 0.17211 to 0.17196, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4700 - val_loss: 0.1571 - val_mae: 0.4436\n",
            "Epoch 231/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4707\n",
            "Epoch 231: loss did not improve from 0.17196\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1730 - mae: 0.4709 - val_loss: 0.1573 - val_mae: 0.4444\n",
            "Epoch 232/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1705 - mae: 0.4683\n",
            "Epoch 232: loss improved from 0.17196 to 0.17180, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1718 - mae: 0.4698 - val_loss: 0.1577 - val_mae: 0.4442\n",
            "Epoch 233/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1713 - mae: 0.4678\n",
            "Epoch 233: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1731 - mae: 0.4704 - val_loss: 0.1576 - val_mae: 0.4443\n",
            "Epoch 234/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4718\n",
            "Epoch 234: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4707 - val_loss: 0.1554 - val_mae: 0.4413\n",
            "Epoch 235/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4698\n",
            "Epoch 235: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1721 - mae: 0.4694 - val_loss: 0.1562 - val_mae: 0.4423\n",
            "Epoch 236/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4703\n",
            "Epoch 236: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1731 - mae: 0.4706 - val_loss: 0.1556 - val_mae: 0.4419\n",
            "Epoch 237/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1727 - mae: 0.4700\n",
            "Epoch 237: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4700 - val_loss: 0.1562 - val_mae: 0.4437\n",
            "Epoch 238/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1702 - mae: 0.4674\n",
            "Epoch 238: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4708 - val_loss: 0.1557 - val_mae: 0.4420\n",
            "Epoch 239/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1722 - mae: 0.4686\n",
            "Epoch 239: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4697 - val_loss: 0.1555 - val_mae: 0.4414\n",
            "Epoch 240/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4713\n",
            "Epoch 240: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4704 - val_loss: 0.1552 - val_mae: 0.4415\n",
            "Epoch 241/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4713\n",
            "Epoch 241: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4701 - val_loss: 0.1574 - val_mae: 0.4443\n",
            "Epoch 242/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4700\n",
            "Epoch 242: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4703 - val_loss: 0.1567 - val_mae: 0.4431\n",
            "Epoch 243/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4720\n",
            "Epoch 243: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4698 - val_loss: 0.1570 - val_mae: 0.4441\n",
            "Epoch 244/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1714 - mae: 0.4683\n",
            "Epoch 244: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1727 - mae: 0.4702 - val_loss: 0.1569 - val_mae: 0.4438\n",
            "Epoch 245/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1724 - mae: 0.4709\n",
            "Epoch 245: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4708 - val_loss: 0.1562 - val_mae: 0.4425\n",
            "Epoch 246/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4704\n",
            "Epoch 246: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1729 - mae: 0.4702 - val_loss: 0.1573 - val_mae: 0.4442\n",
            "Epoch 247/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4705\n",
            "Epoch 247: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4704 - val_loss: 0.1568 - val_mae: 0.4432\n",
            "Epoch 248/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1706 - mae: 0.4682\n",
            "Epoch 248: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1719 - mae: 0.4696 - val_loss: 0.1577 - val_mae: 0.4445\n",
            "Epoch 249/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4705\n",
            "Epoch 249: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4702 - val_loss: 0.1574 - val_mae: 0.4446\n",
            "Epoch 250/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1700 - mae: 0.4663\n",
            "Epoch 250: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 0.1727 - mae: 0.4697 - val_loss: 0.1566 - val_mae: 0.4439\n",
            "Epoch 251/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4724\n",
            "Epoch 251: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4700 - val_loss: 0.1557 - val_mae: 0.4424\n",
            "Epoch 252/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4705\n",
            "Epoch 252: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4704 - val_loss: 0.1568 - val_mae: 0.4431\n",
            "Epoch 253/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.4697\n",
            "Epoch 253: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1726 - mae: 0.4697 - val_loss: 0.1559 - val_mae: 0.4426\n",
            "Epoch 254/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1719 - mae: 0.4684\n",
            "Epoch 254: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4702 - val_loss: 0.1556 - val_mae: 0.4419\n",
            "Epoch 255/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4701\n",
            "Epoch 255: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1727 - mae: 0.4704 - val_loss: 0.1553 - val_mae: 0.4415\n",
            "Epoch 256/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4731\n",
            "Epoch 256: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1728 - mae: 0.4706 - val_loss: 0.1571 - val_mae: 0.4446\n",
            "Epoch 257/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1721 - mae: 0.4696\n",
            "Epoch 257: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1721 - mae: 0.4696 - val_loss: 0.1570 - val_mae: 0.4432\n",
            "Epoch 258/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1721 - mae: 0.4689\n",
            "Epoch 258: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4703 - val_loss: 0.1550 - val_mae: 0.4408\n",
            "Epoch 259/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1726 - mae: 0.4705\n",
            "Epoch 259: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1728 - mae: 0.4704 - val_loss: 0.1569 - val_mae: 0.4441\n",
            "Epoch 260/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4715\n",
            "Epoch 260: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4695 - val_loss: 0.1561 - val_mae: 0.4426\n",
            "Epoch 261/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4700\n",
            "Epoch 261: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4693 - val_loss: 0.1575 - val_mae: 0.4450\n",
            "Epoch 262/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4706\n",
            "Epoch 262: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1729 - mae: 0.4709 - val_loss: 0.1570 - val_mae: 0.4431\n",
            "Epoch 263/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4688\n",
            "Epoch 263: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4688 - val_loss: 0.1577 - val_mae: 0.4449\n",
            "Epoch 264/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1721 - mae: 0.4707\n",
            "Epoch 264: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4708 - val_loss: 0.1549 - val_mae: 0.4410\n",
            "Epoch 265/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4702\n",
            "Epoch 265: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1721 - mae: 0.4693 - val_loss: 0.1550 - val_mae: 0.4410\n",
            "Epoch 266/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1725 - mae: 0.4699\n",
            "Epoch 266: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4699 - val_loss: 0.1576 - val_mae: 0.4450\n",
            "Epoch 267/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1722 - mae: 0.4698\n",
            "Epoch 267: loss did not improve from 0.17180\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1721 - mae: 0.4692 - val_loss: 0.1570 - val_mae: 0.4442\n",
            "Epoch 268/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.4678\n",
            "Epoch 268: loss improved from 0.17180 to 0.17145, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1714 - mae: 0.4681 - val_loss: 0.1551 - val_mae: 0.4412\n",
            "Epoch 269/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1746 - mae: 0.4733\n",
            "Epoch 269: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4700 - val_loss: 0.1562 - val_mae: 0.4424\n",
            "Epoch 270/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1709 - mae: 0.4669\n",
            "Epoch 270: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4698 - val_loss: 0.1553 - val_mae: 0.4415\n",
            "Epoch 271/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1740 - mae: 0.4720\n",
            "Epoch 271: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4692 - val_loss: 0.1561 - val_mae: 0.4427\n",
            "Epoch 272/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4703\n",
            "Epoch 272: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4699 - val_loss: 0.1570 - val_mae: 0.4439\n",
            "Epoch 273/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1699 - mae: 0.4662\n",
            "Epoch 273: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4696 - val_loss: 0.1578 - val_mae: 0.4453\n",
            "Epoch 274/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1719 - mae: 0.4696\n",
            "Epoch 274: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4702 - val_loss: 0.1570 - val_mae: 0.4440\n",
            "Epoch 275/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1724 - mae: 0.4690\n",
            "Epoch 275: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4690 - val_loss: 0.1565 - val_mae: 0.4439\n",
            "Epoch 276/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4704\n",
            "Epoch 276: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1720 - mae: 0.4695 - val_loss: 0.1575 - val_mae: 0.4448\n",
            "Epoch 277/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1724 - mae: 0.4698\n",
            "Epoch 277: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4698 - val_loss: 0.1572 - val_mae: 0.4448\n",
            "Epoch 278/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4693\n",
            "Epoch 278: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4693 - val_loss: 0.1549 - val_mae: 0.4404\n",
            "Epoch 279/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4714\n",
            "Epoch 279: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4698 - val_loss: 0.1572 - val_mae: 0.4443\n",
            "Epoch 280/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.4688\n",
            "Epoch 280: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4697 - val_loss: 0.1563 - val_mae: 0.4431\n",
            "Epoch 281/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1725 - mae: 0.4693\n",
            "Epoch 281: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4693 - val_loss: 0.1563 - val_mae: 0.4439\n",
            "Epoch 282/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1718 - mae: 0.4693\n",
            "Epoch 282: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4700 - val_loss: 0.1567 - val_mae: 0.4439\n",
            "Epoch 283/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4707\n",
            "Epoch 283: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1565 - val_mae: 0.4431\n",
            "Epoch 284/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1724 - mae: 0.4700\n",
            "Epoch 284: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4700 - val_loss: 0.1567 - val_mae: 0.4431\n",
            "Epoch 285/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1707 - mae: 0.4675\n",
            "Epoch 285: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1721 - mae: 0.4693 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 286/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1715 - mae: 0.4684\n",
            "Epoch 286: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1718 - mae: 0.4691 - val_loss: 0.1566 - val_mae: 0.4432\n",
            "Epoch 287/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1725 - mae: 0.4697\n",
            "Epoch 287: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4697 - val_loss: 0.1562 - val_mae: 0.4430\n",
            "Epoch 288/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4718\n",
            "Epoch 288: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4696 - val_loss: 0.1558 - val_mae: 0.4427\n",
            "Epoch 289/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4712\n",
            "Epoch 289: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4695 - val_loss: 0.1570 - val_mae: 0.4445\n",
            "Epoch 290/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1730 - mae: 0.4712\n",
            "Epoch 290: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4704 - val_loss: 0.1566 - val_mae: 0.4435\n",
            "Epoch 291/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1701 - mae: 0.4669\n",
            "Epoch 291: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4701 - val_loss: 0.1562 - val_mae: 0.4428\n",
            "Epoch 292/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1720 - mae: 0.4692\n",
            "Epoch 292: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1720 - mae: 0.4692 - val_loss: 0.1553 - val_mae: 0.4416\n",
            "Epoch 293/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1728 - mae: 0.4700\n",
            "Epoch 293: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4694 - val_loss: 0.1570 - val_mae: 0.4435\n",
            "Epoch 294/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1727 - mae: 0.4701\n",
            "Epoch 294: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1727 - mae: 0.4701 - val_loss: 0.1556 - val_mae: 0.4421\n",
            "Epoch 295/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4705\n",
            "Epoch 295: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4699 - val_loss: 0.1563 - val_mae: 0.4431\n",
            "Epoch 296/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1716 - mae: 0.4681\n",
            "Epoch 296: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4699 - val_loss: 0.1566 - val_mae: 0.4427\n",
            "Epoch 297/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1726 - mae: 0.4701\n",
            "Epoch 297: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4698 - val_loss: 0.1574 - val_mae: 0.4445\n",
            "Epoch 298/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1718 - mae: 0.4683\n",
            "Epoch 298: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4693 - val_loss: 0.1561 - val_mae: 0.4423\n",
            "Epoch 299/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4707\n",
            "Epoch 299: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1726 - mae: 0.4704 - val_loss: 0.1560 - val_mae: 0.4428\n",
            "Epoch 300/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1730 - mae: 0.4700\n",
            "Epoch 300: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4697 - val_loss: 0.1567 - val_mae: 0.4435\n",
            "Epoch 301/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4710\n",
            "Epoch 301: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4697 - val_loss: 0.1574 - val_mae: 0.4446\n",
            "Epoch 302/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4688\n",
            "Epoch 302: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4696 - val_loss: 0.1569 - val_mae: 0.4440\n",
            "Epoch 303/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1724 - mae: 0.4695\n",
            "Epoch 303: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4695 - val_loss: 0.1566 - val_mae: 0.4432\n",
            "Epoch 304/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1722 - mae: 0.4697\n",
            "Epoch 304: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4698 - val_loss: 0.1573 - val_mae: 0.4441\n",
            "Epoch 305/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1717 - mae: 0.4685\n",
            "Epoch 305: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4699 - val_loss: 0.1567 - val_mae: 0.4435\n",
            "Epoch 306/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1719 - mae: 0.4694\n",
            "Epoch 306: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4701 - val_loss: 0.1570 - val_mae: 0.4435\n",
            "Epoch 307/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1717 - mae: 0.4687\n",
            "Epoch 307: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1719 - mae: 0.4691 - val_loss: 0.1571 - val_mae: 0.4443\n",
            "Epoch 308/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1725 - mae: 0.4695\n",
            "Epoch 308: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4695 - val_loss: 0.1570 - val_mae: 0.4442\n",
            "Epoch 309/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1725 - mae: 0.4706\n",
            "Epoch 309: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4695 - val_loss: 0.1563 - val_mae: 0.4431\n",
            "Epoch 310/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1718 - mae: 0.4685\n",
            "Epoch 310: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4695 - val_loss: 0.1574 - val_mae: 0.4442\n",
            "Epoch 311/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1726 - mae: 0.4704\n",
            "Epoch 311: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4704 - val_loss: 0.1554 - val_mae: 0.4411\n",
            "Epoch 312/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1716 - mae: 0.4683\n",
            "Epoch 312: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4694 - val_loss: 0.1577 - val_mae: 0.4448\n",
            "Epoch 313/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.4697\n",
            "Epoch 313: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4695 - val_loss: 0.1567 - val_mae: 0.4433\n",
            "Epoch 314/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4712\n",
            "Epoch 314: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4697 - val_loss: 0.1563 - val_mae: 0.4422\n",
            "Epoch 315/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1728 - mae: 0.4705\n",
            "Epoch 315: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4697 - val_loss: 0.1556 - val_mae: 0.4420\n",
            "Epoch 316/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1721 - mae: 0.4689\n",
            "Epoch 316: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4689 - val_loss: 0.1572 - val_mae: 0.4443\n",
            "Epoch 317/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1717 - mae: 0.4692\n",
            "Epoch 317: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1721 - mae: 0.4694 - val_loss: 0.1574 - val_mae: 0.4451\n",
            "Epoch 318/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4694\n",
            "Epoch 318: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1561 - val_mae: 0.4427\n",
            "Epoch 319/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1725 - mae: 0.4693\n",
            "Epoch 319: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4691 - val_loss: 0.1571 - val_mae: 0.4434\n",
            "Epoch 320/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4706\n",
            "Epoch 320: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1719 - mae: 0.4690 - val_loss: 0.1556 - val_mae: 0.4415\n",
            "Epoch 321/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1709 - mae: 0.4679\n",
            "Epoch 321: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1715 - mae: 0.4687 - val_loss: 0.1555 - val_mae: 0.4418\n",
            "Epoch 322/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4709\n",
            "Epoch 322: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4695 - val_loss: 0.1567 - val_mae: 0.4434\n",
            "Epoch 323/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4688\n",
            "Epoch 323: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4698 - val_loss: 0.1572 - val_mae: 0.4443\n",
            "Epoch 324/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1713 - mae: 0.4673\n",
            "Epoch 324: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4687 - val_loss: 0.1563 - val_mae: 0.4426\n",
            "Epoch 325/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4697\n",
            "Epoch 325: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4697 - val_loss: 0.1571 - val_mae: 0.4449\n",
            "Epoch 326/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.4700\n",
            "Epoch 326: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1720 - mae: 0.4696 - val_loss: 0.1566 - val_mae: 0.4431\n",
            "Epoch 327/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.4689\n",
            "Epoch 327: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4697 - val_loss: 0.1568 - val_mae: 0.4431\n",
            "Epoch 328/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1695 - mae: 0.4661\n",
            "Epoch 328: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1719 - mae: 0.4693 - val_loss: 0.1577 - val_mae: 0.4454\n",
            "Epoch 329/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1709 - mae: 0.4684\n",
            "Epoch 329: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4698 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 330/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4674\n",
            "Epoch 330: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4694 - val_loss: 0.1564 - val_mae: 0.4434\n",
            "Epoch 331/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4705\n",
            "Epoch 331: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4690 - val_loss: 0.1575 - val_mae: 0.4448\n",
            "Epoch 332/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1720 - mae: 0.4691\n",
            "Epoch 332: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1720 - mae: 0.4691 - val_loss: 0.1576 - val_mae: 0.4455\n",
            "Epoch 333/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1717 - mae: 0.4685\n",
            "Epoch 333: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4698 - val_loss: 0.1548 - val_mae: 0.4412\n",
            "Epoch 334/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1724 - mae: 0.4700\n",
            "Epoch 334: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4700 - val_loss: 0.1567 - val_mae: 0.4430\n",
            "Epoch 335/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1709 - mae: 0.4667\n",
            "Epoch 335: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1721 - mae: 0.4690 - val_loss: 0.1563 - val_mae: 0.4423\n",
            "Epoch 336/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4696\n",
            "Epoch 336: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4698 - val_loss: 0.1561 - val_mae: 0.4423\n",
            "Epoch 337/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1717 - mae: 0.4686\n",
            "Epoch 337: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1717 - mae: 0.4686 - val_loss: 0.1571 - val_mae: 0.4444\n",
            "Epoch 338/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4715\n",
            "Epoch 338: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4697 - val_loss: 0.1562 - val_mae: 0.4422\n",
            "Epoch 339/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1713 - mae: 0.4679\n",
            "Epoch 339: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1726 - mae: 0.4701 - val_loss: 0.1568 - val_mae: 0.4430\n",
            "Epoch 340/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4692\n",
            "Epoch 340: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4692 - val_loss: 0.1563 - val_mae: 0.4424\n",
            "Epoch 341/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4730\n",
            "Epoch 341: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4696 - val_loss: 0.1562 - val_mae: 0.4424\n",
            "Epoch 342/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4706\n",
            "Epoch 342: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4689 - val_loss: 0.1567 - val_mae: 0.4438\n",
            "Epoch 343/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4739\n",
            "Epoch 343: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1726 - mae: 0.4705 - val_loss: 0.1571 - val_mae: 0.4436\n",
            "Epoch 344/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4694\n",
            "Epoch 344: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4694 - val_loss: 0.1551 - val_mae: 0.4417\n",
            "Epoch 345/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1721 - mae: 0.4693\n",
            "Epoch 345: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4693 - val_loss: 0.1554 - val_mae: 0.4415\n",
            "Epoch 346/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1716 - mae: 0.4685\n",
            "Epoch 346: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1724 - mae: 0.4697 - val_loss: 0.1580 - val_mae: 0.4455\n",
            "Epoch 347/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1710 - mae: 0.4678\n",
            "Epoch 347: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1726 - mae: 0.4700 - val_loss: 0.1572 - val_mae: 0.4445\n",
            "Epoch 348/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1719 - mae: 0.4695\n",
            "Epoch 348: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.1720 - mae: 0.4692 - val_loss: 0.1552 - val_mae: 0.4414\n",
            "Epoch 349/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.4679\n",
            "Epoch 349: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1725 - mae: 0.4699 - val_loss: 0.1576 - val_mae: 0.4450\n",
            "Epoch 350/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1702 - mae: 0.4668\n",
            "Epoch 350: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1720 - mae: 0.4690 - val_loss: 0.1563 - val_mae: 0.4430\n",
            "Epoch 351/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4695\n",
            "Epoch 351: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1723 - mae: 0.4695 - val_loss: 0.1550 - val_mae: 0.4408\n",
            "Epoch 352/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1712 - mae: 0.4680\n",
            "Epoch 352: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1719 - mae: 0.4689 - val_loss: 0.1570 - val_mae: 0.4442\n",
            "Epoch 353/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.4684\n",
            "Epoch 353: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1724 - mae: 0.4694 - val_loss: 0.1564 - val_mae: 0.4427\n",
            "Epoch 354/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1708 - mae: 0.4680\n",
            "Epoch 354: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1720 - mae: 0.4693 - val_loss: 0.1577 - val_mae: 0.4451\n",
            "Epoch 355/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4705\n",
            "Epoch 355: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1720 - mae: 0.4690 - val_loss: 0.1562 - val_mae: 0.4429\n",
            "Epoch 356/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1713 - mae: 0.4686\n",
            "Epoch 356: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4697 - val_loss: 0.1564 - val_mae: 0.4425\n",
            "Epoch 357/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1718 - mae: 0.4699\n",
            "Epoch 357: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4696 - val_loss: 0.1558 - val_mae: 0.4422\n",
            "Epoch 358/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1721 - mae: 0.4690\n",
            "Epoch 358: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4690 - val_loss: 0.1548 - val_mae: 0.4410\n",
            "Epoch 359/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1727 - mae: 0.4702\n",
            "Epoch 359: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4699 - val_loss: 0.1575 - val_mae: 0.4443\n",
            "Epoch 360/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1721 - mae: 0.4698\n",
            "Epoch 360: loss did not improve from 0.17145\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4691 - val_loss: 0.1558 - val_mae: 0.4424\n",
            "Epoch 361/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1705 - mae: 0.4667\n",
            "Epoch 361: loss improved from 0.17145 to 0.17107, saving model to poids_entrainement.hdf5\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1711 - mae: 0.4677 - val_loss: 0.1571 - val_mae: 0.4441\n",
            "Epoch 362/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1718 - mae: 0.4690\n",
            "Epoch 362: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1717 - mae: 0.4687 - val_loss: 0.1560 - val_mae: 0.4424\n",
            "Epoch 363/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4689\n",
            "Epoch 363: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4689 - val_loss: 0.1577 - val_mae: 0.4450\n",
            "Epoch 364/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4695\n",
            "Epoch 364: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4695 - val_loss: 0.1572 - val_mae: 0.4439\n",
            "Epoch 365/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1710 - mae: 0.4679\n",
            "Epoch 365: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1720 - mae: 0.4691 - val_loss: 0.1575 - val_mae: 0.4451\n",
            "Epoch 366/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1711 - mae: 0.4675\n",
            "Epoch 366: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1719 - mae: 0.4687 - val_loss: 0.1568 - val_mae: 0.4438\n",
            "Epoch 367/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1708 - mae: 0.4669\n",
            "Epoch 367: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1719 - mae: 0.4690 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 368/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4696\n",
            "Epoch 368: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1723 - mae: 0.4696 - val_loss: 0.1569 - val_mae: 0.4436\n",
            "Epoch 369/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4695\n",
            "Epoch 369: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1722 - mae: 0.4695 - val_loss: 0.1568 - val_mae: 0.4432\n",
            "Epoch 370/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1720 - mae: 0.4688\n",
            "Epoch 370: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4688 - val_loss: 0.1571 - val_mae: 0.4439\n",
            "Epoch 371/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1721 - mae: 0.4685\n",
            "Epoch 371: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4692 - val_loss: 0.1562 - val_mae: 0.4423\n",
            "Epoch 372/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.4696\n",
            "Epoch 372: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4695 - val_loss: 0.1561 - val_mae: 0.4425\n",
            "Epoch 373/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4690\n",
            "Epoch 373: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4690 - val_loss: 0.1567 - val_mae: 0.4435\n",
            "Epoch 374/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1714 - mae: 0.4682\n",
            "Epoch 374: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4699 - val_loss: 0.1575 - val_mae: 0.4444\n",
            "Epoch 375/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1718 - mae: 0.4682\n",
            "Epoch 375: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1719 - mae: 0.4689 - val_loss: 0.1570 - val_mae: 0.4439\n",
            "Epoch 376/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4711\n",
            "Epoch 376: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1721 - mae: 0.4688 - val_loss: 0.1566 - val_mae: 0.4431\n",
            "Epoch 377/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4688\n",
            "Epoch 377: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4695 - val_loss: 0.1580 - val_mae: 0.4461\n",
            "Epoch 378/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1724 - mae: 0.4695\n",
            "Epoch 378: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1724 - mae: 0.4695 - val_loss: 0.1567 - val_mae: 0.4434\n",
            "Epoch 379/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1732 - mae: 0.4720\n",
            "Epoch 379: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1724 - mae: 0.4699 - val_loss: 0.1557 - val_mae: 0.4416\n",
            "Epoch 380/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4703\n",
            "Epoch 380: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1724 - mae: 0.4696 - val_loss: 0.1557 - val_mae: 0.4420\n",
            "Epoch 381/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1721 - mae: 0.4689\n",
            "Epoch 381: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1718 - mae: 0.4689 - val_loss: 0.1567 - val_mae: 0.4429\n",
            "Epoch 382/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4696\n",
            "Epoch 382: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4696 - val_loss: 0.1562 - val_mae: 0.4426\n",
            "Epoch 383/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1721 - mae: 0.4690\n",
            "Epoch 383: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4687 - val_loss: 0.1568 - val_mae: 0.4435\n",
            "Epoch 384/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1724 - mae: 0.4696\n",
            "Epoch 384: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4696 - val_loss: 0.1566 - val_mae: 0.4436\n",
            "Epoch 385/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4694\n",
            "Epoch 385: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1723 - mae: 0.4694 - val_loss: 0.1577 - val_mae: 0.4450\n",
            "Epoch 386/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1721 - mae: 0.4695\n",
            "Epoch 386: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4695 - val_loss: 0.1574 - val_mae: 0.4441\n",
            "Epoch 387/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1725 - mae: 0.4704\n",
            "Epoch 387: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4704 - val_loss: 0.1571 - val_mae: 0.4439\n",
            "Epoch 388/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1742 - mae: 0.4722\n",
            "Epoch 388: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4694 - val_loss: 0.1577 - val_mae: 0.4446\n",
            "Epoch 389/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4711\n",
            "Epoch 389: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4688 - val_loss: 0.1578 - val_mae: 0.4452\n",
            "Epoch 390/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4736\n",
            "Epoch 390: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1565 - val_mae: 0.4428\n",
            "Epoch 391/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4702\n",
            "Epoch 391: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4693 - val_loss: 0.1560 - val_mae: 0.4426\n",
            "Epoch 392/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4698\n",
            "Epoch 392: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4692 - val_loss: 0.1556 - val_mae: 0.4420\n",
            "Epoch 393/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1728 - mae: 0.4704\n",
            "Epoch 393: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1719 - mae: 0.4690 - val_loss: 0.1565 - val_mae: 0.4431\n",
            "Epoch 394/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4691\n",
            "Epoch 394: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4692 - val_loss: 0.1565 - val_mae: 0.4424\n",
            "Epoch 395/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1729 - mae: 0.4704\n",
            "Epoch 395: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4695 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 396/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1728 - mae: 0.4696\n",
            "Epoch 396: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4694 - val_loss: 0.1571 - val_mae: 0.4437\n",
            "Epoch 397/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1721 - mae: 0.4689\n",
            "Epoch 397: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4689 - val_loss: 0.1577 - val_mae: 0.4451\n",
            "Epoch 398/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1719 - mae: 0.4690\n",
            "Epoch 398: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4695 - val_loss: 0.1550 - val_mae: 0.4407\n",
            "Epoch 399/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1723 - mae: 0.4697\n",
            "Epoch 399: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1558 - val_mae: 0.4421\n",
            "Epoch 400/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4691\n",
            "Epoch 400: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4691 - val_loss: 0.1574 - val_mae: 0.4444\n",
            "Epoch 401/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1720 - mae: 0.4688\n",
            "Epoch 401: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4688 - val_loss: 0.1560 - val_mae: 0.4425\n",
            "Epoch 402/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1717 - mae: 0.4683\n",
            "Epoch 402: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1717 - mae: 0.4684 - val_loss: 0.1560 - val_mae: 0.4416\n",
            "Epoch 403/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1726 - mae: 0.4699\n",
            "Epoch 403: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4696 - val_loss: 0.1565 - val_mae: 0.4433\n",
            "Epoch 404/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4712\n",
            "Epoch 404: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4696 - val_loss: 0.1573 - val_mae: 0.4440\n",
            "Epoch 405/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1720 - mae: 0.4692\n",
            "Epoch 405: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1720 - mae: 0.4692 - val_loss: 0.1562 - val_mae: 0.4425\n",
            "Epoch 406/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1715 - mae: 0.4687\n",
            "Epoch 406: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1551 - val_mae: 0.4411\n",
            "Epoch 407/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.4692\n",
            "Epoch 407: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4691 - val_loss: 0.1581 - val_mae: 0.4456\n",
            "Epoch 408/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1709 - mae: 0.4674\n",
            "Epoch 408: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1718 - mae: 0.4689 - val_loss: 0.1565 - val_mae: 0.4433\n",
            "Epoch 409/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4706\n",
            "Epoch 409: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1559 - val_mae: 0.4413\n",
            "Epoch 410/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4720\n",
            "Epoch 410: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1719 - mae: 0.4689 - val_loss: 0.1568 - val_mae: 0.4433\n",
            "Epoch 411/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4693\n",
            "Epoch 411: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4693 - val_loss: 0.1569 - val_mae: 0.4429\n",
            "Epoch 412/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4728\n",
            "Epoch 412: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1724 - mae: 0.4695 - val_loss: 0.1567 - val_mae: 0.4439\n",
            "Epoch 413/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1716 - mae: 0.4684\n",
            "Epoch 413: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1715 - mae: 0.4684 - val_loss: 0.1559 - val_mae: 0.4421\n",
            "Epoch 414/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1718 - mae: 0.4695\n",
            "Epoch 414: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1717 - mae: 0.4690 - val_loss: 0.1556 - val_mae: 0.4410\n",
            "Epoch 415/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1729 - mae: 0.4691\n",
            "Epoch 415: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1719 - mae: 0.4684 - val_loss: 0.1567 - val_mae: 0.4436\n",
            "Epoch 416/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4694\n",
            "Epoch 416: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4695 - val_loss: 0.1570 - val_mae: 0.4438\n",
            "Epoch 417/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1718 - mae: 0.4691\n",
            "Epoch 417: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1718 - mae: 0.4691 - val_loss: 0.1561 - val_mae: 0.4427\n",
            "Epoch 418/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1721 - mae: 0.4689\n",
            "Epoch 418: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4689 - val_loss: 0.1577 - val_mae: 0.4452\n",
            "Epoch 419/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4699\n",
            "Epoch 419: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4692 - val_loss: 0.1560 - val_mae: 0.4411\n",
            "Epoch 420/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4696\n",
            "Epoch 420: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4696 - val_loss: 0.1560 - val_mae: 0.4422\n",
            "Epoch 421/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1734 - mae: 0.4702\n",
            "Epoch 421: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4695 - val_loss: 0.1564 - val_mae: 0.4432\n",
            "Epoch 422/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1719 - mae: 0.4687\n",
            "Epoch 422: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4686 - val_loss: 0.1581 - val_mae: 0.4457\n",
            "Epoch 423/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4691\n",
            "Epoch 423: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1723 - mae: 0.4698 - val_loss: 0.1556 - val_mae: 0.4416\n",
            "Epoch 424/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1716 - mae: 0.4680\n",
            "Epoch 424: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4691 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 425/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1726 - mae: 0.4698\n",
            "Epoch 425: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 21ms/step - loss: 0.1723 - mae: 0.4694 - val_loss: 0.1552 - val_mae: 0.4414\n",
            "Epoch 426/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1718 - mae: 0.4685\n",
            "Epoch 426: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1718 - mae: 0.4685 - val_loss: 0.1565 - val_mae: 0.4431\n",
            "Epoch 427/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4733\n",
            "Epoch 427: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1723 - mae: 0.4694 - val_loss: 0.1566 - val_mae: 0.4435\n",
            "Epoch 428/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1725 - mae: 0.4703\n",
            "Epoch 428: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4703 - val_loss: 0.1569 - val_mae: 0.4434\n",
            "Epoch 429/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1726 - mae: 0.4696\n",
            "Epoch 429: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4689 - val_loss: 0.1566 - val_mae: 0.4430\n",
            "Epoch 430/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1728 - mae: 0.4712\n",
            "Epoch 430: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4697 - val_loss: 0.1574 - val_mae: 0.4439\n",
            "Epoch 431/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.4690\n",
            "Epoch 431: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4692 - val_loss: 0.1556 - val_mae: 0.4410\n",
            "Epoch 432/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4690\n",
            "Epoch 432: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4690 - val_loss: 0.1565 - val_mae: 0.4438\n",
            "Epoch 433/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4707\n",
            "Epoch 433: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4696 - val_loss: 0.1572 - val_mae: 0.4449\n",
            "Epoch 434/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.4684\n",
            "Epoch 434: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1723 - mae: 0.4692 - val_loss: 0.1562 - val_mae: 0.4428\n",
            "Epoch 435/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1703 - mae: 0.4669\n",
            "Epoch 435: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1725 - mae: 0.4702 - val_loss: 0.1570 - val_mae: 0.4434\n",
            "Epoch 436/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1724 - mae: 0.4697\n",
            "Epoch 436: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4690 - val_loss: 0.1559 - val_mae: 0.4420\n",
            "Epoch 437/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1734 - mae: 0.4715\n",
            "Epoch 437: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4695 - val_loss: 0.1572 - val_mae: 0.4443\n",
            "Epoch 438/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1721 - mae: 0.4688\n",
            "Epoch 438: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4688 - val_loss: 0.1569 - val_mae: 0.4436\n",
            "Epoch 439/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4695\n",
            "Epoch 439: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1572 - val_mae: 0.4438\n",
            "Epoch 440/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4698\n",
            "Epoch 440: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4687 - val_loss: 0.1568 - val_mae: 0.4439\n",
            "Epoch 441/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1719 - mae: 0.4686\n",
            "Epoch 441: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4698 - val_loss: 0.1551 - val_mae: 0.4409\n",
            "Epoch 442/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1721 - mae: 0.4693\n",
            "Epoch 442: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4690 - val_loss: 0.1566 - val_mae: 0.4432\n",
            "Epoch 443/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4697\n",
            "Epoch 443: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4697 - val_loss: 0.1574 - val_mae: 0.4442\n",
            "Epoch 444/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1716 - mae: 0.4676\n",
            "Epoch 444: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4687 - val_loss: 0.1570 - val_mae: 0.4433\n",
            "Epoch 445/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4694\n",
            "Epoch 445: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 446/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1719 - mae: 0.4696\n",
            "Epoch 446: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4688 - val_loss: 0.1565 - val_mae: 0.4427\n",
            "Epoch 447/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4720\n",
            "Epoch 447: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1572 - val_mae: 0.4437\n",
            "Epoch 448/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4692\n",
            "Epoch 448: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4692 - val_loss: 0.1570 - val_mae: 0.4444\n",
            "Epoch 449/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4685\n",
            "Epoch 449: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1719 - mae: 0.4691 - val_loss: 0.1566 - val_mae: 0.4434\n",
            "Epoch 450/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1720 - mae: 0.4690\n",
            "Epoch 450: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1720 - mae: 0.4690 - val_loss: 0.1553 - val_mae: 0.4411\n",
            "Epoch 451/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1733 - mae: 0.4713\n",
            "Epoch 451: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4692 - val_loss: 0.1550 - val_mae: 0.4405\n",
            "Epoch 452/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1733 - mae: 0.4711\n",
            "Epoch 452: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1720 - mae: 0.4684 - val_loss: 0.1570 - val_mae: 0.4444\n",
            "Epoch 453/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1712 - mae: 0.4681\n",
            "Epoch 453: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1721 - mae: 0.4691 - val_loss: 0.1558 - val_mae: 0.4419\n",
            "Epoch 454/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1709 - mae: 0.4680\n",
            "Epoch 454: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1718 - mae: 0.4690 - val_loss: 0.1566 - val_mae: 0.4430\n",
            "Epoch 455/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4694\n",
            "Epoch 455: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4689 - val_loss: 0.1560 - val_mae: 0.4421\n",
            "Epoch 456/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1710 - mae: 0.4679\n",
            "Epoch 456: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1716 - mae: 0.4683 - val_loss: 0.1552 - val_mae: 0.4413\n",
            "Epoch 457/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4694\n",
            "Epoch 457: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1714 - mae: 0.4681 - val_loss: 0.1560 - val_mae: 0.4425\n",
            "Epoch 458/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1707 - mae: 0.4673\n",
            "Epoch 458: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1718 - mae: 0.4686 - val_loss: 0.1567 - val_mae: 0.4429\n",
            "Epoch 459/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1734 - mae: 0.4699\n",
            "Epoch 459: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4690 - val_loss: 0.1567 - val_mae: 0.4435\n",
            "Epoch 460/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4690\n",
            "Epoch 460: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4689 - val_loss: 0.1556 - val_mae: 0.4417\n",
            "Epoch 461/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4699\n",
            "Epoch 461: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1723 - mae: 0.4699 - val_loss: 0.1568 - val_mae: 0.4436\n",
            "Epoch 462/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1713 - mae: 0.4677\n",
            "Epoch 462: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1719 - mae: 0.4687 - val_loss: 0.1578 - val_mae: 0.4450\n",
            "Epoch 463/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4691\n",
            "Epoch 463: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4691 - val_loss: 0.1568 - val_mae: 0.4436\n",
            "Epoch 464/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4699\n",
            "Epoch 464: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1560 - val_mae: 0.4428\n",
            "Epoch 465/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4700\n",
            "Epoch 465: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1723 - mae: 0.4692 - val_loss: 0.1573 - val_mae: 0.4441\n",
            "Epoch 466/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4693\n",
            "Epoch 466: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4693 - val_loss: 0.1567 - val_mae: 0.4444\n",
            "Epoch 467/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4692\n",
            "Epoch 467: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1723 - mae: 0.4692 - val_loss: 0.1576 - val_mae: 0.4444\n",
            "Epoch 468/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1721 - mae: 0.4693\n",
            "Epoch 468: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4693 - val_loss: 0.1559 - val_mae: 0.4423\n",
            "Epoch 469/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1708 - mae: 0.4672\n",
            "Epoch 469: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1719 - mae: 0.4687 - val_loss: 0.1568 - val_mae: 0.4433\n",
            "Epoch 470/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1722 - mae: 0.4693\n",
            "Epoch 470: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1722 - mae: 0.4691 - val_loss: 0.1575 - val_mae: 0.4447\n",
            "Epoch 471/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1717 - mae: 0.4695\n",
            "Epoch 471: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1716 - mae: 0.4689 - val_loss: 0.1569 - val_mae: 0.4433\n",
            "Epoch 472/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1728 - mae: 0.4699\n",
            "Epoch 472: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4690 - val_loss: 0.1569 - val_mae: 0.4435\n",
            "Epoch 473/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1722 - mae: 0.4694\n",
            "Epoch 473: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4694 - val_loss: 0.1566 - val_mae: 0.4434\n",
            "Epoch 474/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1720 - mae: 0.4686\n",
            "Epoch 474: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1715 - mae: 0.4683 - val_loss: 0.1570 - val_mae: 0.4438\n",
            "Epoch 475/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1714 - mae: 0.4682\n",
            "Epoch 475: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1721 - mae: 0.4692 - val_loss: 0.1575 - val_mae: 0.4447\n",
            "Epoch 476/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1714 - mae: 0.4679\n",
            "Epoch 476: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1721 - mae: 0.4690 - val_loss: 0.1573 - val_mae: 0.4442\n",
            "Epoch 477/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1727 - mae: 0.4704\n",
            "Epoch 477: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1719 - mae: 0.4688 - val_loss: 0.1554 - val_mae: 0.4416\n",
            "Epoch 478/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4709\n",
            "Epoch 478: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4692 - val_loss: 0.1565 - val_mae: 0.4431\n",
            "Epoch 479/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1719 - mae: 0.4685\n",
            "Epoch 479: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1719 - mae: 0.4685 - val_loss: 0.1547 - val_mae: 0.4401\n",
            "Epoch 480/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1709 - mae: 0.4675\n",
            "Epoch 480: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4694 - val_loss: 0.1563 - val_mae: 0.4425\n",
            "Epoch 481/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1705 - mae: 0.4667\n",
            "Epoch 481: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4690 - val_loss: 0.1576 - val_mae: 0.4440\n",
            "Epoch 482/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1717 - mae: 0.4684\n",
            "Epoch 482: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1717 - mae: 0.4684 - val_loss: 0.1581 - val_mae: 0.4456\n",
            "Epoch 483/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1723 - mae: 0.4698\n",
            "Epoch 483: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1723 - mae: 0.4698 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 484/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1715 - mae: 0.4679\n",
            "Epoch 484: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1715 - mae: 0.4679 - val_loss: 0.1558 - val_mae: 0.4418\n",
            "Epoch 485/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1710 - mae: 0.4675\n",
            "Epoch 485: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1718 - mae: 0.4687 - val_loss: 0.1567 - val_mae: 0.4432\n",
            "Epoch 486/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1720 - mae: 0.4685\n",
            "Epoch 486: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1720 - mae: 0.4685 - val_loss: 0.1555 - val_mae: 0.4420\n",
            "Epoch 487/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1717 - mae: 0.4685\n",
            "Epoch 487: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1717 - mae: 0.4685 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 488/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1731 - mae: 0.4714\n",
            "Epoch 488: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.1719 - mae: 0.4691 - val_loss: 0.1563 - val_mae: 0.4427\n",
            "Epoch 489/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1720 - mae: 0.4687\n",
            "Epoch 489: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1716 - mae: 0.4681 - val_loss: 0.1556 - val_mae: 0.4412\n",
            "Epoch 490/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1723 - mae: 0.4696\n",
            "Epoch 490: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4693 - val_loss: 0.1552 - val_mae: 0.4410\n",
            "Epoch 491/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1725 - mae: 0.4692\n",
            "Epoch 491: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4689 - val_loss: 0.1561 - val_mae: 0.4426\n",
            "Epoch 492/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1722 - mae: 0.4694\n",
            "Epoch 492: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1722 - mae: 0.4691 - val_loss: 0.1569 - val_mae: 0.4441\n",
            "Epoch 493/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1718 - mae: 0.4687\n",
            "Epoch 493: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1718 - mae: 0.4687 - val_loss: 0.1559 - val_mae: 0.4417\n",
            "Epoch 494/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1716 - mae: 0.4686\n",
            "Epoch 494: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1718 - mae: 0.4686 - val_loss: 0.1563 - val_mae: 0.4426\n",
            "Epoch 495/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1726 - mae: 0.4695\n",
            "Epoch 495: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 0.1717 - mae: 0.4682 - val_loss: 0.1568 - val_mae: 0.4430\n",
            "Epoch 496/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1730 - mae: 0.4700\n",
            "Epoch 496: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4692 - val_loss: 0.1567 - val_mae: 0.4430\n",
            "Epoch 497/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1715 - mae: 0.4680\n",
            "Epoch 497: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.1714 - mae: 0.4678 - val_loss: 0.1553 - val_mae: 0.4410\n",
            "Epoch 498/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1684 - mae: 0.4640\n",
            "Epoch 498: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1721 - mae: 0.4686 - val_loss: 0.1568 - val_mae: 0.4440\n",
            "Epoch 499/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1717 - mae: 0.4688\n",
            "Epoch 499: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1718 - mae: 0.4688 - val_loss: 0.1564 - val_mae: 0.4424\n",
            "Epoch 500/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1725 - mae: 0.4704\n",
            "Epoch 500: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.1720 - mae: 0.4687 - val_loss: 0.1555 - val_mae: 0.4410\n",
            "Step time : 0.023\n",
            "Total time : 783.429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "Serie_Normalisee = np.array(Serie_Normalisee)\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "metadata": {
        "id": "FwjvFRNlnuI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTM bidirectionnnel')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTM bidirectionnnel (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "j_V6g7mLnuE6",
        "outputId": "50330896-39d4-415c-e6b5-fb24c8b99d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcule de l'erreur quadratique moyenne et de l'erreur absolue moyenne \n",
        "\n",
        "mae = tf.keras.metrics.mean_absolute_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "\n",
        "print(mae)\n",
        "print(mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1HKKbdfndr9",
        "outputId": "07fb4bcb-c5d2-4bd2-eebc-58d427d28a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8030996831356954\n",
            "5.365732406941697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZyF9J7u65lm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choix du modèle :"
      ],
      "metadata": {
        "id": "fax4_Sx_v3Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **modèle LSTM choisi**                              \t| **metrique mse et mae**         \t|                         **Observations**                        \t| **Caractéristiques du modèle**                                                                                                                   \t|   \t|\n",
        "|-----------------------------------------------------\t|---------------------------------\t|:---------------------------------------------------------------:\t|--------------------------------------------------------------------------------------------------------------------------------------------------\t|---\t|\n",
        "| **Le modèle de base**                               \t| ###########################     \t|                     #######################                     \t| - une couche récurrente à 40 neurones<br>- une couche dense avec 40 neurones (activation tanh)<br>- une couche dense avec 1 neurone (générateur) \t|   \t|\n",
        "| **LSTM avec sélection <br>du taux d'apprentissage** \t| unitile à cause de l'overfiting \t|                  Le modèle fait de l'overfiting                 \t|                                                                                                                                                  \t|   \t|\n",
        "| **LSTM avec adaptation<br>du taux d'apprentissage** \t| - mse = 1.808<br>- mae = 5.35   \t| on fait décroissance continue <br>du taux d'apprentissage de 1% \t|                                                                                                                                                  \t|   \t|\n",
        "| **LSTM avec ajout de la<br>régularisation**         \t| - mse = 1.801<br>- mae = 5.322  \t|     Le modèle a des performances<br>similaires au précédent     \t|                                                                                                                                                  \t|   \t|\n",
        "| **LSTM Bi-directionnel**                            \t| - mse = 1.803<br>- mae = 5.365  \t| Le modèle a des performances <br>similaires au précédent        \t|                                                                                                                                                  \t|   \t|"
      ],
      "metadata": {
        "id": "DrUDNQ4C8o1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONCLUSION : **Le modèle retenue est le LSTM Bi-Directionnel** "
      ],
      "metadata": {
        "id": "7XCFWmOI9Rux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JxQh0bxr97Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Evaluation du modèle retenu sur les années avec le mse"
      ],
      "metadata": {
        "id": "Fi0rVbkS95lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**On crée une boucle qui va nous permettre d'obtenir tout les résultats**"
      ],
      "metadata": {
        "id": "UoBEGvhrjfPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MyList = []\n",
        "for i in range(1,10):\n",
        "  valeur = 30 * 3 * i\n",
        "  MyList.append(valeur)\n",
        "\n",
        "for k in MyList:\n",
        "  temps_separation = 2201\n",
        "  temps_1987_3Mois = temps_separation + k + 1\n",
        "\n",
        "  # Extraction des temps et des données d'entrainement\n",
        "  temps_entrainement = temps[:temps_separation]\n",
        "  x_entrainement = serie[:temps_separation]\n",
        "\n",
        "  # Exctraction des temps et des données de validation\n",
        "  temps_test = temps[temps_separation:temps_1987_3Mois]\n",
        "  x_test = serie[temps_separation:]\n",
        "\n",
        "  # Chargement des poids sauvegardés\n",
        "  model.load_weights(\"poids.hdf5\")\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "  taille_fenetre = 20\n",
        "\n",
        "  # Création d'une liste vide pour recevoir les prédictions\n",
        "  predictions = []\n",
        "  Serie_Normalisee = np.array(Serie_Normalisee)\n",
        "\n",
        "  # Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "  # dans l'intervalle de validation\n",
        "  for t in temps[temps_separation:-taille_fenetre]:\n",
        "      X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "      predictions.append(model.predict(X))\n",
        "\n",
        "  mse = tf.keras.metrics.mean_squared_error(serie[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0]).numpy()\n",
        "  print(\"La valeur du mse obtenue \", round(k/(3*30)), \" trimestres plus tard est \", round(mse, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fss5tdqAhFDQ",
        "outputId": "551390f6-7e68-492d-d05b-bfecc0c27243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 252/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4742\n",
            "Epoch 252: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1587 - val_mae: 0.4461\n",
            "Epoch 253/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 253: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4459\n",
            "Epoch 254/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 254: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1589 - val_mae: 0.4467\n",
            "Epoch 255/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4727\n",
            "Epoch 255: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1750 - mae: 0.4736 - val_loss: 0.1567 - val_mae: 0.4427\n",
            "Epoch 256/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4751\n",
            "Epoch 256: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1571 - val_mae: 0.4435\n",
            "Epoch 257/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4754\n",
            "Epoch 257: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4753 - val_loss: 0.1568 - val_mae: 0.4435\n",
            "Epoch 258/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 258: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1581 - val_mae: 0.4460\n",
            "Epoch 259/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4752\n",
            "Epoch 259: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1568 - val_mae: 0.4432\n",
            "Epoch 260/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 260: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1581 - val_mae: 0.4453\n",
            "Epoch 261/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1773 - mae: 0.4763\n",
            "Epoch 261: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1571 - val_mae: 0.4437\n",
            "Epoch 262/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4764\n",
            "Epoch 262: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4754 - val_loss: 0.1590 - val_mae: 0.4473\n",
            "Epoch 263/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4717\n",
            "Epoch 263: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1572 - val_mae: 0.4443\n",
            "Epoch 264/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4736\n",
            "Epoch 264: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1582 - val_mae: 0.4455\n",
            "Epoch 265/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1774 - mae: 0.4777\n",
            "Epoch 265: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1577 - val_mae: 0.4448\n",
            "Epoch 266/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4761\n",
            "Epoch 266: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1582 - val_mae: 0.4459\n",
            "Epoch 267/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 267: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1577 - val_mae: 0.4444\n",
            "Epoch 268/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4756\n",
            "Epoch 268: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1591 - val_mae: 0.4471\n",
            "Epoch 269/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4765\n",
            "Epoch 269: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1567 - val_mae: 0.4433\n",
            "Epoch 270/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4761\n",
            "Epoch 270: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1591 - val_mae: 0.4473\n",
            "Epoch 271/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4763\n",
            "Epoch 271: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4754 - val_loss: 0.1554 - val_mae: 0.4414\n",
            "Epoch 272/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4739\n",
            "Epoch 272: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1583 - val_mae: 0.4452\n",
            "Epoch 273/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4760\n",
            "Epoch 273: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1577 - val_mae: 0.4452\n",
            "Epoch 274/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4757\n",
            "Epoch 274: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1572 - val_mae: 0.4440\n",
            "Epoch 275/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4754\n",
            "Epoch 275: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4451\n",
            "Epoch 276/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 276: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 277/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 277: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1566 - val_mae: 0.4437\n",
            "Epoch 278/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1774 - mae: 0.4768\n",
            "Epoch 278: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1573 - val_mae: 0.4439\n",
            "Epoch 279/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4757\n",
            "Epoch 279: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1565 - val_mae: 0.4433\n",
            "Epoch 280/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 280: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1580 - val_mae: 0.4456\n",
            "Epoch 281/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4742\n",
            "Epoch 281: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1572 - val_mae: 0.4451\n",
            "Epoch 282/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4735\n",
            "Epoch 282: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1571 - val_mae: 0.4439\n",
            "Epoch 283/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4755\n",
            "Epoch 283: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1574 - val_mae: 0.4445\n",
            "Epoch 284/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4755\n",
            "Epoch 284: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4755 - val_loss: 0.1584 - val_mae: 0.4457\n",
            "Epoch 285/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4729\n",
            "Epoch 285: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1586 - val_mae: 0.4460\n",
            "Epoch 286/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4736\n",
            "Epoch 286: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4740 - val_loss: 0.1580 - val_mae: 0.4449\n",
            "Epoch 287/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4756\n",
            "Epoch 287: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4448\n",
            "Epoch 288/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4744\n",
            "Epoch 288: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4753 - val_loss: 0.1579 - val_mae: 0.4460\n",
            "Epoch 289/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4747\n",
            "Epoch 289: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1577 - val_mae: 0.4446\n",
            "Epoch 290/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1763 - mae: 0.4754\n",
            "Epoch 290: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1567 - val_mae: 0.4439\n",
            "Epoch 291/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 291: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 292/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 292: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1582 - val_mae: 0.4455\n",
            "Epoch 293/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4755\n",
            "Epoch 293: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1574 - val_mae: 0.4450\n",
            "Epoch 294/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4741\n",
            "Epoch 294: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1749 - mae: 0.4738 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 295/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4745\n",
            "Epoch 295: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4745 - val_loss: 0.1573 - val_mae: 0.4439\n",
            "Epoch 296/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4726\n",
            "Epoch 296: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4738 - val_loss: 0.1575 - val_mae: 0.4444\n",
            "Epoch 297/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 297: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1586 - val_mae: 0.4462\n",
            "Epoch 298/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 298: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 299/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4757\n",
            "Epoch 299: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 300/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1774 - mae: 0.4769\n",
            "Epoch 300: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1581 - val_mae: 0.4449\n",
            "Epoch 301/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 301: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1584 - val_mae: 0.4461\n",
            "Epoch 302/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4748\n",
            "Epoch 302: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4460\n",
            "Epoch 303/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4745\n",
            "Epoch 303: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1582 - val_mae: 0.4457\n",
            "Epoch 304/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4739\n",
            "Epoch 304: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4739 - val_loss: 0.1590 - val_mae: 0.4470\n",
            "Epoch 305/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4754\n",
            "Epoch 305: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4459\n",
            "Epoch 306/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4747\n",
            "Epoch 306: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1586 - val_mae: 0.4465\n",
            "Epoch 307/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4759\n",
            "Epoch 307: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1581 - val_mae: 0.4454\n",
            "Epoch 308/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1773 - mae: 0.4766\n",
            "Epoch 308: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1578 - val_mae: 0.4444\n",
            "Epoch 309/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4743\n",
            "Epoch 309: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 310/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4764\n",
            "Epoch 310: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1568 - val_mae: 0.4432\n",
            "Epoch 311/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4734\n",
            "Epoch 311: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1751 - mae: 0.4739 - val_loss: 0.1583 - val_mae: 0.4453\n",
            "Epoch 312/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 312: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1581 - val_mae: 0.4446\n",
            "Epoch 313/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4756\n",
            "Epoch 313: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1579 - val_mae: 0.4452\n",
            "Epoch 314/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 314: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1576 - val_mae: 0.4454\n",
            "Epoch 315/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4753\n",
            "Epoch 315: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1587 - val_mae: 0.4460\n",
            "Epoch 316/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4730\n",
            "Epoch 316: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4740 - val_loss: 0.1579 - val_mae: 0.4452\n",
            "Epoch 317/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4753\n",
            "Epoch 317: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1588 - val_mae: 0.4465\n",
            "Epoch 318/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4742\n",
            "Epoch 318: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1575 - val_mae: 0.4445\n",
            "Epoch 319/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 319: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1590 - val_mae: 0.4466\n",
            "Epoch 320/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4763\n",
            "Epoch 320: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 321/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 321: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1575 - val_mae: 0.4440\n",
            "Epoch 322/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 322: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1580 - val_mae: 0.4448\n",
            "Epoch 323/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 323: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1584 - val_mae: 0.4455\n",
            "Epoch 324/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 324: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1588 - val_mae: 0.4466\n",
            "Epoch 325/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4739\n",
            "Epoch 325: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1577 - val_mae: 0.4442\n",
            "Epoch 326/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4729\n",
            "Epoch 326: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1565 - val_mae: 0.4430\n",
            "Epoch 327/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 327: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1576 - val_mae: 0.4443\n",
            "Epoch 328/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 328: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1588 - val_mae: 0.4466\n",
            "Epoch 329/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4738\n",
            "Epoch 329: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1569 - val_mae: 0.4438\n",
            "Epoch 330/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 330: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1567 - val_mae: 0.4434\n",
            "Epoch 331/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1743 - mae: 0.4729\n",
            "Epoch 331: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1589 - val_mae: 0.4470\n",
            "Epoch 332/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4752\n",
            "Epoch 332: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 333/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4753\n",
            "Epoch 333: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4753 - val_loss: 0.1569 - val_mae: 0.4436\n",
            "Epoch 334/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4742\n",
            "Epoch 334: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1574 - val_mae: 0.4443\n",
            "Epoch 335/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 335: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1564 - val_mae: 0.4428\n",
            "Epoch 336/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4731\n",
            "Epoch 336: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1585 - val_mae: 0.4458\n",
            "Epoch 337/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 337: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1580 - val_mae: 0.4461\n",
            "Epoch 338/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4740\n",
            "Epoch 338: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1582 - val_mae: 0.4454\n",
            "Epoch 339/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4744\n",
            "Epoch 339: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1579 - val_mae: 0.4452\n",
            "Epoch 340/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4764\n",
            "Epoch 340: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1571 - val_mae: 0.4437\n",
            "Epoch 341/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 341: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1567 - val_mae: 0.4427\n",
            "Epoch 342/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1759 - mae: 0.4757\n",
            "Epoch 342: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1585 - val_mae: 0.4456\n",
            "Epoch 343/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4729\n",
            "Epoch 343: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1577 - val_mae: 0.4441\n",
            "Epoch 344/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4753\n",
            "Epoch 344: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1570 - val_mae: 0.4436\n",
            "Epoch 345/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4758\n",
            "Epoch 345: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4746 - val_loss: 0.1576 - val_mae: 0.4441\n",
            "Epoch 346/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4724\n",
            "Epoch 346: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1586 - val_mae: 0.4465\n",
            "Epoch 347/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 347: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 348/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 348: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1588 - val_mae: 0.4465\n",
            "Epoch 349/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4759\n",
            "Epoch 349: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1586 - val_mae: 0.4463\n",
            "Epoch 350/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4756\n",
            "Epoch 350: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4755 - val_loss: 0.1570 - val_mae: 0.4437\n",
            "Epoch 351/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4759\n",
            "Epoch 351: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1569 - val_mae: 0.4435\n",
            "Epoch 352/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4764\n",
            "Epoch 352: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1563 - val_mae: 0.4429\n",
            "Epoch 353/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4753\n",
            "Epoch 353: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1589 - val_mae: 0.4465\n",
            "Epoch 354/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 354: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1750 - mae: 0.4738 - val_loss: 0.1588 - val_mae: 0.4466\n",
            "Epoch 355/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4753\n",
            "Epoch 355: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 356/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 356: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1590 - val_mae: 0.4474\n",
            "Epoch 357/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4752\n",
            "Epoch 357: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 358/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4751\n",
            "Epoch 358: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4754 - val_loss: 0.1590 - val_mae: 0.4467\n",
            "Epoch 359/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 359: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1583 - val_mae: 0.4455\n",
            "Epoch 360/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 360: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1589 - val_mae: 0.4467\n",
            "Epoch 361/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4747\n",
            "Epoch 361: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1751 - mae: 0.4738 - val_loss: 0.1577 - val_mae: 0.4453\n",
            "Epoch 362/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 362: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1584 - val_mae: 0.4461\n",
            "Epoch 363/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 363: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 364/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4746\n",
            "Epoch 364: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1565 - val_mae: 0.4424\n",
            "Epoch 365/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.4764\n",
            "Epoch 365: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1575 - val_mae: 0.4438\n",
            "Epoch 366/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4755\n",
            "Epoch 366: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1567 - val_mae: 0.4437\n",
            "Epoch 367/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4743\n",
            "Epoch 367: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1583 - val_mae: 0.4459\n",
            "Epoch 368/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 368: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1568 - val_mae: 0.4432\n",
            "Epoch 369/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4733\n",
            "Epoch 369: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1747 - mae: 0.4734 - val_loss: 0.1574 - val_mae: 0.4447\n",
            "Epoch 370/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 370: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1579 - val_mae: 0.4448\n",
            "Epoch 371/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 371: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1571 - val_mae: 0.4440\n",
            "Epoch 372/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4751\n",
            "Epoch 372: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1576 - val_mae: 0.4451\n",
            "Epoch 373/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1750 - mae: 0.4736\n",
            "Epoch 373: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1750 - mae: 0.4736 - val_loss: 0.1580 - val_mae: 0.4446\n",
            "Epoch 374/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 374: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1572 - val_mae: 0.4443\n",
            "Epoch 375/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1767 - mae: 0.4758\n",
            "Epoch 375: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1585 - val_mae: 0.4461\n",
            "Epoch 376/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 376: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1589 - val_mae: 0.4471\n",
            "Epoch 377/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4737\n",
            "Epoch 377: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1580 - val_mae: 0.4453\n",
            "Epoch 378/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1748 - mae: 0.4736\n",
            "Epoch 378: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1748 - mae: 0.4736 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 379/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 379: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4447\n",
            "Epoch 380/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4752\n",
            "Epoch 380: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1583 - val_mae: 0.4457\n",
            "Epoch 381/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 381: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1571 - val_mae: 0.4444\n",
            "Epoch 382/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4751\n",
            "Epoch 382: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 383/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1757 - mae: 0.4748\n",
            "Epoch 383: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1579 - val_mae: 0.4449\n",
            "Epoch 384/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4742\n",
            "Epoch 384: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1580 - val_mae: 0.4452\n",
            "Epoch 385/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 385: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1571 - val_mae: 0.4441\n",
            "Epoch 386/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4732\n",
            "Epoch 386: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 387/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4739\n",
            "Epoch 387: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4747 - val_loss: 0.1587 - val_mae: 0.4467\n",
            "Epoch 388/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1753 - mae: 0.4742\n",
            "Epoch 388: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4742 - val_loss: 0.1581 - val_mae: 0.4451\n",
            "Epoch 389/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4753\n",
            "Epoch 389: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4753 - val_loss: 0.1559 - val_mae: 0.4422\n",
            "Epoch 390/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4708\n",
            "Epoch 390: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1572 - val_mae: 0.4445\n",
            "Epoch 391/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4758\n",
            "Epoch 391: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1568 - val_mae: 0.4432\n",
            "Epoch 392/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 392: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 393/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4733\n",
            "Epoch 393: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1572 - val_mae: 0.4436\n",
            "Epoch 394/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4725\n",
            "Epoch 394: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1581 - val_mae: 0.4453\n",
            "Epoch 395/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4746\n",
            "Epoch 395: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4753 - val_loss: 0.1585 - val_mae: 0.4463\n",
            "Epoch 396/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 396: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1575 - val_mae: 0.4445\n",
            "Epoch 397/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4766\n",
            "Epoch 397: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4448\n",
            "Epoch 398/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4748\n",
            "Epoch 398: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1581 - val_mae: 0.4454\n",
            "Epoch 399/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4761\n",
            "Epoch 399: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1578 - val_mae: 0.4447\n",
            "Epoch 400/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 400: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1580 - val_mae: 0.4449\n",
            "Epoch 401/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4737\n",
            "Epoch 401: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1585 - val_mae: 0.4457\n",
            "Epoch 402/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4754\n",
            "Epoch 402: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1578 - val_mae: 0.4454\n",
            "Epoch 403/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 403: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1577 - val_mae: 0.4447\n",
            "Epoch 404/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4730\n",
            "Epoch 404: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1577 - val_mae: 0.4451\n",
            "Epoch 405/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4742\n",
            "Epoch 405: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1567 - val_mae: 0.4435\n",
            "Epoch 406/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4752\n",
            "Epoch 406: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1587 - val_mae: 0.4468\n",
            "Epoch 407/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4743\n",
            "Epoch 407: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1581 - val_mae: 0.4452\n",
            "Epoch 408/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4750\n",
            "Epoch 408: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1574 - val_mae: 0.4456\n",
            "Epoch 409/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4723\n",
            "Epoch 409: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1751 - mae: 0.4738 - val_loss: 0.1582 - val_mae: 0.4459\n",
            "Epoch 410/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 410: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1592 - val_mae: 0.4476\n",
            "Epoch 411/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 411: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1589 - val_mae: 0.4467\n",
            "Epoch 412/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4732\n",
            "Epoch 412: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1577 - val_mae: 0.4449\n",
            "Epoch 413/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4741\n",
            "Epoch 413: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1582 - val_mae: 0.4457\n",
            "Epoch 414/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 414: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4454\n",
            "Epoch 415/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 415: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1590 - val_mae: 0.4470\n",
            "Epoch 416/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4759\n",
            "Epoch 416: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1589 - val_mae: 0.4470\n",
            "Epoch 417/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1753 - mae: 0.4742\n",
            "Epoch 417: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4742 - val_loss: 0.1573 - val_mae: 0.4447\n",
            "Epoch 418/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4760\n",
            "Epoch 418: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1572 - val_mae: 0.4440\n",
            "Epoch 419/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4731\n",
            "Epoch 419: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1746 - mae: 0.4733 - val_loss: 0.1583 - val_mae: 0.4454\n",
            "Epoch 420/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4726\n",
            "Epoch 420: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1561 - val_mae: 0.4424\n",
            "Epoch 421/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 421: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1573 - val_mae: 0.4441\n",
            "Epoch 422/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4757\n",
            "Epoch 422: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1579 - val_mae: 0.4449\n",
            "Epoch 423/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 423: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1585 - val_mae: 0.4464\n",
            "Epoch 424/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1767 - mae: 0.4774\n",
            "Epoch 424: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1574 - val_mae: 0.4447\n",
            "Epoch 425/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 425: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1587 - val_mae: 0.4467\n",
            "Epoch 426/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4759\n",
            "Epoch 426: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1579 - val_mae: 0.4455\n",
            "Epoch 427/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4746\n",
            "Epoch 427: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1751 - mae: 0.4740 - val_loss: 0.1580 - val_mae: 0.4456\n",
            "Epoch 428/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1751 - mae: 0.4739\n",
            "Epoch 428: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1751 - mae: 0.4739 - val_loss: 0.1576 - val_mae: 0.4444\n",
            "Epoch 429/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4761\n",
            "Epoch 429: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4451\n",
            "Epoch 430/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4741\n",
            "Epoch 430: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1585 - val_mae: 0.4455\n",
            "Epoch 431/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4752\n",
            "Epoch 431: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1573 - val_mae: 0.4444\n",
            "Epoch 432/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4747\n",
            "Epoch 432: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1587 - val_mae: 0.4461\n",
            "Epoch 433/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 433: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4743 - val_loss: 0.1560 - val_mae: 0.4431\n",
            "Epoch 434/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4751\n",
            "Epoch 434: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4751 - val_loss: 0.1566 - val_mae: 0.4434\n",
            "Epoch 435/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4764\n",
            "Epoch 435: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1572 - val_mae: 0.4432\n",
            "Epoch 436/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4733\n",
            "Epoch 436: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1590 - val_mae: 0.4469\n",
            "Epoch 437/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 437: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1569 - val_mae: 0.4440\n",
            "Epoch 438/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 438: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1568 - val_mae: 0.4434\n",
            "Epoch 439/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4770\n",
            "Epoch 439: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1584 - val_mae: 0.4464\n",
            "Epoch 440/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4752\n",
            "Epoch 440: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1589 - val_mae: 0.4469\n",
            "Epoch 441/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 441: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1580 - val_mae: 0.4452\n",
            "Epoch 442/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4739\n",
            "Epoch 442: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1574 - val_mae: 0.4443\n",
            "Epoch 443/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4733\n",
            "Epoch 443: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1748 - mae: 0.4735 - val_loss: 0.1585 - val_mae: 0.4457\n",
            "Epoch 444/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 444: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1569 - val_mae: 0.4438\n",
            "Epoch 445/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 445: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1584 - val_mae: 0.4456\n",
            "Epoch 446/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 446: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1582 - val_mae: 0.4461\n",
            "Epoch 447/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 447: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1587 - val_mae: 0.4461\n",
            "Epoch 448/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4747\n",
            "Epoch 448: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1570 - val_mae: 0.4445\n",
            "Epoch 449/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4752\n",
            "Epoch 449: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 450/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4745\n",
            "Epoch 450: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4745 - val_loss: 0.1577 - val_mae: 0.4450\n",
            "Epoch 451/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1768 - mae: 0.4764\n",
            "Epoch 451: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 452/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 452: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1570 - val_mae: 0.4439\n",
            "Epoch 453/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1777 - mae: 0.4778\n",
            "Epoch 453: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1578 - val_mae: 0.4451\n",
            "Epoch 454/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4772\n",
            "Epoch 454: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1580 - val_mae: 0.4454\n",
            "Epoch 455/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4743\n",
            "Epoch 455: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1587 - val_mae: 0.4465\n",
            "Epoch 456/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4727\n",
            "Epoch 456: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1579 - val_mae: 0.4452\n",
            "Epoch 457/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4744\n",
            "Epoch 457: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 458/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 458: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1561 - val_mae: 0.4426\n",
            "Epoch 459/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4757\n",
            "Epoch 459: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1573 - val_mae: 0.4441\n",
            "Epoch 460/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4749\n",
            "Epoch 460: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4749 - val_loss: 0.1583 - val_mae: 0.4459\n",
            "Epoch 461/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1746 - mae: 0.4733\n",
            "Epoch 461: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1746 - mae: 0.4733 - val_loss: 0.1572 - val_mae: 0.4438\n",
            "Epoch 462/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4748\n",
            "Epoch 462: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1584 - val_mae: 0.4457\n",
            "Epoch 463/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4738\n",
            "Epoch 463: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1743 - mae: 0.4731 - val_loss: 0.1587 - val_mae: 0.4464\n",
            "Epoch 464/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4755\n",
            "Epoch 464: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1581 - val_mae: 0.4459\n",
            "Epoch 465/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4744\n",
            "Epoch 465: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1577 - val_mae: 0.4447\n",
            "Epoch 466/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4733\n",
            "Epoch 466: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4450\n",
            "Epoch 467/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1753 - mae: 0.4742\n",
            "Epoch 467: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1753 - mae: 0.4742 - val_loss: 0.1579 - val_mae: 0.4460\n",
            "Epoch 468/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4747\n",
            "Epoch 468: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4742 - val_loss: 0.1587 - val_mae: 0.4469\n",
            "Epoch 469/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 469: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4754 - val_loss: 0.1577 - val_mae: 0.4448\n",
            "Epoch 470/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 470: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1576 - val_mae: 0.4451\n",
            "Epoch 471/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4751\n",
            "Epoch 471: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1584 - val_mae: 0.4463\n",
            "Epoch 472/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4757\n",
            "Epoch 472: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4751 - val_loss: 0.1588 - val_mae: 0.4465\n",
            "Epoch 473/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4742\n",
            "Epoch 473: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4742 - val_loss: 0.1576 - val_mae: 0.4445\n",
            "Epoch 474/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4767\n",
            "Epoch 474: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 475/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4736\n",
            "Epoch 475: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4741 - val_loss: 0.1587 - val_mae: 0.4470\n",
            "Epoch 476/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4734\n",
            "Epoch 476: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1753 - mae: 0.4742 - val_loss: 0.1574 - val_mae: 0.4445\n",
            "Epoch 477/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 477: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1594 - val_mae: 0.4476\n",
            "Epoch 478/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.4717\n",
            "Epoch 478: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1750 - mae: 0.4736 - val_loss: 0.1575 - val_mae: 0.4452\n",
            "Epoch 479/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1765 - mae: 0.4756\n",
            "Epoch 479: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1569 - val_mae: 0.4435\n",
            "Epoch 480/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4755\n",
            "Epoch 480: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1571 - val_mae: 0.4437\n",
            "Epoch 481/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4753\n",
            "Epoch 481: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1555 - val_mae: 0.4424\n",
            "Epoch 482/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4758\n",
            "Epoch 482: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1577 - val_mae: 0.4446\n",
            "Epoch 483/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4742\n",
            "Epoch 483: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 484/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4743\n",
            "Epoch 484: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1574 - val_mae: 0.4439\n",
            "Epoch 485/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4747\n",
            "Epoch 485: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1578 - val_mae: 0.4444\n",
            "Epoch 486/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4771\n",
            "Epoch 486: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1585 - val_mae: 0.4470\n",
            "Epoch 487/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 487: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1751 - mae: 0.4740 - val_loss: 0.1584 - val_mae: 0.4457\n",
            "Epoch 488/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4745\n",
            "Epoch 488: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4745 - val_loss: 0.1584 - val_mae: 0.4464\n",
            "Epoch 489/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4741\n",
            "Epoch 489: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 490/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4748\n",
            "Epoch 490: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4462\n",
            "Epoch 491/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4734\n",
            "Epoch 491: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4747 - val_loss: 0.1580 - val_mae: 0.4455\n",
            "Epoch 492/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1757 - mae: 0.4748\n",
            "Epoch 492: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 493/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4752\n",
            "Epoch 493: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1577 - val_mae: 0.4451\n",
            "Epoch 494/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1749 - mae: 0.4736\n",
            "Epoch 494: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1749 - mae: 0.4736 - val_loss: 0.1581 - val_mae: 0.4458\n",
            "Epoch 495/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 495: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1580 - val_mae: 0.4450\n",
            "Epoch 496/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4761\n",
            "Epoch 496: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1566 - val_mae: 0.4433\n",
            "Epoch 497/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4755\n",
            "Epoch 497: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4441\n",
            "Epoch 498/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4745\n",
            "Epoch 498: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1577 - val_mae: 0.4445\n",
            "Epoch 499/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1754 - mae: 0.4736\n",
            "Epoch 499: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1573 - val_mae: 0.4447\n",
            "Epoch 500/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4756\n",
            "Epoch 500: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1581 - val_mae: 0.4456\n",
            "La valeur du mse obtenue  7  trimestres plus tard est  5.398\n",
            "Epoch 1/500\n",
            "     67/Unknown - 4s 23ms/step - loss: 0.1878 - mae: 0.4986\n",
            "Epoch 1: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 6s 45ms/step - loss: 0.1878 - mae: 0.4985 - val_loss: 0.1683 - val_mae: 0.4662\n",
            "Epoch 2/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1858 - mae: 0.4955\n",
            "Epoch 2: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1858 - mae: 0.4955 - val_loss: 0.1653 - val_mae: 0.4610\n",
            "Epoch 3/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1825 - mae: 0.4911\n",
            "Epoch 3: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1840 - mae: 0.4925 - val_loss: 0.1646 - val_mae: 0.4589\n",
            "Epoch 4/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1818 - mae: 0.4891\n",
            "Epoch 4: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1818 - mae: 0.4891 - val_loss: 0.1631 - val_mae: 0.4565\n",
            "Epoch 5/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1812 - mae: 0.4880\n",
            "Epoch 5: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1811 - mae: 0.4878 - val_loss: 0.1621 - val_mae: 0.4553\n",
            "Epoch 6/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1808 - mae: 0.4869\n",
            "Epoch 6: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1806 - mae: 0.4865 - val_loss: 0.1616 - val_mae: 0.4537\n",
            "Epoch 7/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1790 - mae: 0.4838\n",
            "Epoch 7: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1801 - mae: 0.4855 - val_loss: 0.1616 - val_mae: 0.4541\n",
            "Epoch 8/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1801 - mae: 0.4857\n",
            "Epoch 8: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1793 - mae: 0.4839 - val_loss: 0.1602 - val_mae: 0.4515\n",
            "Epoch 9/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1794 - mae: 0.4837\n",
            "Epoch 9: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1791 - mae: 0.4836 - val_loss: 0.1604 - val_mae: 0.4517\n",
            "Epoch 10/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1794 - mae: 0.4835\n",
            "Epoch 10: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1786 - mae: 0.4825 - val_loss: 0.1592 - val_mae: 0.4495\n",
            "Epoch 11/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1795 - mae: 0.4836\n",
            "Epoch 11: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1776 - mae: 0.4807 - val_loss: 0.1602 - val_mae: 0.4505\n",
            "Epoch 12/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1780 - mae: 0.4807\n",
            "Epoch 12: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1779 - mae: 0.4808 - val_loss: 0.1594 - val_mae: 0.4494\n",
            "Epoch 13/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1772 - mae: 0.4796\n",
            "Epoch 13: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1772 - mae: 0.4796 - val_loss: 0.1598 - val_mae: 0.4498\n",
            "Epoch 14/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1770 - mae: 0.4792\n",
            "Epoch 14: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1770 - mae: 0.4792 - val_loss: 0.1604 - val_mae: 0.4507\n",
            "Epoch 15/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1771 - mae: 0.4793\n",
            "Epoch 15: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1771 - mae: 0.4793 - val_loss: 0.1597 - val_mae: 0.4493\n",
            "Epoch 16/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1776 - mae: 0.4796\n",
            "Epoch 16: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1776 - mae: 0.4796 - val_loss: 0.1597 - val_mae: 0.4495\n",
            "Epoch 17/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1774 - mae: 0.4802\n",
            "Epoch 17: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1774 - mae: 0.4792 - val_loss: 0.1587 - val_mae: 0.4480\n",
            "Epoch 18/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.4785\n",
            "Epoch 18: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1769 - mae: 0.4781 - val_loss: 0.1566 - val_mae: 0.4452\n",
            "Epoch 19/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4777\n",
            "Epoch 19: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1770 - mae: 0.4780 - val_loss: 0.1600 - val_mae: 0.4496\n",
            "Epoch 20/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1770 - mae: 0.4780\n",
            "Epoch 20: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1770 - mae: 0.4780 - val_loss: 0.1573 - val_mae: 0.4464\n",
            "Epoch 21/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4774\n",
            "Epoch 21: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1772 - mae: 0.4784 - val_loss: 0.1597 - val_mae: 0.4483\n",
            "Epoch 22/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4771\n",
            "Epoch 22: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1769 - mae: 0.4778 - val_loss: 0.1584 - val_mae: 0.4467\n",
            "Epoch 23/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1778 - mae: 0.4796\n",
            "Epoch 23: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1771 - mae: 0.4779 - val_loss: 0.1592 - val_mae: 0.4478\n",
            "Epoch 24/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4785\n",
            "Epoch 24: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1768 - mae: 0.4774 - val_loss: 0.1584 - val_mae: 0.4468\n",
            "Epoch 25/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4783\n",
            "Epoch 25: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1770 - mae: 0.4776 - val_loss: 0.1581 - val_mae: 0.4464\n",
            "Epoch 26/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4765\n",
            "Epoch 26: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1765 - mae: 0.4768 - val_loss: 0.1580 - val_mae: 0.4462\n",
            "Epoch 27/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1776 - mae: 0.4775\n",
            "Epoch 27: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1766 - mae: 0.4766 - val_loss: 0.1583 - val_mae: 0.4465\n",
            "Epoch 28/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4772\n",
            "Epoch 28: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1766 - mae: 0.4765 - val_loss: 0.1583 - val_mae: 0.4463\n",
            "Epoch 29/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1782 - mae: 0.4788\n",
            "Epoch 29: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1769 - mae: 0.4772 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 30/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4762\n",
            "Epoch 30: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4762 - val_loss: 0.1584 - val_mae: 0.4465\n",
            "Epoch 31/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4768\n",
            "Epoch 31: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1766 - mae: 0.4764 - val_loss: 0.1568 - val_mae: 0.4443\n",
            "Epoch 32/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4751\n",
            "Epoch 32: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1766 - mae: 0.4763 - val_loss: 0.1581 - val_mae: 0.4462\n",
            "Epoch 33/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4765\n",
            "Epoch 33: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4758 - val_loss: 0.1584 - val_mae: 0.4465\n",
            "Epoch 34/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1765 - mae: 0.4760\n",
            "Epoch 34: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1765 - mae: 0.4760 - val_loss: 0.1586 - val_mae: 0.4468\n",
            "Epoch 35/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4765\n",
            "Epoch 35: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1766 - mae: 0.4762 - val_loss: 0.1591 - val_mae: 0.4470\n",
            "Epoch 36/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4772\n",
            "Epoch 36: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4759 - val_loss: 0.1581 - val_mae: 0.4459\n",
            "Epoch 37/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4745\n",
            "Epoch 37: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4754 - val_loss: 0.1583 - val_mae: 0.4465\n",
            "Epoch 38/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 38: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1763 - mae: 0.4757 - val_loss: 0.1573 - val_mae: 0.4455\n",
            "Epoch 39/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1776 - mae: 0.4775\n",
            "Epoch 39: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1766 - mae: 0.4761 - val_loss: 0.1581 - val_mae: 0.4461\n",
            "Epoch 40/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4763\n",
            "Epoch 40: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1766 - mae: 0.4760 - val_loss: 0.1572 - val_mae: 0.4441\n",
            "Epoch 41/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 41: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4757 - val_loss: 0.1594 - val_mae: 0.4479\n",
            "Epoch 42/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4772\n",
            "Epoch 42: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4757 - val_loss: 0.1591 - val_mae: 0.4468\n",
            "Epoch 43/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4766\n",
            "Epoch 43: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4754 - val_loss: 0.1566 - val_mae: 0.4439\n",
            "Epoch 44/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 44: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1588 - val_mae: 0.4466\n",
            "Epoch 45/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1769 - mae: 0.4772\n",
            "Epoch 45: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4756 - val_loss: 0.1592 - val_mae: 0.4480\n",
            "Epoch 46/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1779 - mae: 0.4781\n",
            "Epoch 46: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1766 - mae: 0.4762 - val_loss: 0.1574 - val_mae: 0.4453\n",
            "Epoch 47/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4768\n",
            "Epoch 47: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1765 - mae: 0.4757 - val_loss: 0.1574 - val_mae: 0.4450\n",
            "Epoch 48/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4753\n",
            "Epoch 48: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1568 - val_mae: 0.4441\n",
            "Epoch 49/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4758\n",
            "Epoch 49: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4455\n",
            "Epoch 50/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 50: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1765 - mae: 0.4758 - val_loss: 0.1578 - val_mae: 0.4454\n",
            "Epoch 51/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 51: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1573 - val_mae: 0.4441\n",
            "Epoch 52/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4739\n",
            "Epoch 52: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1585 - val_mae: 0.4458\n",
            "Epoch 53/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 53: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1579 - val_mae: 0.4452\n",
            "Epoch 54/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4746\n",
            "Epoch 54: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1582 - val_mae: 0.4460\n",
            "Epoch 55/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4757\n",
            "Epoch 55: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4756 - val_loss: 0.1581 - val_mae: 0.4455\n",
            "Epoch 56/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4745\n",
            "Epoch 56: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1764 - mae: 0.4754 - val_loss: 0.1584 - val_mae: 0.4455\n",
            "Epoch 57/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4755\n",
            "Epoch 57: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 58/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4727\n",
            "Epoch 58: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1762 - mae: 0.4754 - val_loss: 0.1583 - val_mae: 0.4459\n",
            "Epoch 59/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4746\n",
            "Epoch 59: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1584 - val_mae: 0.4454\n",
            "Epoch 60/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4740\n",
            "Epoch 60: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1571 - val_mae: 0.4442\n",
            "Epoch 61/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1759 - mae: 0.4753\n",
            "Epoch 61: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4754 - val_loss: 0.1566 - val_mae: 0.4442\n",
            "Epoch 62/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4752\n",
            "Epoch 62: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1591 - val_mae: 0.4477\n",
            "Epoch 63/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1759 - mae: 0.4755\n",
            "Epoch 63: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1577 - val_mae: 0.4453\n",
            "Epoch 64/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1774 - mae: 0.4765\n",
            "Epoch 64: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1764 - mae: 0.4754 - val_loss: 0.1583 - val_mae: 0.4455\n",
            "Epoch 65/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4741\n",
            "Epoch 65: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1583 - val_mae: 0.4456\n",
            "Epoch 66/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4733\n",
            "Epoch 66: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1571 - val_mae: 0.4441\n",
            "Epoch 67/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4750\n",
            "Epoch 67: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1571 - val_mae: 0.4435\n",
            "Epoch 68/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4751\n",
            "Epoch 68: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1575 - val_mae: 0.4449\n",
            "Epoch 69/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4753\n",
            "Epoch 69: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1579 - val_mae: 0.4446\n",
            "Epoch 70/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4742\n",
            "Epoch 70: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1764 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4457\n",
            "Epoch 71/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4760\n",
            "Epoch 71: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4754 - val_loss: 0.1576 - val_mae: 0.4450\n",
            "Epoch 72/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1776 - mae: 0.4766\n",
            "Epoch 72: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1579 - val_mae: 0.4455\n",
            "Epoch 73/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4754\n",
            "Epoch 73: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1763 - mae: 0.4754 - val_loss: 0.1583 - val_mae: 0.4452\n",
            "Epoch 74/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4767\n",
            "Epoch 74: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1590 - val_mae: 0.4471\n",
            "Epoch 75/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1781 - mae: 0.4788\n",
            "Epoch 75: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1581 - val_mae: 0.4460\n",
            "Epoch 76/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4741\n",
            "Epoch 76: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4741 - val_loss: 0.1584 - val_mae: 0.4463\n",
            "Epoch 77/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 77: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4756 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 78/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4733\n",
            "Epoch 78: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1586 - val_mae: 0.4462\n",
            "Epoch 79/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1764 - mae: 0.4753\n",
            "Epoch 79: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1764 - mae: 0.4753 - val_loss: 0.1582 - val_mae: 0.4461\n",
            "Epoch 80/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4757\n",
            "Epoch 80: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1764 - mae: 0.4755 - val_loss: 0.1581 - val_mae: 0.4451\n",
            "Epoch 81/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4744\n",
            "Epoch 81: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1580 - val_mae: 0.4461\n",
            "Epoch 82/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 82: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1575 - val_mae: 0.4448\n",
            "Epoch 83/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4763\n",
            "Epoch 83: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1589 - val_mae: 0.4471\n",
            "Epoch 84/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4760\n",
            "Epoch 84: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1577 - val_mae: 0.4450\n",
            "Epoch 85/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4750\n",
            "Epoch 85: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4750 - val_loss: 0.1583 - val_mae: 0.4461\n",
            "Epoch 86/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4755\n",
            "Epoch 86: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1764 - mae: 0.4753 - val_loss: 0.1578 - val_mae: 0.4446\n",
            "Epoch 87/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4756\n",
            "Epoch 87: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1587 - val_mae: 0.4466\n",
            "Epoch 88/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4755\n",
            "Epoch 88: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4754 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 89/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4758\n",
            "Epoch 89: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1587 - val_mae: 0.4455\n",
            "Epoch 90/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4743\n",
            "Epoch 90: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4743 - val_loss: 0.1577 - val_mae: 0.4446\n",
            "Epoch 91/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1764 - mae: 0.4754\n",
            "Epoch 91: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1764 - mae: 0.4754 - val_loss: 0.1586 - val_mae: 0.4465\n",
            "Epoch 92/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 92: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1576 - val_mae: 0.4456\n",
            "Epoch 93/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4740\n",
            "Epoch 93: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4455\n",
            "Epoch 94/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4742\n",
            "Epoch 94: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4742 - val_loss: 0.1577 - val_mae: 0.4447\n",
            "Epoch 95/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4765\n",
            "Epoch 95: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1581 - val_mae: 0.4455\n",
            "Epoch 96/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4766\n",
            "Epoch 96: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4751 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 97/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4751\n",
            "Epoch 97: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4747 - val_loss: 0.1587 - val_mae: 0.4459\n",
            "Epoch 98/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1770 - mae: 0.4755\n",
            "Epoch 98: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1581 - val_mae: 0.4458\n",
            "Epoch 99/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4722\n",
            "Epoch 99: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1575 - val_mae: 0.4442\n",
            "Epoch 100/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4752\n",
            "Epoch 100: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1574 - val_mae: 0.4443\n",
            "Epoch 101/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4756\n",
            "Epoch 101: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1589 - val_mae: 0.4463\n",
            "Epoch 102/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4741\n",
            "Epoch 102: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4754 - val_loss: 0.1580 - val_mae: 0.4456\n",
            "Epoch 103/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4730\n",
            "Epoch 103: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1592 - val_mae: 0.4469\n",
            "Epoch 104/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4749\n",
            "Epoch 104: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4755 - val_loss: 0.1582 - val_mae: 0.4462\n",
            "Epoch 105/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4747\n",
            "Epoch 105: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1578 - val_mae: 0.4444\n",
            "Epoch 106/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4772\n",
            "Epoch 106: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4745 - val_loss: 0.1582 - val_mae: 0.4458\n",
            "Epoch 107/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1761 - mae: 0.4743\n",
            "Epoch 107: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1575 - val_mae: 0.4447\n",
            "Epoch 108/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4744\n",
            "Epoch 108: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4744 - val_loss: 0.1587 - val_mae: 0.4464\n",
            "Epoch 109/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4743\n",
            "Epoch 109: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1585 - val_mae: 0.4462\n",
            "Epoch 110/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4759\n",
            "Epoch 110: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4749 - val_loss: 0.1588 - val_mae: 0.4462\n",
            "Epoch 111/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4754\n",
            "Epoch 111: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1582 - val_mae: 0.4459\n",
            "Epoch 112/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1777 - mae: 0.4779\n",
            "Epoch 112: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1578 - val_mae: 0.4447\n",
            "Epoch 113/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4742\n",
            "Epoch 113: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4744 - val_loss: 0.1592 - val_mae: 0.4477\n",
            "Epoch 114/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4751\n",
            "Epoch 114: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4751 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 115/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4756\n",
            "Epoch 115: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1579 - val_mae: 0.4444\n",
            "Epoch 116/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4753\n",
            "Epoch 116: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 117/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4741\n",
            "Epoch 117: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1589 - val_mae: 0.4464\n",
            "Epoch 118/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4748\n",
            "Epoch 118: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1574 - val_mae: 0.4435\n",
            "Epoch 119/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1753 - mae: 0.4740\n",
            "Epoch 119: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4740 - val_loss: 0.1580 - val_mae: 0.4450\n",
            "Epoch 120/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4731\n",
            "Epoch 120: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4456\n",
            "Epoch 121/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4749\n",
            "Epoch 121: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4744 - val_loss: 0.1571 - val_mae: 0.4436\n",
            "Epoch 122/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4735\n",
            "Epoch 122: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1761 - mae: 0.4747 - val_loss: 0.1568 - val_mae: 0.4435\n",
            "Epoch 123/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4746\n",
            "Epoch 123: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4746 - val_loss: 0.1582 - val_mae: 0.4462\n",
            "Epoch 124/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4740\n",
            "Epoch 124: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1584 - val_mae: 0.4453\n",
            "Epoch 125/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4734\n",
            "Epoch 125: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1574 - val_mae: 0.4445\n",
            "Epoch 126/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4743\n",
            "Epoch 126: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 127/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1780 - mae: 0.4779\n",
            "Epoch 127: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 128/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4738\n",
            "Epoch 128: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1575 - val_mae: 0.4442\n",
            "Epoch 129/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4754\n",
            "Epoch 129: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1569 - val_mae: 0.4432\n",
            "Epoch 130/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4759\n",
            "Epoch 130: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1584 - val_mae: 0.4455\n",
            "Epoch 131/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1748 - mae: 0.4737\n",
            "Epoch 131: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1586 - val_mae: 0.4461\n",
            "Epoch 132/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4745\n",
            "Epoch 132: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1584 - val_mae: 0.4457\n",
            "Epoch 133/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4755\n",
            "Epoch 133: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 134/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4753\n",
            "Epoch 134: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4460\n",
            "Epoch 135/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4752\n",
            "Epoch 135: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1582 - val_mae: 0.4454\n",
            "Epoch 136/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4731\n",
            "Epoch 136: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1589 - val_mae: 0.4467\n",
            "Epoch 137/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4742\n",
            "Epoch 137: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1576 - val_mae: 0.4455\n",
            "Epoch 138/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 138: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1574 - val_mae: 0.4446\n",
            "Epoch 139/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1763 - mae: 0.4743\n",
            "Epoch 139: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1589 - val_mae: 0.4468\n",
            "Epoch 140/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4739\n",
            "Epoch 140: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1576 - val_mae: 0.4442\n",
            "Epoch 141/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4748\n",
            "Epoch 141: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1578 - val_mae: 0.4447\n",
            "Epoch 142/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4759\n",
            "Epoch 142: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1570 - val_mae: 0.4438\n",
            "Epoch 143/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4740\n",
            "Epoch 143: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4747 - val_loss: 0.1583 - val_mae: 0.4456\n",
            "Epoch 144/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 144: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1583 - val_mae: 0.4453\n",
            "Epoch 145/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1778 - mae: 0.4776\n",
            "Epoch 145: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4746 - val_loss: 0.1585 - val_mae: 0.4454\n",
            "Epoch 146/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1755 - mae: 0.4746\n",
            "Epoch 146: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1585 - val_mae: 0.4460\n",
            "Epoch 147/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4739\n",
            "Epoch 147: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1583 - val_mae: 0.4455\n",
            "Epoch 148/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1769 - mae: 0.4768\n",
            "Epoch 148: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1571 - val_mae: 0.4438\n",
            "Epoch 149/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4756\n",
            "Epoch 149: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1581 - val_mae: 0.4452\n",
            "Epoch 150/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4739\n",
            "Epoch 150: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4739 - val_loss: 0.1578 - val_mae: 0.4446\n",
            "Epoch 151/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4741\n",
            "Epoch 151: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4745 - val_loss: 0.1586 - val_mae: 0.4463\n",
            "Epoch 152/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4758\n",
            "Epoch 152: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1560 - val_mae: 0.4426\n",
            "Epoch 153/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1752 - mae: 0.4736\n",
            "Epoch 153: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1752 - mae: 0.4736 - val_loss: 0.1572 - val_mae: 0.4441\n",
            "Epoch 154/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4717\n",
            "Epoch 154: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1586 - val_mae: 0.4464\n",
            "Epoch 155/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4730\n",
            "Epoch 155: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1578 - val_mae: 0.4451\n",
            "Epoch 156/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4721\n",
            "Epoch 156: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1575 - val_mae: 0.4454\n",
            "Epoch 157/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4751\n",
            "Epoch 157: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 158/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 158: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1594 - val_mae: 0.4480\n",
            "Epoch 159/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4739\n",
            "Epoch 159: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1566 - val_mae: 0.4433\n",
            "Epoch 160/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4718\n",
            "Epoch 160: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1570 - val_mae: 0.4434\n",
            "Epoch 161/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4739\n",
            "Epoch 161: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1571 - val_mae: 0.4438\n",
            "Epoch 162/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4746\n",
            "Epoch 162: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4746 - val_loss: 0.1557 - val_mae: 0.4427\n",
            "Epoch 163/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4746\n",
            "Epoch 163: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1590 - val_mae: 0.4465\n",
            "Epoch 164/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4736\n",
            "Epoch 164: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1563 - val_mae: 0.4433\n",
            "Epoch 165/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 165: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1581 - val_mae: 0.4448\n",
            "Epoch 166/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4761\n",
            "Epoch 166: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4742 - val_loss: 0.1588 - val_mae: 0.4464\n",
            "Epoch 167/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 167: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1590 - val_mae: 0.4471\n",
            "Epoch 168/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4752\n",
            "Epoch 168: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1590 - val_mae: 0.4467\n",
            "Epoch 169/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4749\n",
            "Epoch 169: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1577 - val_mae: 0.4446\n",
            "Epoch 170/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4753\n",
            "Epoch 170: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 171/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4721\n",
            "Epoch 171: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1583 - val_mae: 0.4457\n",
            "Epoch 172/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4738\n",
            "Epoch 172: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4739 - val_loss: 0.1580 - val_mae: 0.4457\n",
            "Epoch 173/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4731\n",
            "Epoch 173: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1586 - val_mae: 0.4461\n",
            "Epoch 174/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4713\n",
            "Epoch 174: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4744 - val_loss: 0.1591 - val_mae: 0.4475\n",
            "Epoch 175/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4750\n",
            "Epoch 175: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1572 - val_mae: 0.4444\n",
            "Epoch 176/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4738\n",
            "Epoch 176: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1581 - val_mae: 0.4453\n",
            "Epoch 177/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1767 - mae: 0.4766\n",
            "Epoch 177: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1582 - val_mae: 0.4458\n",
            "Epoch 178/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4758\n",
            "Epoch 178: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1584 - val_mae: 0.4455\n",
            "Epoch 179/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 179: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1575 - val_mae: 0.4443\n",
            "Epoch 180/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4744\n",
            "Epoch 180: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4740 - val_loss: 0.1562 - val_mae: 0.4431\n",
            "Epoch 181/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4753\n",
            "Epoch 181: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1580 - val_mae: 0.4452\n",
            "Epoch 182/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4751\n",
            "Epoch 182: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1587 - val_mae: 0.4467\n",
            "Epoch 183/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4750\n",
            "Epoch 183: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1585 - val_mae: 0.4464\n",
            "Epoch 184/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4747\n",
            "Epoch 184: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1579 - val_mae: 0.4449\n",
            "Epoch 185/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4757\n",
            "Epoch 185: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1568 - val_mae: 0.4432\n",
            "Epoch 186/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.4758\n",
            "Epoch 186: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1565 - val_mae: 0.4428\n",
            "Epoch 187/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4746\n",
            "Epoch 187: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1580 - val_mae: 0.4453\n",
            "Epoch 188/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1781 - mae: 0.4786\n",
            "Epoch 188: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1569 - val_mae: 0.4442\n",
            "Epoch 189/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4757\n",
            "Epoch 189: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1570 - val_mae: 0.4434\n",
            "Epoch 190/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4748\n",
            "Epoch 190: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1583 - val_mae: 0.4454\n",
            "Epoch 191/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4752\n",
            "Epoch 191: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1574 - val_mae: 0.4450\n",
            "Epoch 192/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4743\n",
            "Epoch 192: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1561 - val_mae: 0.4427\n",
            "Epoch 193/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4748\n",
            "Epoch 193: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1578 - val_mae: 0.4445\n",
            "Epoch 194/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4749\n",
            "Epoch 194: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1577 - val_mae: 0.4452\n",
            "Epoch 195/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4750\n",
            "Epoch 195: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1583 - val_mae: 0.4457\n",
            "Epoch 196/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4746\n",
            "Epoch 196: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1563 - val_mae: 0.4425\n",
            "Epoch 197/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4768\n",
            "Epoch 197: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1579 - val_mae: 0.4448\n",
            "Epoch 198/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4748\n",
            "Epoch 198: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1569 - val_mae: 0.4441\n",
            "Epoch 199/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4749\n",
            "Epoch 199: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1594 - val_mae: 0.4476\n",
            "Epoch 200/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4751\n",
            "Epoch 200: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1585 - val_mae: 0.4460\n",
            "Epoch 201/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4755\n",
            "Epoch 201: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4755 - val_loss: 0.1566 - val_mae: 0.4435\n",
            "Epoch 202/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4753\n",
            "Epoch 202: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1577 - val_mae: 0.4447\n",
            "Epoch 203/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4737\n",
            "Epoch 203: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1576 - val_mae: 0.4444\n",
            "Epoch 204/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 204: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1579 - val_mae: 0.4448\n",
            "Epoch 205/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4753\n",
            "Epoch 205: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1561 - val_mae: 0.4419\n",
            "Epoch 206/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 206: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 207/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4734\n",
            "Epoch 207: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4755 - val_loss: 0.1579 - val_mae: 0.4449\n",
            "Epoch 208/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4751\n",
            "Epoch 208: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1574 - val_mae: 0.4440\n",
            "Epoch 209/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4736\n",
            "Epoch 209: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1574 - val_mae: 0.4439\n",
            "Epoch 210/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4739\n",
            "Epoch 210: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1580 - val_mae: 0.4448\n",
            "Epoch 211/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4746\n",
            "Epoch 211: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1579 - val_mae: 0.4452\n",
            "Epoch 212/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 212: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1590 - val_mae: 0.4476\n",
            "Epoch 213/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4748\n",
            "Epoch 213: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1585 - val_mae: 0.4460\n",
            "Epoch 214/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4736\n",
            "Epoch 214: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 215/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4742\n",
            "Epoch 215: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1571 - val_mae: 0.4443\n",
            "Epoch 216/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 216: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 217/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1778 - mae: 0.4774\n",
            "Epoch 217: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 218/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4769\n",
            "Epoch 218: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1588 - val_mae: 0.4468\n",
            "Epoch 219/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4748\n",
            "Epoch 219: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1573 - val_mae: 0.4439\n",
            "Epoch 220/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 220: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1574 - val_mae: 0.4443\n",
            "Epoch 221/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1772 - mae: 0.4761\n",
            "Epoch 221: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 222/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 222: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1593 - val_mae: 0.4474\n",
            "Epoch 223/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4756\n",
            "Epoch 223: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4451\n",
            "Epoch 224/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 224: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1584 - val_mae: 0.4463\n",
            "Epoch 225/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 225: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1579 - val_mae: 0.4448\n",
            "Epoch 226/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 226: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 227/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4753\n",
            "Epoch 227: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1592 - val_mae: 0.4472\n",
            "Epoch 228/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 228: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1580 - val_mae: 0.4456\n",
            "Epoch 229/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4746\n",
            "Epoch 229: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 230/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4740\n",
            "Epoch 230: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1576 - val_mae: 0.4447\n",
            "Epoch 231/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4734\n",
            "Epoch 231: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1577 - val_mae: 0.4451\n",
            "Epoch 232/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 232: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1583 - val_mae: 0.4454\n",
            "Epoch 233/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4746\n",
            "Epoch 233: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1574 - val_mae: 0.4441\n",
            "Epoch 234/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4742\n",
            "Epoch 234: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1581 - val_mae: 0.4454\n",
            "Epoch 235/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4734\n",
            "Epoch 235: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4460\n",
            "Epoch 236/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 236: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 237/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4752\n",
            "Epoch 237: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 238/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4762\n",
            "Epoch 238: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4756 - val_loss: 0.1571 - val_mae: 0.4433\n",
            "Epoch 239/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4739\n",
            "Epoch 239: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1586 - val_mae: 0.4464\n",
            "Epoch 240/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4730\n",
            "Epoch 240: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1584 - val_mae: 0.4461\n",
            "Epoch 241/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4743\n",
            "Epoch 241: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1572 - val_mae: 0.4435\n",
            "Epoch 242/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4733\n",
            "Epoch 242: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4741 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 243/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4743\n",
            "Epoch 243: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1580 - val_mae: 0.4454\n",
            "Epoch 244/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 244: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1565 - val_mae: 0.4431\n",
            "Epoch 245/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4736\n",
            "Epoch 245: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1585 - val_mae: 0.4462\n",
            "Epoch 246/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 246: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1581 - val_mae: 0.4448\n",
            "Epoch 247/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4722\n",
            "Epoch 247: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4740 - val_loss: 0.1580 - val_mae: 0.4453\n",
            "Epoch 248/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1759 - mae: 0.4755\n",
            "Epoch 248: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1585 - val_mae: 0.4462\n",
            "Epoch 249/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 249: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1580 - val_mae: 0.4449\n",
            "Epoch 250/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 250: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1580 - val_mae: 0.4454\n",
            "Epoch 251/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4757\n",
            "Epoch 251: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1573 - val_mae: 0.4445\n",
            "Epoch 252/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 252: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1563 - val_mae: 0.4426\n",
            "Epoch 253/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 253: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1580 - val_mae: 0.4454\n",
            "Epoch 254/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4761\n",
            "Epoch 254: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1587 - val_mae: 0.4458\n",
            "Epoch 255/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4763\n",
            "Epoch 255: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1573 - val_mae: 0.4440\n",
            "Epoch 256/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4748\n",
            "Epoch 256: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1583 - val_mae: 0.4456\n",
            "Epoch 257/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4731\n",
            "Epoch 257: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1568 - val_mae: 0.4438\n",
            "Epoch 258/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 258: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1561 - val_mae: 0.4427\n",
            "Epoch 259/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 259: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1572 - val_mae: 0.4439\n",
            "Epoch 260/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1753 - mae: 0.4739\n",
            "Epoch 260: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1591 - val_mae: 0.4472\n",
            "Epoch 261/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4755\n",
            "Epoch 261: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1570 - val_mae: 0.4437\n",
            "Epoch 262/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4753\n",
            "Epoch 262: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4753 - val_loss: 0.1574 - val_mae: 0.4442\n",
            "Epoch 263/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4745\n",
            "Epoch 263: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1579 - val_mae: 0.4454\n",
            "Epoch 264/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4733\n",
            "Epoch 264: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1579 - val_mae: 0.4449\n",
            "Epoch 265/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4772\n",
            "Epoch 265: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 266/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4752\n",
            "Epoch 266: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1573 - val_mae: 0.4446\n",
            "Epoch 267/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4759\n",
            "Epoch 267: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1564 - val_mae: 0.4432\n",
            "Epoch 268/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4754\n",
            "Epoch 268: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1577 - val_mae: 0.4442\n",
            "Epoch 269/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 269: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1578 - val_mae: 0.4448\n",
            "Epoch 270/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4754\n",
            "Epoch 270: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 271/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4761\n",
            "Epoch 271: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1577 - val_mae: 0.4451\n",
            "Epoch 272/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4728\n",
            "Epoch 272: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1564 - val_mae: 0.4428\n",
            "Epoch 273/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4740\n",
            "Epoch 273: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 274/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 274: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1577 - val_mae: 0.4449\n",
            "Epoch 275/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4755\n",
            "Epoch 275: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1569 - val_mae: 0.4436\n",
            "Epoch 276/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1773 - mae: 0.4769\n",
            "Epoch 276: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1564 - val_mae: 0.4428\n",
            "Epoch 277/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4748\n",
            "Epoch 277: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1565 - val_mae: 0.4429\n",
            "Epoch 278/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4733\n",
            "Epoch 278: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1586 - val_mae: 0.4470\n",
            "Epoch 279/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4739\n",
            "Epoch 279: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1588 - val_mae: 0.4470\n",
            "Epoch 280/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 280: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1574 - val_mae: 0.4439\n",
            "Epoch 281/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4739\n",
            "Epoch 281: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1575 - val_mae: 0.4442\n",
            "Epoch 282/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 282: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1582 - val_mae: 0.4458\n",
            "Epoch 283/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4719\n",
            "Epoch 283: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1570 - val_mae: 0.4438\n",
            "Epoch 284/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4750\n",
            "Epoch 284: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1566 - val_mae: 0.4432\n",
            "Epoch 285/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4753\n",
            "Epoch 285: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4753 - val_loss: 0.1579 - val_mae: 0.4456\n",
            "Epoch 286/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1777 - mae: 0.4774\n",
            "Epoch 286: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1577 - val_mae: 0.4445\n",
            "Epoch 287/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1774 - mae: 0.4771\n",
            "Epoch 287: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1575 - val_mae: 0.4446\n",
            "Epoch 288/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1774 - mae: 0.4772\n",
            "Epoch 288: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1570 - val_mae: 0.4438\n",
            "Epoch 289/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4755\n",
            "Epoch 289: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4755 - val_loss: 0.1575 - val_mae: 0.4442\n",
            "Epoch 290/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4747\n",
            "Epoch 290: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1589 - val_mae: 0.4465\n",
            "Epoch 291/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4772\n",
            "Epoch 291: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4462\n",
            "Epoch 292/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 292: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1584 - val_mae: 0.4455\n",
            "Epoch 293/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4718\n",
            "Epoch 293: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 294/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4741\n",
            "Epoch 294: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1580 - val_mae: 0.4450\n",
            "Epoch 295/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4734\n",
            "Epoch 295: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1585 - val_mae: 0.4460\n",
            "Epoch 296/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1757 - mae: 0.4749\n",
            "Epoch 296: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1562 - val_mae: 0.4425\n",
            "Epoch 297/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1754 - mae: 0.4741\n",
            "Epoch 297: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1751 - mae: 0.4738 - val_loss: 0.1563 - val_mae: 0.4432\n",
            "Epoch 298/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4756\n",
            "Epoch 298: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 299/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 299: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1570 - val_mae: 0.4436\n",
            "Epoch 300/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4757\n",
            "Epoch 300: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1586 - val_mae: 0.4466\n",
            "Epoch 301/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 301: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1575 - val_mae: 0.4447\n",
            "Epoch 302/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4755\n",
            "Epoch 302: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1592 - val_mae: 0.4469\n",
            "Epoch 303/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4745\n",
            "Epoch 303: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1569 - val_mae: 0.4443\n",
            "Epoch 304/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4746\n",
            "Epoch 304: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1581 - val_mae: 0.4459\n",
            "Epoch 305/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4743\n",
            "Epoch 305: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1586 - val_mae: 0.4459\n",
            "Epoch 306/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4739\n",
            "Epoch 306: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4755 - val_loss: 0.1584 - val_mae: 0.4463\n",
            "Epoch 307/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 307: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1565 - val_mae: 0.4422\n",
            "Epoch 308/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4746\n",
            "Epoch 308: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4753 - val_loss: 0.1581 - val_mae: 0.4455\n",
            "Epoch 309/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 309: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1571 - val_mae: 0.4443\n",
            "Epoch 310/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1765 - mae: 0.4750\n",
            "Epoch 310: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1574 - val_mae: 0.4451\n",
            "Epoch 311/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 311: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1753 - mae: 0.4742 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 312/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 312: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1589 - val_mae: 0.4471\n",
            "Epoch 313/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4745\n",
            "Epoch 313: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1574 - val_mae: 0.4440\n",
            "Epoch 314/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4750\n",
            "Epoch 314: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 315/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4722\n",
            "Epoch 315: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1587 - val_mae: 0.4465\n",
            "Epoch 316/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.4764\n",
            "Epoch 316: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1583 - val_mae: 0.4455\n",
            "Epoch 317/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1754 - mae: 0.4745\n",
            "Epoch 317: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4455\n",
            "Epoch 318/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4760\n",
            "Epoch 318: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1584 - val_mae: 0.4459\n",
            "Epoch 319/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4772\n",
            "Epoch 319: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1582 - val_mae: 0.4455\n",
            "Epoch 320/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4760\n",
            "Epoch 320: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1562 - val_mae: 0.4423\n",
            "Epoch 321/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1773 - mae: 0.4765\n",
            "Epoch 321: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1580 - val_mae: 0.4446\n",
            "Epoch 322/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4753\n",
            "Epoch 322: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1570 - val_mae: 0.4441\n",
            "Epoch 323/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4750\n",
            "Epoch 323: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1578 - val_mae: 0.4446\n",
            "Epoch 324/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4755\n",
            "Epoch 324: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 325/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4754\n",
            "Epoch 325: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4753 - val_loss: 0.1580 - val_mae: 0.4452\n",
            "Epoch 326/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 326: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1583 - val_mae: 0.4459\n",
            "Epoch 327/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4731\n",
            "Epoch 327: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1564 - val_mae: 0.4429\n",
            "Epoch 328/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 328: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1570 - val_mae: 0.4436\n",
            "Epoch 329/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4755\n",
            "Epoch 329: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4748 - val_loss: 0.1587 - val_mae: 0.4456\n",
            "Epoch 330/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4742\n",
            "Epoch 330: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1578 - val_mae: 0.4448\n",
            "Epoch 331/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4754\n",
            "Epoch 331: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1588 - val_mae: 0.4466\n",
            "Epoch 332/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 332: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1584 - val_mae: 0.4454\n",
            "Epoch 333/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 333: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1561 - val_mae: 0.4424\n",
            "Epoch 334/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1741 - mae: 0.4722\n",
            "Epoch 334: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1581 - val_mae: 0.4455\n",
            "Epoch 335/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4755\n",
            "Epoch 335: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1761 - mae: 0.4756 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 336/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4740\n",
            "Epoch 336: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1564 - val_mae: 0.4427\n",
            "Epoch 337/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1767 - mae: 0.4760\n",
            "Epoch 337: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1587 - val_mae: 0.4466\n",
            "Epoch 338/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4753\n",
            "Epoch 338: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1586 - val_mae: 0.4466\n",
            "Epoch 339/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1770 - mae: 0.4753\n",
            "Epoch 339: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1751 - mae: 0.4737 - val_loss: 0.1582 - val_mae: 0.4452\n",
            "Epoch 340/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 340: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1573 - val_mae: 0.4436\n",
            "Epoch 341/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4743\n",
            "Epoch 341: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1578 - val_mae: 0.4454\n",
            "Epoch 342/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 342: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1571 - val_mae: 0.4439\n",
            "Epoch 343/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 343: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1579 - val_mae: 0.4443\n",
            "Epoch 344/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4756\n",
            "Epoch 344: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4457\n",
            "Epoch 345/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 345: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 32ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1589 - val_mae: 0.4466\n",
            "Epoch 346/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4737\n",
            "Epoch 346: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1587 - val_mae: 0.4466\n",
            "Epoch 347/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4737\n",
            "Epoch 347: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 32ms/step - loss: 0.1756 - mae: 0.4747 - val_loss: 0.1586 - val_mae: 0.4457\n",
            "Epoch 348/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 348: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1579 - val_mae: 0.4458\n",
            "Epoch 349/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1753 - mae: 0.4743\n",
            "Epoch 349: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1753 - mae: 0.4743 - val_loss: 0.1572 - val_mae: 0.4438\n",
            "Epoch 350/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4756\n",
            "Epoch 350: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1568 - val_mae: 0.4430\n",
            "Epoch 351/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4747\n",
            "Epoch 351: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 352/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4754\n",
            "Epoch 352: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4454\n",
            "Epoch 353/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 353: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1574 - val_mae: 0.4453\n",
            "Epoch 354/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4743\n",
            "Epoch 354: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1566 - val_mae: 0.4433\n",
            "Epoch 355/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4743\n",
            "Epoch 355: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1589 - val_mae: 0.4476\n",
            "Epoch 356/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1766 - mae: 0.4762\n",
            "Epoch 356: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1577 - val_mae: 0.4444\n",
            "Epoch 357/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 357: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 358/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4747\n",
            "Epoch 358: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1587 - val_mae: 0.4467\n",
            "Epoch 359/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 359: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1559 - val_mae: 0.4425\n",
            "Epoch 360/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4751\n",
            "Epoch 360: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1587 - val_mae: 0.4462\n",
            "Epoch 361/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4735\n",
            "Epoch 361: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1578 - val_mae: 0.4448\n",
            "Epoch 362/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4730\n",
            "Epoch 362: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 363/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4747\n",
            "Epoch 363: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4747 - val_loss: 0.1588 - val_mae: 0.4466\n",
            "Epoch 364/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 364: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 365/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4756\n",
            "Epoch 365: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4747 - val_loss: 0.1585 - val_mae: 0.4460\n",
            "Epoch 366/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 366: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1751 - mae: 0.4739 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 367/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 367: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1590 - val_mae: 0.4467\n",
            "Epoch 368/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 368: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1575 - val_mae: 0.4446\n",
            "Epoch 369/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4765\n",
            "Epoch 369: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1587 - val_mae: 0.4468\n",
            "Epoch 370/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4751\n",
            "Epoch 370: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1583 - val_mae: 0.4454\n",
            "Epoch 371/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4735\n",
            "Epoch 371: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1584 - val_mae: 0.4454\n",
            "Epoch 372/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 372: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1585 - val_mae: 0.4462\n",
            "Epoch 373/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4761\n",
            "Epoch 373: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1583 - val_mae: 0.4457\n",
            "Epoch 374/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1767 - mae: 0.4766\n",
            "Epoch 374: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4454\n",
            "Epoch 375/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 375: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 376/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4751\n",
            "Epoch 376: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1584 - val_mae: 0.4456\n",
            "Epoch 377/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4758\n",
            "Epoch 377: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1589 - val_mae: 0.4465\n",
            "Epoch 378/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4733\n",
            "Epoch 378: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1570 - val_mae: 0.4438\n",
            "Epoch 379/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 379: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1568 - val_mae: 0.4436\n",
            "Epoch 380/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4753\n",
            "Epoch 380: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4753 - val_loss: 0.1581 - val_mae: 0.4458\n",
            "Epoch 381/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 381: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1576 - val_mae: 0.4447\n",
            "Epoch 382/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4734\n",
            "Epoch 382: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 383/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 383: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1587 - val_mae: 0.4460\n",
            "Epoch 384/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4763\n",
            "Epoch 384: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1566 - val_mae: 0.4432\n",
            "Epoch 385/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4770\n",
            "Epoch 385: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1573 - val_mae: 0.4449\n",
            "Epoch 386/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1744 - mae: 0.4718\n",
            "Epoch 386: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4738 - val_loss: 0.1588 - val_mae: 0.4468\n",
            "Epoch 387/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4740\n",
            "Epoch 387: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4749 - val_loss: 0.1582 - val_mae: 0.4454\n",
            "Epoch 388/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1753 - mae: 0.4739\n",
            "Epoch 388: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4739 - val_loss: 0.1573 - val_mae: 0.4444\n",
            "Epoch 389/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 389: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1566 - val_mae: 0.4431\n",
            "Epoch 390/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 390: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1571 - val_mae: 0.4440\n",
            "Epoch 391/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 391: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1586 - val_mae: 0.4460\n",
            "Epoch 392/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1774 - mae: 0.4770\n",
            "Epoch 392: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1572 - val_mae: 0.4441\n",
            "Epoch 393/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 393: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1585 - val_mae: 0.4455\n",
            "Epoch 394/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4746\n",
            "Epoch 394: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1753 - mae: 0.4742 - val_loss: 0.1570 - val_mae: 0.4435\n",
            "Epoch 395/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 395: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1580 - val_mae: 0.4453\n",
            "Epoch 396/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 396: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1573 - val_mae: 0.4443\n",
            "Epoch 397/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4767\n",
            "Epoch 397: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1574 - val_mae: 0.4446\n",
            "Epoch 398/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 398: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1582 - val_mae: 0.4457\n",
            "Epoch 399/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1735 - mae: 0.4712\n",
            "Epoch 399: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1576 - val_mae: 0.4443\n",
            "Epoch 400/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 400: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1587 - val_mae: 0.4465\n",
            "Epoch 401/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4759\n",
            "Epoch 401: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1581 - val_mae: 0.4456\n",
            "Epoch 402/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 402: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1572 - val_mae: 0.4437\n",
            "Epoch 403/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4745\n",
            "Epoch 403: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4453\n",
            "Epoch 404/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4747\n",
            "Epoch 404: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1574 - val_mae: 0.4443\n",
            "Epoch 405/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4737\n",
            "Epoch 405: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1574 - val_mae: 0.4444\n",
            "Epoch 406/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4761\n",
            "Epoch 406: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 407/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 407: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1576 - val_mae: 0.4447\n",
            "Epoch 408/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1752 - mae: 0.4743\n",
            "Epoch 408: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4743 - val_loss: 0.1588 - val_mae: 0.4468\n",
            "Epoch 409/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4751\n",
            "Epoch 409: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1576 - val_mae: 0.4442\n",
            "Epoch 410/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 410: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1587 - val_mae: 0.4463\n",
            "Epoch 411/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4737\n",
            "Epoch 411: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1580 - val_mae: 0.4454\n",
            "Epoch 412/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1765 - mae: 0.4755\n",
            "Epoch 412: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1578 - val_mae: 0.4447\n",
            "Epoch 413/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1778 - mae: 0.4776\n",
            "Epoch 413: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1564 - val_mae: 0.4428\n",
            "Epoch 414/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4742\n",
            "Epoch 414: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1571 - val_mae: 0.4443\n",
            "Epoch 415/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 415: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1752 - mae: 0.4738 - val_loss: 0.1557 - val_mae: 0.4422\n",
            "Epoch 416/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4749\n",
            "Epoch 416: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4749 - val_loss: 0.1585 - val_mae: 0.4465\n",
            "Epoch 417/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4755\n",
            "Epoch 417: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1572 - val_mae: 0.4438\n",
            "Epoch 418/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 418: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1558 - val_mae: 0.4422\n",
            "Epoch 419/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1750 - mae: 0.4738\n",
            "Epoch 419: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1750 - mae: 0.4738 - val_loss: 0.1581 - val_mae: 0.4453\n",
            "Epoch 420/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 420: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1573 - val_mae: 0.4438\n",
            "Epoch 421/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4751\n",
            "Epoch 421: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1576 - val_mae: 0.4445\n",
            "Epoch 422/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4772\n",
            "Epoch 422: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4747 - val_loss: 0.1574 - val_mae: 0.4444\n",
            "Epoch 423/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4757\n",
            "Epoch 423: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1574 - val_mae: 0.4440\n",
            "Epoch 424/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4747\n",
            "Epoch 424: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 425/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4756\n",
            "Epoch 425: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1570 - val_mae: 0.4434\n",
            "Epoch 426/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1759 - mae: 0.4756\n",
            "Epoch 426: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1567 - val_mae: 0.4433\n",
            "Epoch 427/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4760\n",
            "Epoch 427: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1585 - val_mae: 0.4457\n",
            "Epoch 428/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 428: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 429/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4737\n",
            "Epoch 429: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1750 - mae: 0.4736 - val_loss: 0.1585 - val_mae: 0.4455\n",
            "Epoch 430/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4740\n",
            "Epoch 430: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1577 - val_mae: 0.4450\n",
            "Epoch 431/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4735\n",
            "Epoch 431: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1585 - val_mae: 0.4458\n",
            "Epoch 432/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4743\n",
            "Epoch 432: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1587 - val_mae: 0.4461\n",
            "Epoch 433/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4735\n",
            "Epoch 433: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1563 - val_mae: 0.4424\n",
            "Epoch 434/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4742\n",
            "Epoch 434: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4741 - val_loss: 0.1587 - val_mae: 0.4469\n",
            "Epoch 435/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4755\n",
            "Epoch 435: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1587 - val_mae: 0.4461\n",
            "Epoch 436/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4729\n",
            "Epoch 436: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1567 - val_mae: 0.4439\n",
            "Epoch 437/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4734\n",
            "Epoch 437: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1563 - val_mae: 0.4429\n",
            "Epoch 438/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4763\n",
            "Epoch 438: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1571 - val_mae: 0.4438\n",
            "Epoch 439/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 439: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1567 - val_mae: 0.4433\n",
            "Epoch 440/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 440: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1584 - val_mae: 0.4460\n",
            "Epoch 441/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4747\n",
            "Epoch 441: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1580 - val_mae: 0.4454\n",
            "Epoch 442/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 442: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1565 - val_mae: 0.4432\n",
            "Epoch 443/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4746\n",
            "Epoch 443: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1590 - val_mae: 0.4473\n",
            "Epoch 444/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 444: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1586 - val_mae: 0.4463\n",
            "Epoch 445/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4752\n",
            "Epoch 445: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 446/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4750\n",
            "Epoch 446: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1748 - mae: 0.4736 - val_loss: 0.1576 - val_mae: 0.4443\n",
            "Epoch 447/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4744\n",
            "Epoch 447: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1754 - mae: 0.4745 - val_loss: 0.1574 - val_mae: 0.4439\n",
            "Epoch 448/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1768 - mae: 0.4769\n",
            "Epoch 448: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4748 - val_loss: 0.1571 - val_mae: 0.4439\n",
            "Epoch 449/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4760\n",
            "Epoch 449: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 450/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 450: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1581 - val_mae: 0.4450\n",
            "Epoch 451/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4760\n",
            "Epoch 451: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1579 - val_mae: 0.4454\n",
            "Epoch 452/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4732\n",
            "Epoch 452: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1575 - val_mae: 0.4452\n",
            "Epoch 453/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4750\n",
            "Epoch 453: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1573 - val_mae: 0.4446\n",
            "Epoch 454/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 454: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1579 - val_mae: 0.4449\n",
            "Epoch 455/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 455: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1575 - val_mae: 0.4445\n",
            "Epoch 456/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 456: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1581 - val_mae: 0.4451\n",
            "Epoch 457/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4733\n",
            "Epoch 457: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4742 - val_loss: 0.1577 - val_mae: 0.4447\n",
            "Epoch 458/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4754\n",
            "Epoch 458: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1573 - val_mae: 0.4442\n",
            "Epoch 459/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4766\n",
            "Epoch 459: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 460/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 460: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4454\n",
            "Epoch 461/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4756\n",
            "Epoch 461: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4753 - val_loss: 0.1588 - val_mae: 0.4467\n",
            "Epoch 462/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4727\n",
            "Epoch 462: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4741 - val_loss: 0.1572 - val_mae: 0.4435\n",
            "Epoch 463/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4754\n",
            "Epoch 463: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4754 - val_loss: 0.1582 - val_mae: 0.4455\n",
            "Epoch 464/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4761\n",
            "Epoch 464: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1590 - val_mae: 0.4466\n",
            "Epoch 465/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4738\n",
            "Epoch 465: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1570 - val_mae: 0.4442\n",
            "Epoch 466/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4740\n",
            "Epoch 466: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1565 - val_mae: 0.4435\n",
            "Epoch 467/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4747\n",
            "Epoch 467: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1587 - val_mae: 0.4464\n",
            "Epoch 468/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4741\n",
            "Epoch 468: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1575 - val_mae: 0.4447\n",
            "Epoch 469/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4758\n",
            "Epoch 469: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 470/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 470: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1571 - val_mae: 0.4448\n",
            "Epoch 471/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4761\n",
            "Epoch 471: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1581 - val_mae: 0.4454\n",
            "Epoch 472/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4764\n",
            "Epoch 472: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1573 - val_mae: 0.4442\n",
            "Epoch 473/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4753\n",
            "Epoch 473: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1586 - val_mae: 0.4462\n",
            "Epoch 474/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 474: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1571 - val_mae: 0.4443\n",
            "Epoch 475/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4747\n",
            "Epoch 475: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4747 - val_loss: 0.1590 - val_mae: 0.4468\n",
            "Epoch 476/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4738\n",
            "Epoch 476: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4745 - val_loss: 0.1582 - val_mae: 0.4458\n",
            "Epoch 477/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 477: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1582 - val_mae: 0.4454\n",
            "Epoch 478/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 478: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1568 - val_mae: 0.4429\n",
            "Epoch 479/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4736\n",
            "Epoch 479: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1578 - val_mae: 0.4455\n",
            "Epoch 480/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4776\n",
            "Epoch 480: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1570 - val_mae: 0.4436\n",
            "Epoch 481/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4745\n",
            "Epoch 481: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4740 - val_loss: 0.1576 - val_mae: 0.4445\n",
            "Epoch 482/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4739\n",
            "Epoch 482: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4748 - val_loss: 0.1583 - val_mae: 0.4457\n",
            "Epoch 483/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4749\n",
            "Epoch 483: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1583 - val_mae: 0.4458\n",
            "Epoch 484/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4736\n",
            "Epoch 484: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1574 - val_mae: 0.4445\n",
            "Epoch 485/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4747\n",
            "Epoch 485: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1574 - val_mae: 0.4443\n",
            "Epoch 486/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4746\n",
            "Epoch 486: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4746 - val_loss: 0.1582 - val_mae: 0.4455\n",
            "Epoch 487/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 487: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1581 - val_mae: 0.4451\n",
            "Epoch 488/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4758\n",
            "Epoch 488: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4747 - val_loss: 0.1572 - val_mae: 0.4440\n",
            "Epoch 489/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4753\n",
            "Epoch 489: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 490/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1770 - mae: 0.4761\n",
            "Epoch 490: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1565 - val_mae: 0.4433\n",
            "Epoch 491/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1740 - mae: 0.4717\n",
            "Epoch 491: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4740 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 492/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4749\n",
            "Epoch 492: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4749 - val_loss: 0.1574 - val_mae: 0.4444\n",
            "Epoch 493/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4742\n",
            "Epoch 493: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 494/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 494: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1571 - val_mae: 0.4435\n",
            "Epoch 495/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1763 - mae: 0.4761\n",
            "Epoch 495: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4749 - val_loss: 0.1586 - val_mae: 0.4455\n",
            "Epoch 496/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1766 - mae: 0.4761\n",
            "Epoch 496: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1582 - val_mae: 0.4458\n",
            "Epoch 497/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4740\n",
            "Epoch 497: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1574 - val_mae: 0.4442\n",
            "Epoch 498/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 498: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1575 - val_mae: 0.4441\n",
            "Epoch 499/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4755\n",
            "Epoch 499: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1584 - val_mae: 0.4454\n",
            "Epoch 500/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4756\n",
            "Epoch 500: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1585 - val_mae: 0.4465\n",
            "La valeur du mse obtenue  8  trimestres plus tard est  5.399\n",
            "Epoch 1/500\n",
            "     66/Unknown - 4s 22ms/step - loss: 0.1869 - mae: 0.4985\n",
            "Epoch 1: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 6s 44ms/step - loss: 0.1878 - mae: 0.4983 - val_loss: 0.1662 - val_mae: 0.4627\n",
            "Epoch 2/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1860 - mae: 0.4960\n",
            "Epoch 2: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1860 - mae: 0.4960 - val_loss: 0.1672 - val_mae: 0.4634\n",
            "Epoch 3/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1831 - mae: 0.4907\n",
            "Epoch 3: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1842 - mae: 0.4929 - val_loss: 0.1643 - val_mae: 0.4597\n",
            "Epoch 4/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1830 - mae: 0.4909\n",
            "Epoch 4: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1830 - mae: 0.4909 - val_loss: 0.1642 - val_mae: 0.4590\n",
            "Epoch 5/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1819 - mae: 0.4889\n",
            "Epoch 5: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1819 - mae: 0.4889 - val_loss: 0.1628 - val_mae: 0.4561\n",
            "Epoch 6/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1805 - mae: 0.4866\n",
            "Epoch 6: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1805 - mae: 0.4866 - val_loss: 0.1619 - val_mae: 0.4543\n",
            "Epoch 7/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1804 - mae: 0.4861\n",
            "Epoch 7: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1804 - mae: 0.4861 - val_loss: 0.1617 - val_mae: 0.4539\n",
            "Epoch 8/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1798 - mae: 0.4851\n",
            "Epoch 8: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1798 - mae: 0.4851 - val_loss: 0.1604 - val_mae: 0.4518\n",
            "Epoch 9/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1793 - mae: 0.4840\n",
            "Epoch 9: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1793 - mae: 0.4840 - val_loss: 0.1610 - val_mae: 0.4521\n",
            "Epoch 10/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1783 - mae: 0.4824\n",
            "Epoch 10: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1787 - mae: 0.4829 - val_loss: 0.1612 - val_mae: 0.4524\n",
            "Epoch 11/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1785 - mae: 0.4825\n",
            "Epoch 11: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1785 - mae: 0.4825 - val_loss: 0.1608 - val_mae: 0.4516\n",
            "Epoch 12/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1783 - mae: 0.4817\n",
            "Epoch 12: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1783 - mae: 0.4817 - val_loss: 0.1602 - val_mae: 0.4507\n",
            "Epoch 13/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1786 - mae: 0.4820\n",
            "Epoch 13: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1782 - mae: 0.4813 - val_loss: 0.1607 - val_mae: 0.4512\n",
            "Epoch 14/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1784 - mae: 0.4816\n",
            "Epoch 14: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1778 - mae: 0.4808 - val_loss: 0.1601 - val_mae: 0.4511\n",
            "Epoch 15/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4795\n",
            "Epoch 15: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1776 - mae: 0.4801 - val_loss: 0.1594 - val_mae: 0.4492\n",
            "Epoch 16/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4782\n",
            "Epoch 16: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1774 - mae: 0.4795 - val_loss: 0.1593 - val_mae: 0.4486\n",
            "Epoch 17/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1791 - mae: 0.4817\n",
            "Epoch 17: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1771 - mae: 0.4790 - val_loss: 0.1591 - val_mae: 0.4478\n",
            "Epoch 18/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4763\n",
            "Epoch 18: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1767 - mae: 0.4782 - val_loss: 0.1576 - val_mae: 0.4464\n",
            "Epoch 19/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1790 - mae: 0.4814\n",
            "Epoch 19: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1773 - mae: 0.4790 - val_loss: 0.1595 - val_mae: 0.4489\n",
            "Epoch 20/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1772 - mae: 0.4787\n",
            "Epoch 20: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1772 - mae: 0.4787 - val_loss: 0.1572 - val_mae: 0.4455\n",
            "Epoch 21/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1786 - mae: 0.4807\n",
            "Epoch 21: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1770 - mae: 0.4783 - val_loss: 0.1593 - val_mae: 0.4485\n",
            "Epoch 22/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1777 - mae: 0.4796\n",
            "Epoch 22: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1770 - mae: 0.4781 - val_loss: 0.1586 - val_mae: 0.4475\n",
            "Epoch 23/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1776 - mae: 0.4792\n",
            "Epoch 23: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1771 - mae: 0.4782 - val_loss: 0.1592 - val_mae: 0.4484\n",
            "Epoch 24/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1765 - mae: 0.4771\n",
            "Epoch 24: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1765 - mae: 0.4771 - val_loss: 0.1583 - val_mae: 0.4475\n",
            "Epoch 25/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4776\n",
            "Epoch 25: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1771 - mae: 0.4780 - val_loss: 0.1593 - val_mae: 0.4486\n",
            "Epoch 26/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1767 - mae: 0.4773\n",
            "Epoch 26: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1767 - mae: 0.4773 - val_loss: 0.1588 - val_mae: 0.4473\n",
            "Epoch 27/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1770 - mae: 0.4784\n",
            "Epoch 27: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1770 - mae: 0.4777 - val_loss: 0.1574 - val_mae: 0.4452\n",
            "Epoch 28/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1769 - mae: 0.4774\n",
            "Epoch 28: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1769 - mae: 0.4774 - val_loss: 0.1585 - val_mae: 0.4471\n",
            "Epoch 29/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1774 - mae: 0.4780\n",
            "Epoch 29: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1768 - mae: 0.4770 - val_loss: 0.1574 - val_mae: 0.4455\n",
            "Epoch 30/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4773\n",
            "Epoch 30: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1766 - mae: 0.4767 - val_loss: 0.1570 - val_mae: 0.4448\n",
            "Epoch 31/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1773 - mae: 0.4774\n",
            "Epoch 31: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1769 - mae: 0.4772 - val_loss: 0.1588 - val_mae: 0.4465\n",
            "Epoch 32/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1778 - mae: 0.4795\n",
            "Epoch 32: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4760 - val_loss: 0.1581 - val_mae: 0.4460\n",
            "Epoch 33/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4745\n",
            "Epoch 33: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4760 - val_loss: 0.1571 - val_mae: 0.4442\n",
            "Epoch 34/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4768\n",
            "Epoch 34: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1765 - mae: 0.4763 - val_loss: 0.1592 - val_mae: 0.4476\n",
            "Epoch 35/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4766\n",
            "Epoch 35: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1766 - mae: 0.4767 - val_loss: 0.1585 - val_mae: 0.4466\n",
            "Epoch 36/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4742\n",
            "Epoch 36: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1768 - mae: 0.4767 - val_loss: 0.1582 - val_mae: 0.4462\n",
            "Epoch 37/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1777 - mae: 0.4779\n",
            "Epoch 37: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1768 - mae: 0.4767 - val_loss: 0.1577 - val_mae: 0.4458\n",
            "Epoch 38/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1761 - mae: 0.4758\n",
            "Epoch 38: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1766 - mae: 0.4763 - val_loss: 0.1588 - val_mae: 0.4467\n",
            "Epoch 39/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4757\n",
            "Epoch 39: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1763 - mae: 0.4757 - val_loss: 0.1577 - val_mae: 0.4458\n",
            "Epoch 40/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4737\n",
            "Epoch 40: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1767 - mae: 0.4764 - val_loss: 0.1587 - val_mae: 0.4471\n",
            "Epoch 41/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 41: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1766 - mae: 0.4763 - val_loss: 0.1589 - val_mae: 0.4466\n",
            "Epoch 42/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4771\n",
            "Epoch 42: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1766 - mae: 0.4760 - val_loss: 0.1569 - val_mae: 0.4445\n",
            "Epoch 43/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1765 - mae: 0.4759\n",
            "Epoch 43: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1765 - mae: 0.4759 - val_loss: 0.1583 - val_mae: 0.4458\n",
            "Epoch 44/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4754\n",
            "Epoch 44: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4754 - val_loss: 0.1584 - val_mae: 0.4464\n",
            "Epoch 45/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4749\n",
            "Epoch 45: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4754 - val_loss: 0.1577 - val_mae: 0.4446\n",
            "Epoch 46/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1766 - mae: 0.4762\n",
            "Epoch 46: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1766 - mae: 0.4762 - val_loss: 0.1575 - val_mae: 0.4444\n",
            "Epoch 47/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1773 - mae: 0.4762\n",
            "Epoch 47: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4754 - val_loss: 0.1568 - val_mae: 0.4441\n",
            "Epoch 48/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4770\n",
            "Epoch 48: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1764 - mae: 0.4756 - val_loss: 0.1566 - val_mae: 0.4438\n",
            "Epoch 49/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1744 - mae: 0.4728\n",
            "Epoch 49: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1590 - val_mae: 0.4466\n",
            "Epoch 50/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 50: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4755 - val_loss: 0.1577 - val_mae: 0.4453\n",
            "Epoch 51/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4768\n",
            "Epoch 51: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1766 - mae: 0.4761 - val_loss: 0.1584 - val_mae: 0.4459\n",
            "Epoch 52/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4752\n",
            "Epoch 52: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1595 - val_mae: 0.4476\n",
            "Epoch 53/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4773\n",
            "Epoch 53: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1766 - mae: 0.4759 - val_loss: 0.1577 - val_mae: 0.4445\n",
            "Epoch 54/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1753 - mae: 0.4733\n",
            "Epoch 54: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1580 - val_mae: 0.4462\n",
            "Epoch 55/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4756\n",
            "Epoch 55: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4756 - val_loss: 0.1594 - val_mae: 0.4475\n",
            "Epoch 56/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 56: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4756 - val_loss: 0.1568 - val_mae: 0.4439\n",
            "Epoch 57/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4754\n",
            "Epoch 57: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4756 - val_loss: 0.1592 - val_mae: 0.4482\n",
            "Epoch 58/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4756\n",
            "Epoch 58: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1576 - val_mae: 0.4452\n",
            "Epoch 59/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1773 - mae: 0.4768\n",
            "Epoch 59: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1574 - val_mae: 0.4441\n",
            "Epoch 60/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4756\n",
            "Epoch 60: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1581 - val_mae: 0.4460\n",
            "Epoch 61/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1776 - mae: 0.4781\n",
            "Epoch 61: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 62/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1766 - mae: 0.4757\n",
            "Epoch 62: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1574 - val_mae: 0.4442\n",
            "Epoch 63/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4754\n",
            "Epoch 63: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1765 - mae: 0.4758 - val_loss: 0.1564 - val_mae: 0.4429\n",
            "Epoch 64/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4734\n",
            "Epoch 64: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1764 - mae: 0.4755 - val_loss: 0.1572 - val_mae: 0.4444\n",
            "Epoch 65/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4752\n",
            "Epoch 65: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 66/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4737\n",
            "Epoch 66: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1567 - val_mae: 0.4435\n",
            "Epoch 67/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4742\n",
            "Epoch 67: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1591 - val_mae: 0.4470\n",
            "Epoch 68/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4741\n",
            "Epoch 68: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1588 - val_mae: 0.4473\n",
            "Epoch 69/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4747\n",
            "Epoch 69: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 70/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4748\n",
            "Epoch 70: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1593 - val_mae: 0.4478\n",
            "Epoch 71/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4747\n",
            "Epoch 71: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4747 - val_loss: 0.1577 - val_mae: 0.4452\n",
            "Epoch 72/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4756\n",
            "Epoch 72: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1589 - val_mae: 0.4467\n",
            "Epoch 73/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4752\n",
            "Epoch 73: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1592 - val_mae: 0.4474\n",
            "Epoch 74/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 74: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1591 - val_mae: 0.4471\n",
            "Epoch 75/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4752\n",
            "Epoch 75: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1584 - val_mae: 0.4463\n",
            "Epoch 76/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4764\n",
            "Epoch 76: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 77/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4743\n",
            "Epoch 77: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1585 - val_mae: 0.4460\n",
            "Epoch 78/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4743\n",
            "Epoch 78: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1588 - val_mae: 0.4470\n",
            "Epoch 79/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4755\n",
            "Epoch 79: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 80/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4743\n",
            "Epoch 80: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1580 - val_mae: 0.4456\n",
            "Epoch 81/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4752\n",
            "Epoch 81: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1586 - val_mae: 0.4463\n",
            "Epoch 82/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4766\n",
            "Epoch 82: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1765 - mae: 0.4759 - val_loss: 0.1579 - val_mae: 0.4452\n",
            "Epoch 83/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.4767\n",
            "Epoch 83: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4454\n",
            "Epoch 84/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4731\n",
            "Epoch 84: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4749 - val_loss: 0.1585 - val_mae: 0.4460\n",
            "Epoch 85/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4753\n",
            "Epoch 85: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 86/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4728\n",
            "Epoch 86: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1566 - val_mae: 0.4434\n",
            "Epoch 87/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4750\n",
            "Epoch 87: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4744 - val_loss: 0.1585 - val_mae: 0.4457\n",
            "Epoch 88/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4748\n",
            "Epoch 88: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4459\n",
            "Epoch 89/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4752\n",
            "Epoch 89: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1575 - val_mae: 0.4443\n",
            "Epoch 90/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 90: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4751 - val_loss: 0.1582 - val_mae: 0.4457\n",
            "Epoch 91/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4755\n",
            "Epoch 91: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1581 - val_mae: 0.4451\n",
            "Epoch 92/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4739\n",
            "Epoch 92: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1582 - val_mae: 0.4462\n",
            "Epoch 93/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4754\n",
            "Epoch 93: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1590 - val_mae: 0.4472\n",
            "Epoch 94/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4759\n",
            "Epoch 94: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1577 - val_mae: 0.4445\n",
            "Epoch 95/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4742\n",
            "Epoch 95: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1765 - mae: 0.4756 - val_loss: 0.1580 - val_mae: 0.4457\n",
            "Epoch 96/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1752 - mae: 0.4732\n",
            "Epoch 96: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1764 - mae: 0.4753 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 97/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4749\n",
            "Epoch 97: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1583 - val_mae: 0.4449\n",
            "Epoch 98/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4748\n",
            "Epoch 98: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1567 - val_mae: 0.4434\n",
            "Epoch 99/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1758 - mae: 0.4743\n",
            "Epoch 99: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1593 - val_mae: 0.4474\n",
            "Epoch 100/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4731\n",
            "Epoch 100: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1577 - val_mae: 0.4450\n",
            "Epoch 101/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 101: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1565 - val_mae: 0.4433\n",
            "Epoch 102/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4750\n",
            "Epoch 102: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1580 - val_mae: 0.4458\n",
            "Epoch 103/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 103: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 104/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4737\n",
            "Epoch 104: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1593 - val_mae: 0.4475\n",
            "Epoch 105/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4742\n",
            "Epoch 105: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4744 - val_loss: 0.1574 - val_mae: 0.4446\n",
            "Epoch 106/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 106: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4746 - val_loss: 0.1586 - val_mae: 0.4459\n",
            "Epoch 107/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4752\n",
            "Epoch 107: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1573 - val_mae: 0.4444\n",
            "Epoch 108/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4747\n",
            "Epoch 108: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4743 - val_loss: 0.1584 - val_mae: 0.4455\n",
            "Epoch 109/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 109: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1590 - val_mae: 0.4470\n",
            "Epoch 110/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4749\n",
            "Epoch 110: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 111/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4750\n",
            "Epoch 111: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1590 - val_mae: 0.4470\n",
            "Epoch 112/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4741\n",
            "Epoch 112: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4741 - val_loss: 0.1567 - val_mae: 0.4440\n",
            "Epoch 113/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4730\n",
            "Epoch 113: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1572 - val_mae: 0.4437\n",
            "Epoch 114/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4747\n",
            "Epoch 114: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1593 - val_mae: 0.4470\n",
            "Epoch 115/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 115: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4745 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 116/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4760\n",
            "Epoch 116: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1764 - mae: 0.4753 - val_loss: 0.1570 - val_mae: 0.4437\n",
            "Epoch 117/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 117: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1567 - val_mae: 0.4433\n",
            "Epoch 118/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1785 - mae: 0.4783\n",
            "Epoch 118: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1762 - mae: 0.4749 - val_loss: 0.1579 - val_mae: 0.4446\n",
            "Epoch 119/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4747\n",
            "Epoch 119: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1573 - val_mae: 0.4445\n",
            "Epoch 120/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4754\n",
            "Epoch 120: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 121/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4745\n",
            "Epoch 121: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4754 - val_loss: 0.1576 - val_mae: 0.4448\n",
            "Epoch 122/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1781 - mae: 0.4780\n",
            "Epoch 122: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1764 - mae: 0.4756 - val_loss: 0.1582 - val_mae: 0.4452\n",
            "Epoch 123/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4758\n",
            "Epoch 123: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4747 - val_loss: 0.1589 - val_mae: 0.4470\n",
            "Epoch 124/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 124: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1580 - val_mae: 0.4453\n",
            "Epoch 125/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4761\n",
            "Epoch 125: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1574 - val_mae: 0.4445\n",
            "Epoch 126/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4759\n",
            "Epoch 126: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1588 - val_mae: 0.4467\n",
            "Epoch 127/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4752\n",
            "Epoch 127: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4752 - val_loss: 0.1578 - val_mae: 0.4454\n",
            "Epoch 128/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4751\n",
            "Epoch 128: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4754 - val_loss: 0.1587 - val_mae: 0.4462\n",
            "Epoch 129/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1764 - mae: 0.4755\n",
            "Epoch 129: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4755 - val_loss: 0.1566 - val_mae: 0.4431\n",
            "Epoch 130/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4740\n",
            "Epoch 130: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1566 - val_mae: 0.4426\n",
            "Epoch 131/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4744\n",
            "Epoch 131: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1581 - val_mae: 0.4464\n",
            "Epoch 132/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1768 - mae: 0.4756\n",
            "Epoch 132: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4754 - val_loss: 0.1586 - val_mae: 0.4466\n",
            "Epoch 133/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4749\n",
            "Epoch 133: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 134/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4756\n",
            "Epoch 134: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1586 - val_mae: 0.4461\n",
            "Epoch 135/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4753\n",
            "Epoch 135: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 136/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4757\n",
            "Epoch 136: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1573 - val_mae: 0.4443\n",
            "Epoch 137/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4726\n",
            "Epoch 137: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1567 - val_mae: 0.4437\n",
            "Epoch 138/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4743\n",
            "Epoch 138: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4743 - val_loss: 0.1576 - val_mae: 0.4446\n",
            "Epoch 139/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1764 - mae: 0.4755\n",
            "Epoch 139: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1764 - mae: 0.4755 - val_loss: 0.1565 - val_mae: 0.4435\n",
            "Epoch 140/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1777 - mae: 0.4779\n",
            "Epoch 140: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1585 - val_mae: 0.4460\n",
            "Epoch 141/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4737\n",
            "Epoch 141: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1581 - val_mae: 0.4452\n",
            "Epoch 142/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4756\n",
            "Epoch 142: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1586 - val_mae: 0.4463\n",
            "Epoch 143/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4753\n",
            "Epoch 143: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1763 - mae: 0.4753 - val_loss: 0.1586 - val_mae: 0.4464\n",
            "Epoch 144/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4748\n",
            "Epoch 144: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1578 - val_mae: 0.4454\n",
            "Epoch 145/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4753\n",
            "Epoch 145: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1573 - val_mae: 0.4443\n",
            "Epoch 146/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4729\n",
            "Epoch 146: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1588 - val_mae: 0.4463\n",
            "Epoch 147/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4739\n",
            "Epoch 147: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 32ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1585 - val_mae: 0.4460\n",
            "Epoch 148/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.4767\n",
            "Epoch 148: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1586 - val_mae: 0.4456\n",
            "Epoch 149/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4754\n",
            "Epoch 149: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1590 - val_mae: 0.4468\n",
            "Epoch 150/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4747\n",
            "Epoch 150: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1575 - val_mae: 0.4446\n",
            "Epoch 151/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4759\n",
            "Epoch 151: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1747 - mae: 0.4732 - val_loss: 0.1584 - val_mae: 0.4460\n",
            "Epoch 152/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 152: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1589 - val_mae: 0.4472\n",
            "Epoch 153/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1786 - mae: 0.4794\n",
            "Epoch 153: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1583 - val_mae: 0.4459\n",
            "Epoch 154/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4757\n",
            "Epoch 154: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1588 - val_mae: 0.4464\n",
            "Epoch 155/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4747\n",
            "Epoch 155: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1590 - val_mae: 0.4466\n",
            "Epoch 156/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4756\n",
            "Epoch 156: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1585 - val_mae: 0.4455\n",
            "Epoch 157/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4738\n",
            "Epoch 157: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4738 - val_loss: 0.1578 - val_mae: 0.4454\n",
            "Epoch 158/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1780 - mae: 0.4779\n",
            "Epoch 158: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 159/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 159: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4454\n",
            "Epoch 160/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 160: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1588 - val_mae: 0.4461\n",
            "Epoch 161/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4755\n",
            "Epoch 161: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4746 - val_loss: 0.1570 - val_mae: 0.4437\n",
            "Epoch 162/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4754\n",
            "Epoch 162: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4746 - val_loss: 0.1585 - val_mae: 0.4464\n",
            "Epoch 163/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1774 - mae: 0.4770\n",
            "Epoch 163: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4754 - val_loss: 0.1591 - val_mae: 0.4467\n",
            "Epoch 164/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4762\n",
            "Epoch 164: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4456\n",
            "Epoch 165/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1742 - mae: 0.4718\n",
            "Epoch 165: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1588 - val_mae: 0.4463\n",
            "Epoch 166/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4748\n",
            "Epoch 166: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1585 - val_mae: 0.4461\n",
            "Epoch 167/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4743\n",
            "Epoch 167: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1751 - mae: 0.4737 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 168/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4750\n",
            "Epoch 168: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4750 - val_loss: 0.1585 - val_mae: 0.4464\n",
            "Epoch 169/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 169: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1585 - val_mae: 0.4458\n",
            "Epoch 170/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1773 - mae: 0.4772\n",
            "Epoch 170: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1573 - val_mae: 0.4439\n",
            "Epoch 171/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4728\n",
            "Epoch 171: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1582 - val_mae: 0.4455\n",
            "Epoch 172/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4737\n",
            "Epoch 172: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1580 - val_mae: 0.4448\n",
            "Epoch 173/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4740\n",
            "Epoch 173: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1573 - val_mae: 0.4441\n",
            "Epoch 174/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4762\n",
            "Epoch 174: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1585 - val_mae: 0.4462\n",
            "Epoch 175/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1763 - mae: 0.4754\n",
            "Epoch 175: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1763 - mae: 0.4754 - val_loss: 0.1593 - val_mae: 0.4472\n",
            "Epoch 176/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4741\n",
            "Epoch 176: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1586 - val_mae: 0.4460\n",
            "Epoch 177/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1773 - mae: 0.4770\n",
            "Epoch 177: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4746 - val_loss: 0.1591 - val_mae: 0.4473\n",
            "Epoch 178/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4748\n",
            "Epoch 178: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1573 - val_mae: 0.4444\n",
            "Epoch 179/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4751\n",
            "Epoch 179: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1578 - val_mae: 0.4456\n",
            "Epoch 180/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1736 - mae: 0.4713\n",
            "Epoch 180: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1574 - val_mae: 0.4436\n",
            "Epoch 181/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 181: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1585 - val_mae: 0.4461\n",
            "Epoch 182/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4771\n",
            "Epoch 182: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1573 - val_mae: 0.4447\n",
            "Epoch 183/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4742\n",
            "Epoch 183: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4742 - val_loss: 0.1587 - val_mae: 0.4461\n",
            "Epoch 184/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4762\n",
            "Epoch 184: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1572 - val_mae: 0.4439\n",
            "Epoch 185/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 185: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1763 - mae: 0.4754 - val_loss: 0.1592 - val_mae: 0.4471\n",
            "Epoch 186/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4751\n",
            "Epoch 186: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1579 - val_mae: 0.4445\n",
            "Epoch 187/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 187: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1576 - val_mae: 0.4447\n",
            "Epoch 188/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1777 - mae: 0.4779\n",
            "Epoch 188: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1585 - val_mae: 0.4457\n",
            "Epoch 189/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1778 - mae: 0.4781\n",
            "Epoch 189: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 190/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 190: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1568 - val_mae: 0.4437\n",
            "Epoch 191/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 191: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1577 - val_mae: 0.4448\n",
            "Epoch 192/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4739\n",
            "Epoch 192: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4739 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 193/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4754\n",
            "Epoch 193: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4747 - val_loss: 0.1576 - val_mae: 0.4446\n",
            "Epoch 194/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1772 - mae: 0.4771\n",
            "Epoch 194: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1580 - val_mae: 0.4454\n",
            "Epoch 195/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4749\n",
            "Epoch 195: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1583 - val_mae: 0.4455\n",
            "Epoch 196/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4727\n",
            "Epoch 196: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4738 - val_loss: 0.1573 - val_mae: 0.4440\n",
            "Epoch 197/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 197: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1593 - val_mae: 0.4479\n",
            "Epoch 198/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4741\n",
            "Epoch 198: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1579 - val_mae: 0.4447\n",
            "Epoch 199/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4730\n",
            "Epoch 199: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4744 - val_loss: 0.1591 - val_mae: 0.4468\n",
            "Epoch 200/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1780 - mae: 0.4780\n",
            "Epoch 200: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1582 - val_mae: 0.4451\n",
            "Epoch 201/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4745\n",
            "Epoch 201: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4745 - val_loss: 0.1583 - val_mae: 0.4456\n",
            "Epoch 202/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4751\n",
            "Epoch 202: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1577 - val_mae: 0.4446\n",
            "Epoch 203/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4741\n",
            "Epoch 203: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4740 - val_loss: 0.1595 - val_mae: 0.4480\n",
            "Epoch 204/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 204: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4754 - val_loss: 0.1571 - val_mae: 0.4440\n",
            "Epoch 205/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4719\n",
            "Epoch 205: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4449\n",
            "Epoch 206/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1743 - mae: 0.4730\n",
            "Epoch 206: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1579 - val_mae: 0.4455\n",
            "Epoch 207/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4739\n",
            "Epoch 207: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1580 - val_mae: 0.4450\n",
            "Epoch 208/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4735\n",
            "Epoch 208: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4748 - val_loss: 0.1580 - val_mae: 0.4450\n",
            "Epoch 209/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4716\n",
            "Epoch 209: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4740 - val_loss: 0.1581 - val_mae: 0.4449\n",
            "Epoch 210/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1776 - mae: 0.4774\n",
            "Epoch 210: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4755 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 211/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4757\n",
            "Epoch 211: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1571 - val_mae: 0.4435\n",
            "Epoch 212/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4742\n",
            "Epoch 212: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4742 - val_loss: 0.1574 - val_mae: 0.4445\n",
            "Epoch 213/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4744\n",
            "Epoch 213: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1587 - val_mae: 0.4465\n",
            "Epoch 214/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1773 - mae: 0.4768\n",
            "Epoch 214: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4751 - val_loss: 0.1583 - val_mae: 0.4458\n",
            "Epoch 215/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1753 - mae: 0.4739\n",
            "Epoch 215: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1580 - val_mae: 0.4455\n",
            "Epoch 216/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4752\n",
            "Epoch 216: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1570 - val_mae: 0.4435\n",
            "Epoch 217/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4741\n",
            "Epoch 217: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1557 - val_mae: 0.4417\n",
            "Epoch 218/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4756\n",
            "Epoch 218: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1583 - val_mae: 0.4449\n",
            "Epoch 219/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4767\n",
            "Epoch 219: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1572 - val_mae: 0.4437\n",
            "Epoch 220/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 220: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1575 - val_mae: 0.4448\n",
            "Epoch 221/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4737\n",
            "Epoch 221: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1586 - val_mae: 0.4458\n",
            "Epoch 222/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4767\n",
            "Epoch 222: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1574 - val_mae: 0.4447\n",
            "Epoch 223/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4756\n",
            "Epoch 223: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4749 - val_loss: 0.1596 - val_mae: 0.4484\n",
            "Epoch 224/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4744\n",
            "Epoch 224: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1575 - val_mae: 0.4445\n",
            "Epoch 225/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4747\n",
            "Epoch 225: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1569 - val_mae: 0.4437\n",
            "Epoch 226/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4750\n",
            "Epoch 226: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1553 - val_mae: 0.4414\n",
            "Epoch 227/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 227: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1583 - val_mae: 0.4451\n",
            "Epoch 228/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4759\n",
            "Epoch 228: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1588 - val_mae: 0.4470\n",
            "Epoch 229/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1741 - mae: 0.4723\n",
            "Epoch 229: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1591 - val_mae: 0.4470\n",
            "Epoch 230/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 230: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1568 - val_mae: 0.4431\n",
            "Epoch 231/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 231: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1589 - val_mae: 0.4470\n",
            "Epoch 232/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4754\n",
            "Epoch 232: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1575 - val_mae: 0.4443\n",
            "Epoch 233/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4742\n",
            "Epoch 233: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1580 - val_mae: 0.4452\n",
            "Epoch 234/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 234: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 235/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 235: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4445\n",
            "Epoch 236/500\n",
            "64/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4751\n",
            "Epoch 236: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1748 - mae: 0.4737 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 237/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4749\n",
            "Epoch 237: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1586 - val_mae: 0.4463\n",
            "Epoch 238/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4745\n",
            "Epoch 238: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 239/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1760 - mae: 0.4746\n",
            "Epoch 239: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 240/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4733\n",
            "Epoch 240: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1589 - val_mae: 0.4467\n",
            "Epoch 241/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4750\n",
            "Epoch 241: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1761 - mae: 0.4750 - val_loss: 0.1577 - val_mae: 0.4445\n",
            "Epoch 242/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 242: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1568 - val_mae: 0.4437\n",
            "Epoch 243/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4759\n",
            "Epoch 243: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1581 - val_mae: 0.4451\n",
            "Epoch 244/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4740\n",
            "Epoch 244: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1575 - val_mae: 0.4445\n",
            "Epoch 245/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4760\n",
            "Epoch 245: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1573 - val_mae: 0.4442\n",
            "Epoch 246/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 246: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1559 - val_mae: 0.4428\n",
            "Epoch 247/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4757\n",
            "Epoch 247: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1581 - val_mae: 0.4457\n",
            "Epoch 248/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4730\n",
            "Epoch 248: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1753 - mae: 0.4740 - val_loss: 0.1583 - val_mae: 0.4449\n",
            "Epoch 249/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4742\n",
            "Epoch 249: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1572 - val_mae: 0.4439\n",
            "Epoch 250/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 250: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1568 - val_mae: 0.4435\n",
            "Epoch 251/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4742\n",
            "Epoch 251: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1572 - val_mae: 0.4445\n",
            "Epoch 252/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1756 - mae: 0.4749\n",
            "Epoch 252: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1576 - val_mae: 0.4446\n",
            "Epoch 253/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4731\n",
            "Epoch 253: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1579 - val_mae: 0.4454\n",
            "Epoch 254/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 254: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4738 - val_loss: 0.1580 - val_mae: 0.4446\n",
            "Epoch 255/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4732\n",
            "Epoch 255: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1586 - val_mae: 0.4469\n",
            "Epoch 256/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4769\n",
            "Epoch 256: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1570 - val_mae: 0.4435\n",
            "Epoch 257/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4753\n",
            "Epoch 257: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1574 - val_mae: 0.4440\n",
            "Epoch 258/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4751\n",
            "Epoch 258: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1582 - val_mae: 0.4450\n",
            "Epoch 259/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4740\n",
            "Epoch 259: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1580 - val_mae: 0.4453\n",
            "Epoch 260/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4750\n",
            "Epoch 260: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1567 - val_mae: 0.4429\n",
            "Epoch 261/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 261: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1577 - val_mae: 0.4450\n",
            "Epoch 262/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1762 - mae: 0.4752\n",
            "Epoch 262: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4752 - val_loss: 0.1578 - val_mae: 0.4443\n",
            "Epoch 263/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 263: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1580 - val_mae: 0.4447\n",
            "Epoch 264/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1752 - mae: 0.4737\n",
            "Epoch 264: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1752 - mae: 0.4737 - val_loss: 0.1580 - val_mae: 0.4456\n",
            "Epoch 265/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4757\n",
            "Epoch 265: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1591 - val_mae: 0.4470\n",
            "Epoch 266/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4742\n",
            "Epoch 266: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1575 - val_mae: 0.4438\n",
            "Epoch 267/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4734\n",
            "Epoch 267: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 268/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4761\n",
            "Epoch 268: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1575 - val_mae: 0.4438\n",
            "Epoch 269/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1776 - mae: 0.4773\n",
            "Epoch 269: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1577 - val_mae: 0.4443\n",
            "Epoch 270/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4749\n",
            "Epoch 270: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1572 - val_mae: 0.4438\n",
            "Epoch 271/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1764 - mae: 0.4755\n",
            "Epoch 271: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1570 - val_mae: 0.4437\n",
            "Epoch 272/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4749\n",
            "Epoch 272: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1593 - val_mae: 0.4475\n",
            "Epoch 273/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4760\n",
            "Epoch 273: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4753 - val_loss: 0.1584 - val_mae: 0.4457\n",
            "Epoch 274/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4774\n",
            "Epoch 274: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4751 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 275/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4756\n",
            "Epoch 275: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1577 - val_mae: 0.4451\n",
            "Epoch 276/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4766\n",
            "Epoch 276: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4458\n",
            "Epoch 277/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4743\n",
            "Epoch 277: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1569 - val_mae: 0.4434\n",
            "Epoch 278/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4753\n",
            "Epoch 278: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 279/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1772 - mae: 0.4774\n",
            "Epoch 279: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1762 - mae: 0.4754 - val_loss: 0.1574 - val_mae: 0.4437\n",
            "Epoch 280/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1738 - mae: 0.4716\n",
            "Epoch 280: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4742 - val_loss: 0.1578 - val_mae: 0.4448\n",
            "Epoch 281/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4760\n",
            "Epoch 281: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1762 - mae: 0.4753 - val_loss: 0.1573 - val_mae: 0.4438\n",
            "Epoch 282/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4743\n",
            "Epoch 282: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1579 - val_mae: 0.4454\n",
            "Epoch 283/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4717\n",
            "Epoch 283: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1586 - val_mae: 0.4463\n",
            "Epoch 284/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1751 - mae: 0.4732\n",
            "Epoch 284: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1578 - val_mae: 0.4454\n",
            "Epoch 285/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 285: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1578 - val_mae: 0.4445\n",
            "Epoch 286/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4752\n",
            "Epoch 286: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1576 - val_mae: 0.4442\n",
            "Epoch 287/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.4732\n",
            "Epoch 287: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4742 - val_loss: 0.1584 - val_mae: 0.4462\n",
            "Epoch 288/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4755\n",
            "Epoch 288: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1568 - val_mae: 0.4434\n",
            "Epoch 289/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 289: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1572 - val_mae: 0.4442\n",
            "Epoch 290/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1751 - mae: 0.4738\n",
            "Epoch 290: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1751 - mae: 0.4738 - val_loss: 0.1590 - val_mae: 0.4468\n",
            "Epoch 291/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 291: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1574 - val_mae: 0.4441\n",
            "Epoch 292/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4748\n",
            "Epoch 292: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1579 - val_mae: 0.4446\n",
            "Epoch 293/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4760\n",
            "Epoch 293: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1567 - val_mae: 0.4432\n",
            "Epoch 294/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 294: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1573 - val_mae: 0.4448\n",
            "Epoch 295/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1776 - mae: 0.4776\n",
            "Epoch 295: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1571 - val_mae: 0.4433\n",
            "Epoch 296/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 296: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1580 - val_mae: 0.4449\n",
            "Epoch 297/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4732\n",
            "Epoch 297: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1590 - val_mae: 0.4466\n",
            "Epoch 298/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 298: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1571 - val_mae: 0.4439\n",
            "Epoch 299/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4727\n",
            "Epoch 299: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1580 - val_mae: 0.4455\n",
            "Epoch 300/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4757\n",
            "Epoch 300: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1589 - val_mae: 0.4466\n",
            "Epoch 301/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 301: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1586 - val_mae: 0.4465\n",
            "Epoch 302/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4742\n",
            "Epoch 302: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1578 - val_mae: 0.4451\n",
            "Epoch 303/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4748\n",
            "Epoch 303: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1583 - val_mae: 0.4453\n",
            "Epoch 304/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4745\n",
            "Epoch 304: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4749 - val_loss: 0.1580 - val_mae: 0.4456\n",
            "Epoch 305/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1739 - mae: 0.4714\n",
            "Epoch 305: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1571 - val_mae: 0.4437\n",
            "Epoch 306/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4748\n",
            "Epoch 306: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1574 - val_mae: 0.4441\n",
            "Epoch 307/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4751\n",
            "Epoch 307: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1588 - val_mae: 0.4465\n",
            "Epoch 308/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 308: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1583 - val_mae: 0.4458\n",
            "Epoch 309/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4742\n",
            "Epoch 309: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1576 - val_mae: 0.4444\n",
            "Epoch 310/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4756\n",
            "Epoch 310: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1577 - val_mae: 0.4449\n",
            "Epoch 311/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4747\n",
            "Epoch 311: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1581 - val_mae: 0.4450\n",
            "Epoch 312/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4732\n",
            "Epoch 312: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1573 - val_mae: 0.4440\n",
            "Epoch 313/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 313: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1586 - val_mae: 0.4459\n",
            "Epoch 314/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4739\n",
            "Epoch 314: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1586 - val_mae: 0.4461\n",
            "Epoch 315/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4755\n",
            "Epoch 315: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1587 - val_mae: 0.4463\n",
            "Epoch 316/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4749\n",
            "Epoch 316: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1567 - val_mae: 0.4430\n",
            "Epoch 317/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4747\n",
            "Epoch 317: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1566 - val_mae: 0.4431\n",
            "Epoch 318/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4753\n",
            "Epoch 318: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1584 - val_mae: 0.4457\n",
            "Epoch 319/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4750\n",
            "Epoch 319: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1572 - val_mae: 0.4438\n",
            "Epoch 320/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4756\n",
            "Epoch 320: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1585 - val_mae: 0.4458\n",
            "Epoch 321/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4748\n",
            "Epoch 321: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4748 - val_loss: 0.1579 - val_mae: 0.4449\n",
            "Epoch 322/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 322: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1575 - val_mae: 0.4443\n",
            "Epoch 323/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4755\n",
            "Epoch 323: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1582 - val_mae: 0.4456\n",
            "Epoch 324/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4741\n",
            "Epoch 324: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4741 - val_loss: 0.1549 - val_mae: 0.4417\n",
            "Epoch 325/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 325: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1574 - val_mae: 0.4445\n",
            "Epoch 326/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4736\n",
            "Epoch 326: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1552 - val_mae: 0.4411\n",
            "Epoch 327/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4758\n",
            "Epoch 327: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1584 - val_mae: 0.4462\n",
            "Epoch 328/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4747\n",
            "Epoch 328: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1586 - val_mae: 0.4461\n",
            "Epoch 329/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1740 - mae: 0.4726\n",
            "Epoch 329: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 330/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 330: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1565 - val_mae: 0.4435\n",
            "Epoch 331/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4752\n",
            "Epoch 331: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1574 - val_mae: 0.4446\n",
            "Epoch 332/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4743\n",
            "Epoch 332: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1579 - val_mae: 0.4447\n",
            "Epoch 333/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4741\n",
            "Epoch 333: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1586 - val_mae: 0.4465\n",
            "Epoch 334/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4734\n",
            "Epoch 334: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1562 - val_mae: 0.4429\n",
            "Epoch 335/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 335: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1580 - val_mae: 0.4452\n",
            "Epoch 336/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4771\n",
            "Epoch 336: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1568 - val_mae: 0.4437\n",
            "Epoch 337/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4733\n",
            "Epoch 337: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1556 - val_mae: 0.4416\n",
            "Epoch 338/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4748\n",
            "Epoch 338: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1580 - val_mae: 0.4457\n",
            "Epoch 339/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4735\n",
            "Epoch 339: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4462\n",
            "Epoch 340/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 340: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1585 - val_mae: 0.4464\n",
            "Epoch 341/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4753\n",
            "Epoch 341: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1583 - val_mae: 0.4455\n",
            "Epoch 342/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4744\n",
            "Epoch 342: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1583 - val_mae: 0.4457\n",
            "Epoch 343/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4751\n",
            "Epoch 343: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1575 - val_mae: 0.4448\n",
            "Epoch 344/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4744\n",
            "Epoch 344: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4744 - val_loss: 0.1575 - val_mae: 0.4439\n",
            "Epoch 345/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4750\n",
            "Epoch 345: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1563 - val_mae: 0.4427\n",
            "Epoch 346/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4759\n",
            "Epoch 346: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1751 - mae: 0.4739 - val_loss: 0.1575 - val_mae: 0.4446\n",
            "Epoch 347/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1747 - mae: 0.4736\n",
            "Epoch 347: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1591 - val_mae: 0.4471\n",
            "Epoch 348/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1762 - mae: 0.4749\n",
            "Epoch 348: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4750 - val_loss: 0.1562 - val_mae: 0.4427\n",
            "Epoch 349/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4735\n",
            "Epoch 349: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1587 - val_mae: 0.4461\n",
            "Epoch 350/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4749\n",
            "Epoch 350: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4747 - val_loss: 0.1566 - val_mae: 0.4429\n",
            "Epoch 351/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4754\n",
            "Epoch 351: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4754 - val_loss: 0.1582 - val_mae: 0.4452\n",
            "Epoch 352/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 352: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1761 - mae: 0.4752 - val_loss: 0.1572 - val_mae: 0.4440\n",
            "Epoch 353/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4758\n",
            "Epoch 353: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1582 - val_mae: 0.4455\n",
            "Epoch 354/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1732 - mae: 0.4707\n",
            "Epoch 354: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4745 - val_loss: 0.1583 - val_mae: 0.4458\n",
            "Epoch 355/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4754\n",
            "Epoch 355: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1588 - val_mae: 0.4469\n",
            "Epoch 356/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4759\n",
            "Epoch 356: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4457\n",
            "Epoch 357/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1755 - mae: 0.4747\n",
            "Epoch 357: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4747 - val_loss: 0.1586 - val_mae: 0.4462\n",
            "Epoch 358/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4754\n",
            "Epoch 358: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1578 - val_mae: 0.4445\n",
            "Epoch 359/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 359: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1585 - val_mae: 0.4461\n",
            "Epoch 360/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 360: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1576 - val_mae: 0.4445\n",
            "Epoch 361/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4761\n",
            "Epoch 361: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1581 - val_mae: 0.4452\n",
            "Epoch 362/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1745 - mae: 0.4725\n",
            "Epoch 362: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 3s 38ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1574 - val_mae: 0.4447\n",
            "Epoch 363/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4743\n",
            "Epoch 363: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 3s 38ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1581 - val_mae: 0.4454\n",
            "Epoch 364/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4737\n",
            "Epoch 364: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1752 - mae: 0.4741 - val_loss: 0.1580 - val_mae: 0.4452\n",
            "Epoch 365/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 365: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1578 - val_mae: 0.4452\n",
            "Epoch 366/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 366: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1587 - val_mae: 0.4460\n",
            "Epoch 367/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4742\n",
            "Epoch 367: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4750 - val_loss: 0.1585 - val_mae: 0.4459\n",
            "Epoch 368/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4734\n",
            "Epoch 368: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.1751 - mae: 0.4739 - val_loss: 0.1580 - val_mae: 0.4448\n",
            "Epoch 369/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4735\n",
            "Epoch 369: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1579 - val_mae: 0.4448\n",
            "Epoch 370/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1749 - mae: 0.4736\n",
            "Epoch 370: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1567 - val_mae: 0.4430\n",
            "Epoch 371/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4754\n",
            "Epoch 371: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1749 - mae: 0.4737 - val_loss: 0.1570 - val_mae: 0.4435\n",
            "Epoch 372/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 372: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1577 - val_mae: 0.4448\n",
            "Epoch 373/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1761 - mae: 0.4753\n",
            "Epoch 373: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1761 - mae: 0.4753 - val_loss: 0.1583 - val_mae: 0.4457\n",
            "Epoch 374/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 374: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 375/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 375: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1585 - val_mae: 0.4458\n",
            "Epoch 376/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4734\n",
            "Epoch 376: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1747 - mae: 0.4733 - val_loss: 0.1586 - val_mae: 0.4462\n",
            "Epoch 377/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1768 - mae: 0.4770\n",
            "Epoch 377: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1576 - val_mae: 0.4447\n",
            "Epoch 378/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4733\n",
            "Epoch 378: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1745 - mae: 0.4731 - val_loss: 0.1580 - val_mae: 0.4452\n",
            "Epoch 379/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4771\n",
            "Epoch 379: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1578 - val_mae: 0.4447\n",
            "Epoch 380/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 380: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1574 - val_mae: 0.4439\n",
            "Epoch 381/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1764 - mae: 0.4756\n",
            "Epoch 381: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1577 - val_mae: 0.4442\n",
            "Epoch 382/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4750\n",
            "Epoch 382: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4750 - val_loss: 0.1575 - val_mae: 0.4439\n",
            "Epoch 383/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4743\n",
            "Epoch 383: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1575 - val_mae: 0.4448\n",
            "Epoch 384/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1754 - mae: 0.4741\n",
            "Epoch 384: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1761 - mae: 0.4754 - val_loss: 0.1586 - val_mae: 0.4462\n",
            "Epoch 385/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4754\n",
            "Epoch 385: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1572 - val_mae: 0.4439\n",
            "Epoch 386/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1773 - mae: 0.4777\n",
            "Epoch 386: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 387/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4734\n",
            "Epoch 387: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1582 - val_mae: 0.4458\n",
            "Epoch 388/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 388: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1567 - val_mae: 0.4433\n",
            "Epoch 389/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 389: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1586 - val_mae: 0.4460\n",
            "Epoch 390/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4749\n",
            "Epoch 390: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1577 - val_mae: 0.4453\n",
            "Epoch 391/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4745\n",
            "Epoch 391: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1751 - mae: 0.4738 - val_loss: 0.1579 - val_mae: 0.4450\n",
            "Epoch 392/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 392: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1584 - val_mae: 0.4458\n",
            "Epoch 393/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 393: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1569 - val_mae: 0.4437\n",
            "Epoch 394/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4741\n",
            "Epoch 394: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1760 - mae: 0.4753 - val_loss: 0.1583 - val_mae: 0.4454\n",
            "Epoch 395/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4743\n",
            "Epoch 395: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4446\n",
            "Epoch 396/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1740 - mae: 0.4721\n",
            "Epoch 396: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1578 - val_mae: 0.4452\n",
            "Epoch 397/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1750 - mae: 0.4732\n",
            "Epoch 397: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1575 - val_mae: 0.4445\n",
            "Epoch 398/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4752\n",
            "Epoch 398: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1582 - val_mae: 0.4459\n",
            "Epoch 399/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 399: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1585 - val_mae: 0.4457\n",
            "Epoch 400/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4760\n",
            "Epoch 400: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1575 - val_mae: 0.4447\n",
            "Epoch 401/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1775 - mae: 0.4775\n",
            "Epoch 401: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 402/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 402: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1557 - val_mae: 0.4420\n",
            "Epoch 403/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1734 - mae: 0.4720\n",
            "Epoch 403: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1578 - val_mae: 0.4448\n",
            "Epoch 404/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4736\n",
            "Epoch 404: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1584 - val_mae: 0.4455\n",
            "Epoch 405/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1758 - mae: 0.4742\n",
            "Epoch 405: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1574 - val_mae: 0.4440\n",
            "Epoch 406/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4737\n",
            "Epoch 406: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1751 - mae: 0.4738 - val_loss: 0.1577 - val_mae: 0.4447\n",
            "Epoch 407/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.4714\n",
            "Epoch 407: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1583 - val_mae: 0.4459\n",
            "Epoch 408/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.4752\n",
            "Epoch 408: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1576 - val_mae: 0.4451\n",
            "Epoch 409/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4741\n",
            "Epoch 409: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1575 - val_mae: 0.4442\n",
            "Epoch 410/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4743\n",
            "Epoch 410: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4746 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 411/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4738\n",
            "Epoch 411: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "Epoch 412/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4747\n",
            "Epoch 412: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1592 - val_mae: 0.4474\n",
            "Epoch 413/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4755\n",
            "Epoch 413: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1587 - val_mae: 0.4464\n",
            "Epoch 414/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1745 - mae: 0.4731\n",
            "Epoch 414: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1589 - val_mae: 0.4463\n",
            "Epoch 415/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1762 - mae: 0.4756\n",
            "Epoch 415: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1575 - val_mae: 0.4436\n",
            "Epoch 416/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4722\n",
            "Epoch 416: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1582 - val_mae: 0.4451\n",
            "Epoch 417/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4735\n",
            "Epoch 417: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1749 - mae: 0.4737 - val_loss: 0.1569 - val_mae: 0.4436\n",
            "Epoch 418/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 418: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1572 - val_mae: 0.4440\n",
            "Epoch 419/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4755\n",
            "Epoch 419: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1751 - mae: 0.4738 - val_loss: 0.1580 - val_mae: 0.4446\n",
            "Epoch 420/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4763\n",
            "Epoch 420: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4752 - val_loss: 0.1586 - val_mae: 0.4460\n",
            "Epoch 421/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1760 - mae: 0.4753\n",
            "Epoch 421: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1578 - val_mae: 0.4448\n",
            "Epoch 422/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4756\n",
            "Epoch 422: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1570 - val_mae: 0.4436\n",
            "Epoch 423/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 423: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4745 - val_loss: 0.1582 - val_mae: 0.4455\n",
            "Epoch 424/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4750\n",
            "Epoch 424: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 425/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4757\n",
            "Epoch 425: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1586 - val_mae: 0.4458\n",
            "Epoch 426/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1769 - mae: 0.4759\n",
            "Epoch 426: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1582 - val_mae: 0.4459\n",
            "Epoch 427/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 427: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1591 - val_mae: 0.4467\n",
            "Epoch 428/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4748\n",
            "Epoch 428: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1581 - val_mae: 0.4451\n",
            "Epoch 429/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1751 - mae: 0.4739\n",
            "Epoch 429: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1577 - val_mae: 0.4446\n",
            "Epoch 430/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4742\n",
            "Epoch 430: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4740 - val_loss: 0.1572 - val_mae: 0.4439\n",
            "Epoch 431/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4738\n",
            "Epoch 431: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1577 - val_mae: 0.4450\n",
            "Epoch 432/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4743\n",
            "Epoch 432: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1570 - val_mae: 0.4435\n",
            "Epoch 433/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4761\n",
            "Epoch 433: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1568 - val_mae: 0.4435\n",
            "Epoch 434/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1769 - mae: 0.4757\n",
            "Epoch 434: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1576 - val_mae: 0.4443\n",
            "Epoch 435/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 435: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1579 - val_mae: 0.4456\n",
            "Epoch 436/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4749\n",
            "Epoch 436: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 437/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4761\n",
            "Epoch 437: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1751 - mae: 0.4739 - val_loss: 0.1566 - val_mae: 0.4430\n",
            "Epoch 438/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1763 - mae: 0.4761\n",
            "Epoch 438: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1587 - val_mae: 0.4465\n",
            "Epoch 439/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4745\n",
            "Epoch 439: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4749 - val_loss: 0.1568 - val_mae: 0.4430\n",
            "Epoch 440/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4753\n",
            "Epoch 440: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1579 - val_mae: 0.4451\n",
            "Epoch 441/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4740\n",
            "Epoch 441: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1571 - val_mae: 0.4436\n",
            "Epoch 442/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1759 - mae: 0.4751\n",
            "Epoch 442: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1578 - val_mae: 0.4449\n",
            "Epoch 443/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4745\n",
            "Epoch 443: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1572 - val_mae: 0.4443\n",
            "Epoch 444/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4735\n",
            "Epoch 444: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1583 - val_mae: 0.4460\n",
            "Epoch 445/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 445: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1580 - val_mae: 0.4455\n",
            "Epoch 446/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1742 - mae: 0.4725\n",
            "Epoch 446: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1752 - mae: 0.4741 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 447/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4744\n",
            "Epoch 447: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1579 - val_mae: 0.4449\n",
            "Epoch 448/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4748\n",
            "Epoch 448: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1589 - val_mae: 0.4466\n",
            "Epoch 449/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4746\n",
            "Epoch 449: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1576 - val_mae: 0.4450\n",
            "Epoch 450/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4746\n",
            "Epoch 450: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4746 - val_loss: 0.1592 - val_mae: 0.4472\n",
            "Epoch 451/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1753 - mae: 0.4741\n",
            "Epoch 451: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4745 - val_loss: 0.1586 - val_mae: 0.4462\n",
            "Epoch 452/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 452: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1756 - mae: 0.4746 - val_loss: 0.1593 - val_mae: 0.4474\n",
            "Epoch 453/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1749 - mae: 0.4738\n",
            "Epoch 453: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1578 - val_mae: 0.4453\n",
            "Epoch 454/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1753 - mae: 0.4738\n",
            "Epoch 454: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1576 - val_mae: 0.4449\n",
            "Epoch 455/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4743\n",
            "Epoch 455: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1754 - mae: 0.4743 - val_loss: 0.1577 - val_mae: 0.4456\n",
            "Epoch 456/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1755 - mae: 0.4739\n",
            "Epoch 456: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1583 - val_mae: 0.4455\n",
            "Epoch 457/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4758\n",
            "Epoch 457: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1575 - val_mae: 0.4447\n",
            "Epoch 458/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1746 - mae: 0.4728\n",
            "Epoch 458: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1591 - val_mae: 0.4471\n",
            "Epoch 459/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4766\n",
            "Epoch 459: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1582 - val_mae: 0.4447\n",
            "Epoch 460/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4744\n",
            "Epoch 460: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1587 - val_mae: 0.4463\n",
            "Epoch 461/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4756\n",
            "Epoch 461: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4744 - val_loss: 0.1574 - val_mae: 0.4440\n",
            "Epoch 462/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 462: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1566 - val_mae: 0.4435\n",
            "Epoch 463/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1767 - mae: 0.4769\n",
            "Epoch 463: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1760 - mae: 0.4751 - val_loss: 0.1567 - val_mae: 0.4429\n",
            "Epoch 464/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1737 - mae: 0.4722\n",
            "Epoch 464: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1755 - mae: 0.4744 - val_loss: 0.1584 - val_mae: 0.4456\n",
            "Epoch 465/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1758 - mae: 0.4745\n",
            "Epoch 465: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1568 - val_mae: 0.4430\n",
            "Epoch 466/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.4773\n",
            "Epoch 466: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4749 - val_loss: 0.1576 - val_mae: 0.4444\n",
            "Epoch 467/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1755 - mae: 0.4742\n",
            "Epoch 467: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1572 - val_mae: 0.4436\n",
            "Epoch 468/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4727\n",
            "Epoch 468: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.1753 - mae: 0.4743 - val_loss: 0.1583 - val_mae: 0.4453\n",
            "Epoch 469/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4744\n",
            "Epoch 469: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1754 - mae: 0.4744 - val_loss: 0.1579 - val_mae: 0.4446\n",
            "Epoch 470/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 470: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1569 - val_mae: 0.4434\n",
            "Epoch 471/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4747\n",
            "Epoch 471: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4747 - val_loss: 0.1582 - val_mae: 0.4448\n",
            "Epoch 472/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4753\n",
            "Epoch 472: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1568 - val_mae: 0.4433\n",
            "Epoch 473/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1771 - mae: 0.4760\n",
            "Epoch 473: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4742 - val_loss: 0.1577 - val_mae: 0.4445\n",
            "Epoch 474/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 474: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1567 - val_mae: 0.4435\n",
            "Epoch 475/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4743\n",
            "Epoch 475: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1756 - mae: 0.4743 - val_loss: 0.1579 - val_mae: 0.4453\n",
            "Epoch 476/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4741\n",
            "Epoch 476: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1571 - val_mae: 0.4436\n",
            "Epoch 477/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4754\n",
            "Epoch 477: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1573 - val_mae: 0.4437\n",
            "Epoch 478/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1748 - mae: 0.4730\n",
            "Epoch 478: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1755 - mae: 0.4746 - val_loss: 0.1587 - val_mae: 0.4462\n",
            "Epoch 479/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1754 - mae: 0.4741\n",
            "Epoch 479: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4741 - val_loss: 0.1578 - val_mae: 0.4447\n",
            "Epoch 480/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1772 - mae: 0.4770\n",
            "Epoch 480: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1581 - val_mae: 0.4453\n",
            "Epoch 481/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4751\n",
            "Epoch 481: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1574 - val_mae: 0.4447\n",
            "Epoch 482/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1763 - mae: 0.4754\n",
            "Epoch 482: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1759 - mae: 0.4751 - val_loss: 0.1574 - val_mae: 0.4441\n",
            "Epoch 483/500\n",
            "65/68 [===========================>..] - ETA: 0s - loss: 0.1766 - mae: 0.4771\n",
            "Epoch 483: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4752 - val_loss: 0.1576 - val_mae: 0.4446\n",
            "Epoch 484/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1765 - mae: 0.4758\n",
            "Epoch 484: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1754 - mae: 0.4742 - val_loss: 0.1582 - val_mae: 0.4455\n",
            "Epoch 485/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1757 - mae: 0.4747\n",
            "Epoch 485: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1592 - val_mae: 0.4477\n",
            "Epoch 486/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4774\n",
            "Epoch 486: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1753 - mae: 0.4741 - val_loss: 0.1582 - val_mae: 0.4453\n",
            "Epoch 487/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1782 - mae: 0.4790\n",
            "Epoch 487: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4747 - val_loss: 0.1578 - val_mae: 0.4444\n",
            "Epoch 488/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4722\n",
            "Epoch 488: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1750 - mae: 0.4737 - val_loss: 0.1563 - val_mae: 0.4423\n",
            "Epoch 489/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1750 - mae: 0.4740\n",
            "Epoch 489: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1758 - mae: 0.4748 - val_loss: 0.1587 - val_mae: 0.4467\n",
            "Epoch 490/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1758 - mae: 0.4747\n",
            "Epoch 490: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1758 - mae: 0.4747 - val_loss: 0.1585 - val_mae: 0.4455\n",
            "Epoch 491/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1756 - mae: 0.4747\n",
            "Epoch 491: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 30ms/step - loss: 0.1756 - mae: 0.4747 - val_loss: 0.1579 - val_mae: 0.4456\n",
            "Epoch 492/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1747 - mae: 0.4730\n",
            "Epoch 492: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4743 - val_loss: 0.1578 - val_mae: 0.4452\n",
            "Epoch 493/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1766 - mae: 0.4763\n",
            "Epoch 493: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1583 - val_mae: 0.4461\n",
            "Epoch 494/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1756 - mae: 0.4746\n",
            "Epoch 494: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1755 - mae: 0.4745 - val_loss: 0.1587 - val_mae: 0.4462\n",
            "Epoch 495/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1770 - mae: 0.4771\n",
            "Epoch 495: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1757 - mae: 0.4748 - val_loss: 0.1585 - val_mae: 0.4462\n",
            "Epoch 496/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1742 - mae: 0.4727\n",
            "Epoch 496: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4749 - val_loss: 0.1585 - val_mae: 0.4463\n",
            "Epoch 497/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1753 - mae: 0.4744\n",
            "Epoch 497: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1753 - mae: 0.4744 - val_loss: 0.1572 - val_mae: 0.4445\n",
            "Epoch 498/500\n",
            "67/68 [============================>.] - ETA: 0s - loss: 0.1761 - mae: 0.4748\n",
            "Epoch 498: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 29ms/step - loss: 0.1757 - mae: 0.4746 - val_loss: 0.1588 - val_mae: 0.4470\n",
            "Epoch 499/500\n",
            "66/68 [============================>.] - ETA: 0s - loss: 0.1752 - mae: 0.4742\n",
            "Epoch 499: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1751 - mae: 0.4741 - val_loss: 0.1580 - val_mae: 0.4454\n",
            "Epoch 500/500\n",
            "68/68 [==============================] - ETA: 0s - loss: 0.1759 - mae: 0.4750\n",
            "Epoch 500: loss did not improve from 0.17107\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.1759 - mae: 0.4750 - val_loss: 0.1580 - val_mae: 0.4451\n",
            "La valeur du mse obtenue  9  trimestres plus tard est  5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wBM5oiNtL174"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONCLUSION FINALE DE L'ETUDE : LES RESULTATS"
      ],
      "metadata": {
        "id": "nLRXCtX4R_4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|   **Evaluation year** \t| **Next 3 months** \t| **Next 6 months** \t| **Next 12 months** \t|\n",
        "|:---------------------:\t|:-----------------:\t|:-----------------:\t|:------------------:\t|\n",
        "|        **1987**       \t|       mse = 5.369 \t|    mse = 5.371    \t|     mse = 5.376    \t|\n",
        "|        **1988**       \t|       mse = 5.379 \t|    mse = 5.387    \t|        mse = 5.398 \t|\n",
        "|        **1989**       \t|       mse = 5.398 \t|     mse = 5.399   \t|         mse = 5.4  \t|"
      ],
      "metadata": {
        "id": "I4w5XJv5OGef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "leQgKEd-I6u8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Projet_Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2Yt-EgZ3sgPY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}